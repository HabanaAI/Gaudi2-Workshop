{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6ae3240-aecb-42ae-8e84-4c60c351590c",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b88e7ae8-3888-4cd4-b61b-9a48488e7d01",
   "metadata": {},
   "source": [
    "# Summarization using T5 model from Hugging Face on the Optimum Habana Library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "459dcf44-3d9c-4e51-8032-0a5ceb5f3078",
   "metadata": {},
   "source": [
    "### Summarization with T5-3B model on the Intel&reg; Gaudi&reg; 2 AI acclerator\n",
    "We will use the Hugging Face Summariazion example with the T5-3B model to fine tune the model with the CNN-dailymail dataset\n",
    "\n",
    "run_summarization.py is a lightweight example of how to download and preprocess a dataset from the 🤗 Datasets library "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ce72d9-014e-4f37-aad1-8558b8a1839f",
   "metadata": {},
   "source": [
    "#### Initial Setup\n",
    "We start with a Intel Gaudi PyTorch Docker image and run this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4a58fe9-01fd-412a-84a4-420b76da5d50",
   "metadata": {},
   "source": [
    "#### Install the Intel Gaudi DeepSpeed Fork\n",
    "The Intel Gaudi DeepSpeed Fork has implementations specifically for Gaudi and must be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75951895-150d-42d6-a499-1dcf3ec51576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HabanaAI/DeepSpeed.git@1.13.0\n",
      "  Cloning https://github.com/HabanaAI/DeepSpeed.git (to revision 1.13.0) to /tmp/pip-req-build-h7p9s_le\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/DeepSpeed.git /tmp/pip-req-build-h7p9s_le\n",
      "  Running command git checkout -b 1.13.0 --track origin/1.13.0\n",
      "  Switched to a new branch '1.13.0'\n",
      "  Branch '1.13.0' set up to track remote branch '1.13.0' from 'origin'.\n",
      "  Resolved https://github.com/HabanaAI/DeepSpeed.git to commit 6522014efac08fdcbc37ea4a9d85552ce9cb7b50\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: hjson in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (3.1.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (5.9.7)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (9.0.0)\n",
      "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (1.10.13)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (2.1.0a0+gitf8b6084)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.10.3+hpu.synapse.v1.13.0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic<2.0.0->deepspeed==0.10.3+hpu.synapse.v1.13.0) (4.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch->deepspeed==0.10.3+hpu.synapse.v1.13.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.16.2  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b632625f-98e3-4be3-87a2-90f39af24e34",
   "metadata": {},
   "source": [
    "#### Install the Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0391992b-28d5-4a71-9eaa-b384aedb158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum[habana] in /usr/local/lib/python3.8/dist-packages (1.16.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (15.0.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.12)\n",
      "Requirement already satisfied: transformers>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (4.34.1)\n",
      "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (2.1.0a0+gitf8b6084)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (23.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (0.17.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (2.14.7)\n",
      "Requirement already satisfied: optimum-habana in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (3.13.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2023.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.8.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum[habana]) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum[habana]) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum[habana]) (0.4.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (3.20.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum[habana]) (10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (3.9.1)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana->optimum[habana]) (0.25.0)\n",
      "Requirement already satisfied: diffusers<0.24.0,>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana->optimum[habana]) (0.23.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum[habana]) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.23.0->optimum-habana->optimum[habana]) (5.9.7)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from diffusers<0.24.0,>=0.18.0->optimum-habana->optimum[habana]) (10.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers<0.24.0,>=0.18.0->optimum-habana->optimum[habana]) (6.8.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.9->optimum[habana]) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum[habana]) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers<0.24.0,>=0.18.0->optimum-habana->optimum[habana]) (3.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: ipywidgets in /usr/local/lib/python3.8/dist-packages (8.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (8.12.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install optimum-habana==v1.12.0\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81f3f414-2d51-49ff-b12f-f5f3f521459c",
   "metadata": {},
   "source": [
    "#### Clone the Hugging Face Model Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbe4154-8b24-47f5-a431-da74635e701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n",
      "remote: Enumerating objects: 8392, done.\u001b[K\n",
      "remote: Counting objects: 100% (3154/3154), done.\u001b[K\n",
      "remote: Compressing objects: 100% (658/658), done.\u001b[K\n",
      "remote: Total 8392 (delta 2795), reused 2553 (delta 2482), pack-reused 5238\u001b[K\n",
      "Receiving objects: 100% (8392/8392), 3.77 MiB | 19.21 MiB/s, done.\n",
      "Resolving deltas: 100% (5631/5631), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone -b v1.12.0 https://github.com/huggingface/optimum-habana.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c85e29e-30be-4288-801d-4e819b49b748",
   "metadata": {},
   "source": [
    "#### Go the Summarization example model and install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e27db3ec-af54-41f3-81f7-4c0d4d54a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization\n"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab13f8a-5cb6-4676-b2db-52bbb6aff816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "455c3b87-a11b-44ff-85f9-db5daa9f5d78",
   "metadata": {},
   "source": [
    "### Setup for DeepSpeed\n",
    "Since we are using DeepSpeed, we have to confirm that the model has been configured properly.  We look for the following:\n",
    "\n",
    "* model, optimizer, ... = deepspeed.initialize(args=args, model=model, optimizer=optimizer, ...)\n",
    "* deepspeed.init_distributed(dist_backend=“hccl”, init_method=init_method)\n",
    "* Create a ds_config.json file to set the DS training parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b622e99-4b35-4ee9-b9b0-903524fc6ce1",
   "metadata": {},
   "source": [
    "#### DeepSpeed Initialization\n",
    "Look in deepspeed.py and we see the model being passed to the DeepSpeed engine\n",
    "\n",
    "```\n",
    "    import deepspeed\n",
    "    from deepspeed.utils import logger as ds_logger\n",
    "\n",
    "    model = trainer.model\n",
    "    args = trainer.args\n",
    "    ...\n",
    "\n",
    "    kwargs = {\n",
    "        \"args\": habana_args,\n",
    "        \"model\": model,\n",
    "        \"model_parameters\": model_parameters,\n",
    "        \"config_params\": config,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_scheduler\": lr_scheduler,\n",
    "    }\n",
    "\n",
    "    deepspeedengine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "146c64e9-b31f-4d84-ad0f-ee48d05cd34f",
   "metadata": {},
   "source": [
    "#### DeepSpeed Distributed\n",
    "Look in training_args.py and we see the DeepSpeed Distribution initialization\n",
    "\n",
    "```\n",
    "    from habana_frameworks.torch.distributed.hccl import initialize_distributed_hpu\n",
    "    world_size, rank, self.local_rank = initialize_distributed_hpu()\n",
    "\n",
    "    import deepspeed\n",
    "    deepspeed.init_distributed(dist_backend=\"hccl\", timeout=timedelta(seconds=self.ddp_timeout))\n",
    "       logger.info(\"DeepSpeed is enabled.\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4454d898-f2ff-4e6f-9171-db5f8bfc07f3",
   "metadata": {},
   "source": [
    "#### Create DeepSpeed Config file with ZeRO preferences\n",
    "The ds_config.json file will configure the parameters to run DeepSpeed\n",
    "\n",
    "In this case, we will run the ZeRO2 optimizer and BF16 mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "779b48d2-a098-424d-8756-3362ffa74cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b9f3bf-d0f1-4787-8d7c-5b9fde57f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"steps_per_print\": 64,\n",
      "    \"train_batch_size\": \"auto\",\n",
      "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
      "    \"gradient_accumulation_steps\": \"auto\",\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    },\n",
      "    \"gradient_clipping\": 1.0,\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2,\n",
      "        \"overlap_comm\": false,\n",
      "        \"reduce_scatter\": false,\n",
      "        \"contiguous_gradients\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tee ./ds_config.json <<EOF\n",
    "{\n",
    "    \"steps_per_print\": 64,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"overlap_comm\": false,\n",
    "        \"reduce_scatter\": false,\n",
    "        \"contiguous_gradients\": false\n",
    "    }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f8d68d-53ab-452d-a1cc-d93038951609",
   "metadata": {},
   "source": [
    "#### Fine Tuning T5-3b with the cnn_dailymail dataset\n",
    "The T5-3b model is a large language model that was originally trained on the C4 dataset and in this case will be fined tuned on the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset that is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail.\n",
    "\n",
    "For use of this model on Intel Gaudi First-Gen, users should update the model to \"T5-large\"\n",
    "\n",
    "This is run by `gaudi_spawn.py`, a simple launcher script to collect arguments and send them to `distributed_runner.py` for training on multiple HPUs, which then calls the `run_summarization.py` model.\n",
    "\n",
    "Notice the Habana specific commands to use here:\n",
    "\n",
    "-- use_habana  - allows training to run on Intel Gaudi cards\n",
    "-- use_hpu_graphs - reduces recompilation by replaying the graph  \n",
    "-- gaudi_config_name Habana/t5 - mapping to Hugging Face T5 Model  \n",
    "\n",
    "**Even though a Billion parameter T5 model can be used for Fine Tuning, this fine tuning still takes many hours to complete.  \n",
    "For users that wish to execute the example Fine Tuning, they should modify the `model_name_or_path` to \"t5-small\", which takes about 30 minutes to complete.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "565cc518-a016-4dc3-9b9c-07f749106c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3707f195-b154-4612-a210-2956a7ed2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ft-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c5fbafb-c218-43bd-bc6f-0d993f0858f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 8 --no_local_rank run_summarization.py --model_name_or_path t5-small --do_train --dataset_name cnn_dailymail --dataset_config \"3.0.0\" --source_prefix \"summarize: \" --output_dir ./ft-summarization --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --overwrite_output_dir --predict_with_generate --use_habana --use_lazy_mode --use_hpu_graphs_for_training --gaudi_config_name Habana/t5 --ignore_pad_token_for_loss False --pad_to_max_length --save_strategy epoch --report_to none --throughput_warmup_steps 3 --deepspeed ./ds_config.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "[2024-01-09 23:04:21,461] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-01-09 23:04:23,402] [WARNING] [runner.py:206:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-01-09 23:04:23,467] [INFO] [runner.py:585:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank --enable_each_rank_log=None run_summarization.py --model_name_or_path t5-small --do_train --dataset_name cnn_dailymail --dataset_config 3.0.0 --source_prefix summarize:  --output_dir ./ft-summarization --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --overwrite_output_dir --predict_with_generate --use_habana --use_lazy_mode --use_hpu_graphs_for_training --gaudi_config_name Habana/t5 --ignore_pad_token_for_loss False --pad_to_max_length --save_strategy epoch --report_to none --throughput_warmup_steps 3 --deepspeed ./ds_config.json\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "[2024-01-09 23:04:25,602] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-01-09 23:04:27,051] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2024-01-09 23:04:27,051] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2024-01-09 23:04:27,051] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2024-01-09 23:04:27,051] [INFO] [launch.py:164:main] dist_world_size=8\n",
      "[2024-01-09 23:04:27,051] [INFO] [launch.py:166:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "Downloading gaudi_config.json: 100%|█████████| 59.0/59.0 [00:00<00:00, 8.34kB/s]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "[2024-01-09 23:04:34,526] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-01-09 23:04:34,591] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-01-09 23:04:34,609] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-01-09 23:04:34,612] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-01-09 23:04:34,825] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-01-09 23:04:34,968] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-01-09 23:04:35,260] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-01-09 23:04:35,265] [INFO] [real_accelerator.py:175:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-01-09 23:04:35,821] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2024-01-09 23:04:35,821] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-01-09 23:04:36,448] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2024-01-09 23:04:36,448] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-01-09 23:04:36,477] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2024-01-09 23:04:36,477] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-01-09 23:04:36,498] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2024-01-09 23:04:36,498] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-01-09 23:04:36,676] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2024-01-09 23:04:36,676] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-01-09 23:04:36,843] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2024-01-09 23:04:36,843] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-01-09 23:04:37,042] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2024-01-09 23:04:37,043] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-01-09 23:04:37,138] [WARNING] [comm.py:163:init_deepspeed_backend] HCCL backend in DeepSpeed not yet implemented\n",
      "[2024-01-09 23:04:37,138] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-01-09 23:04:37,138] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl\n",
      "01/09/2024 23:04:37 - WARNING - __main__ - Process rank: 1, device: hpu:1, distributed training: True, mixed-precision training: False\n",
      "01/09/2024 23:04:37 - WARNING - __main__ - Process rank: 2, device: hpu:2, distributed training: True, mixed-precision training: False\n",
      "01/09/2024 23:04:37 - WARNING - __main__ - Process rank: 4, device: hpu:4, distributed training: True, mixed-precision training: False\n",
      "01/09/2024 23:04:37 - WARNING - __main__ - Process rank: 3, device: hpu:3, distributed training: True, mixed-precision training: False\n",
      "01/09/2024 23:04:38 - WARNING - __main__ - Process rank: 6, device: hpu:6, distributed training: True, mixed-precision training: False\n",
      "01/09/2024 23:04:38 - WARNING - __main__ - Process rank: 7, device: hpu:7, distributed training: True, mixed-precision training: False\n",
      "01/09/2024 23:04:38 - WARNING - __main__ - Process rank: 0, device: hpu:0, distributed training: True, mixed-precision training: False\n",
      "01/09/2024 23:04:38 - INFO - __main__ - Training/evaluation parameters GaudiSeq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=./ds_config.json,\n",
      "disable_tensor_cache_hpu_graphs=False,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/t5,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "ignore_eos=True,\n",
      "include_inputs_for_metrics=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./ft-summarization/runs/Jan09_23-04-33_sc09super17-klb2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_hpu_graphs=None,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./ft-summarization,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "pipelining_fwd_bwd=False,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "profiling_record_shapes=True,\n",
      "profiling_steps=0,\n",
      "profiling_warmup_steps=0,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./ft-summarization,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "throughput_warmup_steps=3,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=True,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "01/09/2024 23:04:38 - WARNING - __main__ - Process rank: 5, device: hpu:5, distributed training: True, mixed-precision training: False\n",
      "Downloading builder script: 100%|██████████| 8.33k/8.33k [00:00<00:00, 13.3MB/s]\n",
      "Downloading metadata: 100%|████████████████| 9.88k/9.88k [00:00<00:00, 13.6MB/s]\n",
      "Downloading readme: 100%|██████████████████| 15.1k/15.1k [00:00<00:00, 2.70MB/s]\n",
      "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
      "01/09/2024 23:04:39 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/cnn_dailymail/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
      "Downloading data files:   0%|                             | 0/5 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                              | 0.00/159M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   2%|▎                    | 2.56M/159M [00:00<00:06, 25.6MB/s]\u001b[A\n",
      "Downloading data:   4%|▊                    | 5.67M/159M [00:00<00:05, 28.8MB/s]\u001b[A\n",
      "Downloading data:   6%|█▎                   | 9.72M/159M [00:00<00:04, 33.7MB/s]\u001b[A\n",
      "Downloading data:   9%|█▉                   | 14.7M/159M [00:00<00:03, 40.1MB/s]\u001b[A\n",
      "Downloading data:  13%|██▊                  | 20.8M/159M [00:00<00:02, 47.5MB/s]\u001b[A\n",
      "Downloading data:  18%|███▋                 | 28.1M/159M [00:00<00:02, 56.1MB/s]\u001b[A\n",
      "Downloading data:  22%|████▋                | 35.7M/159M [00:00<00:01, 62.5MB/s]\u001b[A\n",
      "Downloading data:  27%|█████▋               | 43.1M/159M [00:00<00:01, 66.3MB/s]\u001b[A\n",
      "Downloading data:  32%|██████▋              | 50.6M/159M [00:00<00:01, 68.9MB/s]\u001b[A\n",
      "Downloading data:  37%|███████▋             | 58.1M/159M [00:01<00:01, 70.8MB/s]\u001b[A\n",
      "Downloading data:  41%|████████▋            | 65.5M/159M [00:01<00:01, 71.8MB/s]\u001b[A\n",
      "Downloading data:  46%|█████████▋           | 73.0M/159M [00:01<00:01, 72.7MB/s]\u001b[A\n",
      "Downloading data:  51%|██████████▋          | 80.4M/159M [00:01<00:01, 73.2MB/s]\u001b[A\n",
      "Downloading data:  55%|███████████▋         | 87.9M/159M [00:01<00:00, 73.8MB/s]\u001b[A\n",
      "Downloading data:  60%|████████████▋        | 95.5M/159M [00:01<00:00, 74.3MB/s]\u001b[A\n",
      "Downloading data:  65%|██████████████▎       | 103M/159M [00:01<00:00, 72.8MB/s]\u001b[A\n",
      "Downloading data:  69%|███████████████▎      | 110M/159M [00:01<00:00, 70.8MB/s]\u001b[A\n",
      "Downloading data:  74%|████████████████▎     | 118M/159M [00:01<00:00, 72.4MB/s]\u001b[A\n",
      "Downloading data:  79%|█████████████████▍    | 125M/159M [00:01<00:00, 73.0MB/s]\u001b[A\n",
      "Downloading data:  84%|██████████████████▍   | 133M/159M [00:02<00:00, 73.6MB/s]\u001b[A\n",
      "Downloading data:  88%|███████████████████▍  | 140M/159M [00:02<00:00, 74.2MB/s]\u001b[A\n",
      "Downloading data:  93%|████████████████████▌ | 148M/159M [00:02<00:00, 74.4MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 159M/159M [00:02<00:00, 67.1MB/s]\u001b[A\n",
      "Downloading data files:  20%|████▏                | 1/5 [00:02<00:11,  2.93s/it]\n",
      "Downloading data:   0%|                              | 0.00/376M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|▏                    | 3.29M/376M [00:00<00:11, 32.9MB/s]\u001b[A\n",
      "Downloading data:   3%|▌                    | 10.7M/376M [00:00<00:06, 57.1MB/s]\u001b[A\n",
      "Downloading data:   5%|█                    | 18.2M/376M [00:00<00:05, 65.2MB/s]\u001b[A\n",
      "Downloading data:   7%|█▍                   | 25.7M/376M [00:00<00:05, 69.2MB/s]\u001b[A\n",
      "Downloading data:   9%|█▊                   | 33.2M/376M [00:00<00:04, 71.3MB/s]\u001b[A\n",
      "Downloading data:  11%|██▎                  | 40.7M/376M [00:00<00:04, 72.5MB/s]\u001b[A\n",
      "Downloading data:  13%|██▋                  | 48.1M/376M [00:00<00:04, 73.1MB/s]\u001b[A\n",
      "Downloading data:  15%|███                  | 55.5M/376M [00:00<00:04, 73.3MB/s]\u001b[A\n",
      "Downloading data:  17%|███▌                 | 63.0M/376M [00:00<00:04, 73.8MB/s]\u001b[A\n",
      "Downloading data:  19%|███▉                 | 70.7M/376M [00:01<00:04, 74.9MB/s]\u001b[A\n",
      "Downloading data:  21%|████▍                | 78.5M/376M [00:01<00:03, 75.7MB/s]\u001b[A\n",
      "Downloading data:  23%|████▊                | 86.1M/376M [00:01<00:03, 75.6MB/s]\u001b[A\n",
      "Downloading data:  25%|█████▏               | 93.6M/376M [00:01<00:03, 75.4MB/s]\u001b[A\n",
      "Downloading data:  27%|█████▉                | 101M/376M [00:01<00:03, 75.5MB/s]\u001b[A\n",
      "Downloading data:  29%|██████▎               | 109M/376M [00:01<00:03, 73.1MB/s]\u001b[A\n",
      "Downloading data:  31%|██████▊               | 116M/376M [00:01<00:03, 73.8MB/s]\u001b[A\n",
      "Downloading data:  33%|███████▏              | 124M/376M [00:01<00:03, 73.8MB/s]\u001b[A\n",
      "Downloading data:  35%|███████▋              | 131M/376M [00:01<00:03, 74.0MB/s]\u001b[A\n",
      "Downloading data:  37%|████████              | 139M/376M [00:01<00:03, 74.4MB/s]\u001b[A\n",
      "Downloading data:  39%|████████▌             | 146M/376M [00:02<00:03, 74.8MB/s]\u001b[A\n",
      "Downloading data:  41%|█████████             | 154M/376M [00:02<00:02, 74.4MB/s]\u001b[A\n",
      "Downloading data:  43%|█████████▍            | 161M/376M [00:02<00:02, 74.7MB/s]\u001b[A\n",
      "Downloading data:  45%|█████████▉            | 169M/376M [00:02<00:02, 74.9MB/s]\u001b[A\n",
      "Downloading data:  47%|██████████▎           | 176M/376M [00:02<00:02, 75.1MB/s]\u001b[A\n",
      "Downloading data:  49%|██████████▊           | 184M/376M [00:02<00:02, 75.3MB/s]\u001b[A\n",
      "Downloading data:  51%|███████████▏          | 192M/376M [00:02<00:02, 75.4MB/s]\u001b[A\n",
      "Downloading data:  53%|███████████▋          | 199M/376M [00:02<00:02, 75.4MB/s]\u001b[A\n",
      "Downloading data:  55%|████████████          | 207M/376M [00:02<00:02, 75.4MB/s]\u001b[A\n",
      "Downloading data:  57%|████████████▌         | 214M/376M [00:02<00:02, 75.5MB/s]\u001b[A\n",
      "Downloading data:  59%|████████████▉         | 222M/376M [00:03<00:02, 75.5MB/s]\u001b[A\n",
      "Downloading data:  61%|█████████████▍        | 230M/376M [00:03<00:01, 78.7MB/s]\u001b[A\n",
      "Downloading data:  64%|██████████████▏       | 242M/376M [00:03<00:01, 88.6MB/s]\u001b[A\n",
      "Downloading data:  67%|██████████████▊       | 253M/376M [00:03<00:01, 96.1MB/s]\u001b[A\n",
      "Downloading data:  70%|████████████████▏      | 264M/376M [00:03<00:01, 102MB/s]\u001b[A\n",
      "Downloading data:  73%|████████████████▊      | 276M/376M [00:03<00:00, 105MB/s]\u001b[A\n",
      "Downloading data:  76%|█████████████████▌     | 287M/376M [00:03<00:00, 108MB/s]\u001b[A\n",
      "Downloading data:  79%|██████████████████▎    | 299M/376M [00:03<00:00, 110MB/s]\u001b[A\n",
      "Downloading data:  82%|██████████████████▉    | 310M/376M [00:03<00:00, 111MB/s]\u001b[A\n",
      "Downloading data:  86%|███████████████████▋   | 321M/376M [00:03<00:00, 112MB/s]\u001b[A\n",
      "Downloading data:  89%|████████████████████▎  | 333M/376M [00:04<00:00, 113MB/s]\u001b[A\n",
      "Downloading data:  92%|█████████████████████  | 344M/376M [00:04<00:00, 113MB/s]\u001b[A\n",
      "Downloading data:  95%|█████████████████████▊ | 356M/376M [00:04<00:00, 106MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████████████████| 376M/376M [00:04<00:00, 84.9MB/s]\u001b[A\n",
      "Downloading data files:  40%|████████▍            | 2/5 [00:07<00:12,  4.13s/it]\n",
      "Downloading data:   0%|                             | 0.00/12.3M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  70%|██████████████      | 8.63M/12.3M [00:00<00:00, 86.3MB/s]\u001b[A\n",
      "Downloading data: 17.8MB [00:00, 89.6MB/s]                                      \u001b[A\n",
      "Downloading data: 26.8MB [00:00, 88.9MB/s]\u001b[A\n",
      "Downloading data: 35.7MB [00:00, 88.4MB/s]\u001b[A\n",
      "Downloading data: 46.4MB [00:00, 88.2MB/s]\u001b[A\n",
      "Downloading data files:  60%|████████████▌        | 3/5 [00:11<00:07,  3.92s/it]\n",
      "Downloading data: 2.43MB [00:00, 54.4MB/s]                                      \u001b[A\n",
      "Downloading data files:  80%|████████████████▊    | 4/5 [00:12<00:02,  2.57s/it]\n",
      "Downloading data: 2.11MB [00:00, 46.2MB/s]                                      \u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 5/5 [00:12<00:00,  2.54s/it]\n",
      "Generating train split: 100%|██| 287113/287113 [01:03<00:00, 4498.11 examples/s]\n",
      "Generating validation split: 100%|█| 13368/13368 [00:31<00:00, 425.50 examples/s\n",
      "Generating test split: 100%|██████| 11490/11490 [00:31<00:00, 369.47 examples/s]\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "01/09/2024 23:06:58 - INFO - datasets.builder - Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
      "01/09/2024 23:06:58 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Downloading config.json: 100%|██████████████| 1.21k/1.21k [00:00<00:00, 690kB/s]\n",
      "[INFO|configuration_utils.py:715] 2024-01-09 23:06:58,672 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
      "[INFO|configuration_utils.py:775] 2024-01-09 23:06:58,675 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.34.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.8/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "Downloading tokenizer_config.json: 100%|████| 2.32k/2.32k [00:00<00:00, 253kB/s]\n",
      "Downloading spiece.model: 100%|██████████████| 792k/792k [00:00<00:00, 3.92MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.39M/1.39M [00:00<00:00, 2.52MB/s]\n",
      "[INFO|tokenization_utils_base.py:2015] 2024-01-09 23:07:00,402 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2015] 2024-01-09 23:07:00,402 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2015] 2024-01-09 23:07:00,403 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2015] 2024-01-09 23:07:00,403 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2015] 2024-01-09 23:07:00,403 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
      "Downloading model.safetensors: 100%|██████████| 242M/242M [00:01<00:00, 214MB/s]\n",
      "[INFO|modeling_utils.py:2993] 2024-01-09 23:07:01,805 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n",
      "[INFO|configuration_utils.py:770] 2024-01-09 23:07:01,815 >> Generate config GaudiGenerationConfig {\n",
      "  \"attn_softmax_bf16\": null,\n",
      "  \"bucket_size\": -1,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"ignore_eos\": null,\n",
      "  \"kv_cache_fp8\": null,\n",
      "  \"limit_hpu_graphs\": null,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reuse_cache\": null,\n",
      "  \"static_shapes\": null,\n",
      "  \"trim_logits\": null\n",
      "}\n",
      "\n",
      "Downloading generation_config.json: 100%|██████| 147/147 [00:00<00:00, 31.6kB/s]\n",
      "[INFO|modeling_utils.py:3775] 2024-01-09 23:07:05,708 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3783] 2024-01-09 23:07:05,709 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:730] 2024-01-09 23:07:05,808 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n",
      "[INFO|configuration_utils.py:770] 2024-01-09 23:07:05,808 >> Generate config GaudiGenerationConfig {\n",
      "  \"attn_softmax_bf16\": null,\n",
      "  \"bucket_size\": -1,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"ignore_eos\": null,\n",
      "  \"kv_cache_fp8\": null,\n",
      "  \"limit_hpu_graphs\": null,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reuse_cache\": null,\n",
      "  \"static_shapes\": null,\n",
      "  \"trim_logits\": null\n",
      "}\n",
      "\n",
      "Running tokenizer on train dataset:   0%|     | 0/287113 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-ae6df5ed19129945.arrow\n",
      "01/09/2024 23:07:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-ae6df5ed19129945.arrow\n",
      "Running tokenizer on train dataset: 100%|█| 287113/287113 [03:24<00:00, 1402.30 \n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 9.12MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 18.4MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 7.63MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 16.1MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 13.1MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 9.58MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 22.9MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 17.8MB/s]\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 0\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056446876 KB\n",
      "------------------------------------------------------------------------------\n",
      "[2024-01-09 23:10:36,152] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3+hpu.synapse.v1.13.0, git-hash=6522014, git-branch=1.13.0\n",
      "[2024-01-09 23:10:38,577] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-01-09 23:10:38,579] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2024-01-09 23:10:38,579] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-01-09 23:10:38,583] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdamW\n",
      "[2024-01-09 23:10:38,583] [INFO] [utils.py:61:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdamW type=<class 'habana_frameworks.torch.hpex.optimizers.FusedAdamW.FusedAdamW'>\n",
      "[2024-01-09 23:10:38,584] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-01-09 23:10:38,584] [INFO] [stage_1_and_2.py:151:__init__] Reduce bucket size 500,000,000\n",
      "[2024-01-09 23:10:38,584] [INFO] [stage_1_and_2.py:152:__init__] Allgather bucket size 500,000,000\n",
      "[2024-01-09 23:10:38,584] [INFO] [stage_1_and_2.py:153:__init__] CPU Offload: False\n",
      "[2024-01-09 23:10:38,584] [INFO] [stage_1_and_2.py:154:__init__] Round robin gradient partitioning: False\n",
      "Rank: 4 partition count [8, 8] and sizes[(7561216, False), (2112, False)] \n",
      "Rank: 2 partition count [8, 8] and sizes[(7561216, False), (2112, False)] \n",
      "Rank: 0 partition count [8, 8] and sizes[(7561216, False), (2112, False)] \n",
      "Rank: 6 partition count [8, 8] and sizes[(7561216, False), (2112, False)] \n",
      "Rank: 5 partition count [8, 8] and sizes[(7561216, False), (2112, False)] \n",
      "Rank: 3 partition count [8, 8] and sizes[(7561216, False), (2112, False)] \n",
      "Rank: 7 partition count [8, 8] and sizes[(7561216, False), (2112, False)] \n",
      "Rank: 1 partition count [8, 8] and sizes[(7561216, False), (2112, False)] \n",
      "[2024-01-09 23:10:43,670] [INFO] [utils.py:866:see_memory_usage] Before initializing optimizer states\n",
      "[2024-01-09 23:10:43,673] [INFO] [utils.py:867:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2024-01-09 23:10:43,673] [INFO] [utils.py:874:see_memory_usage] CPU Virtual Memory:  used = 61.73 GB, percent = 6.1%\n",
      "[2024-01-09 23:10:43,813] [INFO] [utils.py:866:see_memory_usage] After initializing optimizer states\n",
      "[2024-01-09 23:10:43,816] [INFO] [utils.py:867:see_memory_usage] MA 0.23 GB         Max_MA 0.23 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2024-01-09 23:10:43,816] [INFO] [utils.py:874:see_memory_usage] CPU Virtual Memory:  used = 61.78 GB, percent = 6.1%\n",
      "[2024-01-09 23:10:43,817] [INFO] [stage_1_and_2.py:570:__init__] optimizer state initialized\n",
      "[2024-01-09 23:10:43,916] [INFO] [utils.py:866:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-01-09 23:10:43,919] [INFO] [utils.py:867:see_memory_usage] MA 0.2 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "[2024-01-09 23:10:43,919] [INFO] [utils.py:874:see_memory_usage] CPU Virtual Memory:  used = 61.78 GB, percent = 6.1%\n",
      "[2024-01-09 23:10:43,922] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2024-01-09 23:10:43,922] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-01-09 23:10:43,922] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-01-09 23:10:43,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2024-01-09 23:10:43,923] [INFO] [config.py:982:print] DeepSpeedEngine configuration:\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   amp_enabled .................. False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   amp_params ................... False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   bfloat16_accumulate_grads_via_hooks  False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   bfloat16_enabled ............. True\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fc2636cbd00>\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   communication_data_type ...... None\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   curriculum_enabled_legacy .... False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   curriculum_params_legacy ..... False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   data_efficiency_enabled ...... False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   dataloader_drop_last ......... False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   disable_allgather ............ False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   dump_state ................... False\n",
      "[2024-01-09 23:10:43,924] [INFO] [config.py:986:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   eigenvalue_enabled ........... False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   eigenvalue_verbose ........... False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   elasticity_enabled ........... False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   fp16_auto_cast ............... None\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   fp16_enabled ................. False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   global_rank .................. 0\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   grad_accum_dtype ............. None\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   gradient_accumulation_steps .. 1\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   gradient_clipping ............ 1.0\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   initial_dynamic_scale ........ 1\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   load_universal_checkpoint .... False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   loss_scale ................... 1.0\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   memory_breakdown ............. False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   mics_hierarchial_params_gather  False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   mics_shard_size .............. -1\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   optimizer_name ............... None\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   optimizer_params ............. None\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': 'auto', 'grad_partitioned': 'auto'}\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   pld_enabled .................. False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   pld_params ................... False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   prescale_gradients ........... False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   scheduler_name ............... None\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   scheduler_params ............. None\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   sparse_attention ............. None\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   sparse_gradients_enabled ..... False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   steps_per_print .............. inf\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   train_batch_size ............. 32\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   train_micro_batch_size_per_gpu  4\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   use_node_local_storage ....... False\n",
      "[2024-01-09 23:10:43,925] [INFO] [config.py:986:print]   wall_clock_breakdown ......... False\n",
      "[2024-01-09 23:10:43,926] [INFO] [config.py:986:print]   weight_quantization_config ... None\n",
      "[2024-01-09 23:10:43,926] [INFO] [config.py:986:print]   world_size ................... 8\n",
      "[2024-01-09 23:10:43,926] [INFO] [config.py:986:print]   zero_allow_comm_data_type_fp32  False\n",
      "[2024-01-09 23:10:43,926] [INFO] [config.py:986:print]   zero_allow_untested_optimizer  True\n",
      "[2024-01-09 23:10:43,926] [INFO] [config.py:986:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=False reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False max_group_size=4000000000 load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-01-09 23:10:43,926] [INFO] [config.py:986:print]   zero_enabled ................. True\n",
      "[2024-01-09 23:10:43,926] [INFO] [config.py:986:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-01-09 23:10:43,926] [INFO] [config.py:986:print]   zero_optimization_stage ...... 2\n",
      "[2024-01-09 23:10:43,926] [INFO] [config.py:972:print_user_config]   json = {\n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"overlap_comm\": false, \n",
      "        \"reduce_scatter\": false, \n",
      "        \"contiguous_gradients\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "[INFO|trainer.py:672] 2024-01-09 23:10:43,926 >> ***** Running training *****\n",
      "[INFO|trainer.py:673] 2024-01-09 23:10:43,926 >>   Num examples = 287,113\n",
      "[INFO|trainer.py:674] 2024-01-09 23:10:43,926 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:675] 2024-01-09 23:10:43,926 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:678] 2024-01-09 23:10:43,926 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:679] 2024-01-09 23:10:43,926 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:680] 2024-01-09 23:10:43,927 >>   Total optimization steps = 26,919\n",
      "[INFO|trainer.py:681] 2024-01-09 23:10:43,928 >>   Number of trainable parameters = 60,506,624\n",
      "{'loss': 1.533, 'learning_rate': 4.907128793788774e-05, 'epoch': 0.06, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0871, 'learning_rate': 4.814257587577547e-05, 'epoch': 0.11, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0832, 'learning_rate': 4.7213863813663214e-05, 'epoch': 0.17, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0783, 'learning_rate': 4.6285151751550956e-05, 'epoch': 0.22, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0779, 'learning_rate': 4.535643968943869e-05, 'epoch': 0.28, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0654, 'learning_rate': 4.4427727627326426e-05, 'epoch': 0.33, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0688, 'learning_rate': 4.349901556521416e-05, 'epoch': 0.39, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0646, 'learning_rate': 4.2570303503101897e-05, 'epoch': 0.45, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0625, 'learning_rate': 4.164159144098964e-05, 'epoch': 0.5, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0633, 'learning_rate': 4.0712879378877373e-05, 'epoch': 0.56, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0614, 'learning_rate': 3.9784167316765115e-05, 'epoch': 0.61, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0649, 'learning_rate': 3.885545525465285e-05, 'epoch': 0.67, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0603, 'learning_rate': 3.7926743192540585e-05, 'epoch': 0.72, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0569, 'learning_rate': 3.699803113042832e-05, 'epoch': 0.78, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0545, 'learning_rate': 3.606931906831606e-05, 'epoch': 0.84, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0521, 'learning_rate': 3.51406070062038e-05, 'epoch': 0.89, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0524, 'learning_rate': 3.421189494409154e-05, 'epoch': 0.95, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      " 33%|████████████▋                         | 8973/26919 [10:48<23:58, 12.47it/s][INFO|trainer.py:1406] 2024-01-09 23:21:33,265 >> Saving model checkpoint to ./ft-summarization/checkpoint-8973\n",
      "[INFO|configuration_utils.py:460] 2024-01-09 23:21:33,268 >> Configuration saved in ./ft-summarization/checkpoint-8973/config.json\n",
      "[INFO|configuration_utils.py:544] 2024-01-09 23:21:33,268 >> Configuration saved in ./ft-summarization/checkpoint-8973/generation_config.json\n",
      "[INFO|modeling_utils.py:2118] 2024-01-09 23:21:33,508 >> Model weights saved in ./ft-summarization/checkpoint-8973/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2420] 2024-01-09 23:21:33,510 >> tokenizer config file saved in ./ft-summarization/checkpoint-8973/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2429] 2024-01-09 23:21:33,511 >> Special tokens file saved in ./ft-summarization/checkpoint-8973/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:189] 2024-01-09 23:21:33,512 >> Copy vocab file to ./ft-summarization/checkpoint-8973/spiece.model\n",
      "[INFO|configuration_utils.py:113] 2024-01-09 23:21:33,522 >> Configuration saved in ./ft-summarization/checkpoint-8973/gaudi_config.json\n",
      "[2024-01-09 23:21:33,524] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8973 is about to be saved!\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1882: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1882: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1882: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1882: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1882: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1882: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1882: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1882: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-01-09 23:21:33,559] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./ft-summarization/checkpoint-8973/global_step8973/mp_rank_00_model_states.pt\n",
      "[2024-01-09 23:21:33,559] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-8973/global_step8973/mp_rank_00_model_states.pt...\n",
      "[2024-01-09 23:21:33,860] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./ft-summarization/checkpoint-8973/global_step8973/mp_rank_00_model_states.pt.\n",
      "[2024-01-09 23:21:33,879] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-8973/global_step8973/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-01-09 23:21:34,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./ft-summarization/checkpoint-8973/global_step8973/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-01-09 23:21:34,027] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-8973/global_step8973/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-01-09 23:21:35,760] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./ft-summarization/checkpoint-8973/global_step8973/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-01-09 23:21:35,828] [INFO] [engine.py:3464:_save_zero_checkpoint] zero checkpoint saved ./ft-summarization/checkpoint-8973/global_step8973/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-01-09 23:21:35,828] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8973 is ready now!\n",
      "{'loss': 1.0502, 'learning_rate': 3.3283182881979274e-05, 'epoch': 1.0, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0474, 'learning_rate': 3.235447081986701e-05, 'epoch': 1.06, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0471, 'learning_rate': 3.1425758757754745e-05, 'epoch': 1.11, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0491, 'learning_rate': 3.0497046695642483e-05, 'epoch': 1.17, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.05, 'learning_rate': 2.956833463353022e-05, 'epoch': 1.23, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0408, 'learning_rate': 2.8639622571417957e-05, 'epoch': 1.28, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0462, 'learning_rate': 2.77109105093057e-05, 'epoch': 1.34, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0463, 'learning_rate': 2.6782198447193433e-05, 'epoch': 1.39, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0479, 'learning_rate': 2.5853486385081172e-05, 'epoch': 1.45, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0396, 'learning_rate': 2.4924774322968907e-05, 'epoch': 1.5, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0446, 'learning_rate': 2.3996062260856645e-05, 'epoch': 1.56, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0464, 'learning_rate': 2.3067350198744384e-05, 'epoch': 1.62, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0432, 'learning_rate': 2.213863813663212e-05, 'epoch': 1.67, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0482, 'learning_rate': 2.1209926074519858e-05, 'epoch': 1.73, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0394, 'learning_rate': 2.0281214012407593e-05, 'epoch': 1.78, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0383, 'learning_rate': 1.935250195029533e-05, 'epoch': 1.84, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0404, 'learning_rate': 1.842378988818307e-05, 'epoch': 1.89, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0462, 'learning_rate': 1.7495077826070805e-05, 'epoch': 1.95, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      " 67%|████████████████████████▋            | 17945/26919 [21:36<12:08, 12.32it/s][INFO|trainer.py:1406] 2024-01-09 23:32:21,004 >> Saving model checkpoint to ./ft-summarization/checkpoint-17946\n",
      "[INFO|configuration_utils.py:460] 2024-01-09 23:32:21,006 >> Configuration saved in ./ft-summarization/checkpoint-17946/config.json\n",
      "[INFO|configuration_utils.py:544] 2024-01-09 23:32:21,007 >> Configuration saved in ./ft-summarization/checkpoint-17946/generation_config.json\n",
      "[INFO|modeling_utils.py:2118] 2024-01-09 23:32:21,245 >> Model weights saved in ./ft-summarization/checkpoint-17946/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2420] 2024-01-09 23:32:21,248 >> tokenizer config file saved in ./ft-summarization/checkpoint-17946/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2429] 2024-01-09 23:32:21,248 >> Special tokens file saved in ./ft-summarization/checkpoint-17946/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:189] 2024-01-09 23:32:21,249 >> Copy vocab file to ./ft-summarization/checkpoint-17946/spiece.model\n",
      "[INFO|configuration_utils.py:113] 2024-01-09 23:32:21,260 >> Configuration saved in ./ft-summarization/checkpoint-17946/gaudi_config.json\n",
      "[2024-01-09 23:32:21,262] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step17946 is about to be saved!\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1882: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-01-09 23:32:21,279] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./ft-summarization/checkpoint-17946/global_step17946/mp_rank_00_model_states.pt\n",
      "[2024-01-09 23:32:21,279] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-17946/global_step17946/mp_rank_00_model_states.pt...\n",
      "[2024-01-09 23:32:21,572] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./ft-summarization/checkpoint-17946/global_step17946/mp_rank_00_model_states.pt.\n",
      "[2024-01-09 23:32:21,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-17946/global_step17946/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-01-09 23:32:21,735] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./ft-summarization/checkpoint-17946/global_step17946/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-01-09 23:32:21,735] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-17946/global_step17946/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-01-09 23:32:23,236] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./ft-summarization/checkpoint-17946/global_step17946/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-01-09 23:32:23,237] [INFO] [engine.py:3464:_save_zero_checkpoint] zero checkpoint saved ./ft-summarization/checkpoint-17946/global_step17946/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-01-09 23:32:23,237] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17946 is ready now!\n",
      "{'loss': 1.0405, 'learning_rate': 1.6566365763958543e-05, 'epoch': 2.01, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0418, 'learning_rate': 1.563765370184628e-05, 'epoch': 2.06, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0323, 'learning_rate': 1.4708941639734017e-05, 'epoch': 2.12, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0373, 'learning_rate': 1.3780229577621757e-05, 'epoch': 2.17, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0425, 'learning_rate': 1.2851517515509492e-05, 'epoch': 2.23, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0442, 'learning_rate': 1.1922805453397229e-05, 'epoch': 2.28, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0269, 'learning_rate': 1.0994093391284967e-05, 'epoch': 2.34, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0308, 'learning_rate': 1.0065381329172704e-05, 'epoch': 2.4, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0411, 'learning_rate': 9.13666926706044e-06, 'epoch': 2.45, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0345, 'learning_rate': 8.207957204948179e-06, 'epoch': 2.51, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.033, 'learning_rate': 7.279245142835915e-06, 'epoch': 2.56, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0429, 'learning_rate': 6.350533080723653e-06, 'epoch': 2.62, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0312, 'learning_rate': 5.42182101861139e-06, 'epoch': 2.67, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0341, 'learning_rate': 4.493108956499127e-06, 'epoch': 2.73, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0347, 'learning_rate': 3.564396894386864e-06, 'epoch': 2.79, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0412, 'learning_rate': 2.6356848322746018e-06, 'epoch': 2.84, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0369, 'learning_rate': 1.7069727701623388e-06, 'epoch': 2.9, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0358, 'learning_rate': 7.782607080500762e-07, 'epoch': 2.95, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "100%|█████████████████████████████████████| 26919/26919 [32:26<00:00, 12.23it/s][INFO|trainer.py:1406] 2024-01-09 23:43:10,685 >> Saving model checkpoint to ./ft-summarization/checkpoint-26919\n",
      "[INFO|configuration_utils.py:460] 2024-01-09 23:43:10,687 >> Configuration saved in ./ft-summarization/checkpoint-26919/config.json\n",
      "[INFO|configuration_utils.py:544] 2024-01-09 23:43:10,688 >> Configuration saved in ./ft-summarization/checkpoint-26919/generation_config.json\n",
      "[INFO|modeling_utils.py:2118] 2024-01-09 23:43:10,922 >> Model weights saved in ./ft-summarization/checkpoint-26919/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2420] 2024-01-09 23:43:10,925 >> tokenizer config file saved in ./ft-summarization/checkpoint-26919/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2429] 2024-01-09 23:43:10,926 >> Special tokens file saved in ./ft-summarization/checkpoint-26919/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:189] 2024-01-09 23:43:10,926 >> Copy vocab file to ./ft-summarization/checkpoint-26919/spiece.model\n",
      "[INFO|configuration_utils.py:113] 2024-01-09 23:43:10,936 >> Configuration saved in ./ft-summarization/checkpoint-26919/gaudi_config.json\n",
      "[2024-01-09 23:43:10,939] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step26919 is about to be saved!\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1882: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-01-09 23:43:10,959] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./ft-summarization/checkpoint-26919/global_step26919/mp_rank_00_model_states.pt\n",
      "[2024-01-09 23:43:10,959] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-26919/global_step26919/mp_rank_00_model_states.pt...\n",
      "[2024-01-09 23:43:11,261] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./ft-summarization/checkpoint-26919/global_step26919/mp_rank_00_model_states.pt.\n",
      "[2024-01-09 23:43:11,281] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-26919/global_step26919/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-01-09 23:43:11,419] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./ft-summarization/checkpoint-26919/global_step26919/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-01-09 23:43:11,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-26919/global_step26919/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-01-09 23:43:12,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./ft-summarization/checkpoint-26919/global_step26919/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-01-09 23:43:12,944] [INFO] [engine.py:3464:_save_zero_checkpoint] zero checkpoint saved ./ft-summarization/checkpoint-26919/global_step26919/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-01-09 23:43:12,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26919 is ready now!\n",
      "[INFO|trainer.py:945] 2024-01-09 23:43:13,200 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1949.2719, 'train_samples_per_second': 442.871, 'train_steps_per_second': 13.841, 'train_loss': 1.0572169071595574, 'epoch': 3.0, 'memory_allocated (GB)': 3.94, 'max_memory_allocated (GB)': 4.08, 'total_memory_available (GB)': 94.62}\n",
      "100%|█████████████████████████████████████| 26919/26919 [32:29<00:00, 13.81it/s]\n",
      "[INFO|trainer.py:1406] 2024-01-09 23:43:13,287 >> Saving model checkpoint to ./ft-summarization\n",
      "[INFO|configuration_utils.py:460] 2024-01-09 23:43:13,288 >> Configuration saved in ./ft-summarization/config.json\n",
      "[INFO|configuration_utils.py:544] 2024-01-09 23:43:13,289 >> Configuration saved in ./ft-summarization/generation_config.json\n",
      "[INFO|modeling_utils.py:2118] 2024-01-09 23:43:13,525 >> Model weights saved in ./ft-summarization/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2420] 2024-01-09 23:43:13,528 >> tokenizer config file saved in ./ft-summarization/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2429] 2024-01-09 23:43:13,528 >> Special tokens file saved in ./ft-summarization/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:189] 2024-01-09 23:43:13,529 >> Copy vocab file to ./ft-summarization/spiece.model\n",
      "[INFO|configuration_utils.py:113] 2024-01-09 23:43:13,538 >> Configuration saved in ./ft-summarization/gaudi_config.json\n",
      "***** train metrics *****\n",
      "  epoch                       =        3.0\n",
      "  max_memory_allocated (GB)   =       4.08\n",
      "  memory_allocated (GB)       =       3.94\n",
      "  total_memory_available (GB) =      94.62\n",
      "  train_loss                  =     1.0572\n",
      "  train_runtime               = 0:32:29.27\n",
      "  train_samples               =     287113\n",
      "  train_samples_per_second    =    442.871\n",
      "  train_steps_per_second      =     13.841\n",
      "[INFO|modelcard.py:452] 2024-01-09 23:43:13,841 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}, 'dataset': {'name': 'cnn_dailymail 3.0.0', 'type': 'cnn_dailymail', 'config': '3.0.0', 'split': 'train', 'args': '3.0.0'}}\n",
      "[2024-01-09 23:43:20,713] [INFO] [launch.py:348:main] Process 10112 exits successfully.\n",
      "[2024-01-09 23:43:20,713] [INFO] [launch.py:348:main] Process 10110 exits successfully.\n",
      "[2024-01-09 23:43:20,713] [INFO] [launch.py:348:main] Process 10114 exits successfully.\n",
      "[2024-01-09 23:43:20,713] [INFO] [launch.py:348:main] Process 10111 exits successfully.\n",
      "[2024-01-09 23:43:20,714] [INFO] [launch.py:348:main] Process 10178 exits successfully.\n",
      "[2024-01-09 23:43:20,714] [INFO] [launch.py:348:main] Process 10109 exits successfully.\n",
      "[2024-01-09 23:43:20,714] [INFO] [launch.py:348:main] Process 10113 exits successfully.\n",
      "[2024-01-09 23:43:21,715] [INFO] [launch.py:348:main] Process 10108 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!python ../gaudi_spawn.py \\\n",
    "--world_size 8 --use_deepspeed run_summarization.py \\\n",
    "--model_name_or_path t5-small \\\n",
    "--do_train \\\n",
    "--dataset_name cnn_dailymail \\\n",
    "--dataset_config '\"3.0.0\"' \\\n",
    "--source_prefix '\"summarize: \"' \\\n",
    "--output_dir ./ft-summarization \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--per_device_eval_batch_size 4 \\\n",
    "--overwrite_output_dir \\\n",
    "--predict_with_generate \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs_for_training \\\n",
    "--gaudi_config_name Habana/t5 \\\n",
    "--ignore_pad_token_for_loss False \\\n",
    "--pad_to_max_length \\\n",
    "--save_strategy epoch \\\n",
    "--report_to none \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--deepspeed ./ds_config.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f93c0b-04bb-4fba-adb8-e18af01fd324",
   "metadata": {},
   "source": [
    "### After fine tuning, let's look at the results\n",
    "This fine tuned model has created the new `pytorch_model.bin` and the global_step.. folder contains the checkpoints that will be used in the infernece in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f34cdf3d-118e-4f80-a83d-2237c52f19c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization/ft-summarization\n"
     ]
    }
   ],
   "source": [
    "%cd ./ft-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b035a826-201f-4a42-9222-5a9639cc44ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 217852\n",
      "drwxr-xr-x 5 root root      4096 Jan  9 23:43 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 4 root root      4096 Jan  9 23:10 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root       314 Jan  9 23:43 all_results.json\n",
      "drwxr-xr-x 3 root root      4096 Jan  9 23:32 \u001b[01;34mcheckpoint-17946\u001b[0m/\n",
      "drwxr-xr-x 3 root root      4096 Jan  9 23:43 \u001b[01;34mcheckpoint-26919\u001b[0m/\n",
      "drwxr-xr-x 3 root root      4096 Jan  9 23:21 \u001b[01;34mcheckpoint-8973\u001b[0m/\n",
      "-rw-r--r-- 1 root root      1503 Jan  9 23:43 config.json\n",
      "-rw-r--r-- 1 root root       247 Jan  9 23:43 gaudi_config.json\n",
      "-rw-r--r-- 1 root root       358 Jan  9 23:43 generation_config.json\n",
      "-rw-r--r-- 1 root root 219754786 Jan  9 23:43 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root      1182 Jan  9 23:43 README.md\n",
      "-rw-r--r-- 1 root root      2201 Jan  9 23:43 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root    791656 Jan  9 23:43 spiece.model\n",
      "-rw-r--r-- 1 root root     20746 Jan  9 23:43 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root   2422388 Jan  9 23:43 tokenizer.json\n",
      "-rw-r--r-- 1 root root     13724 Jan  9 23:43 trainer_state.json\n",
      "-rw-r--r-- 1 root root      7928 Jan  9 23:43 training_args.bin\n",
      "-rw-r--r-- 1 root root       314 Jan  9 23:43 train_results.json\n"
     ]
    }
   ],
   "source": [
    "%ls -al"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f75cd58-cca9-4152-889a-75fbfdc24b07",
   "metadata": {},
   "source": [
    "#### Summarization using the Pipeline\n",
    "Now we can run the summarization using Hugging Face Pipeline call with the fine tuned model.  In this case we will point to the model that we fine tuned.   Remember that if you used t5-small to do the Fine Tuning, be sure to change the `model_to_finetune` to \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e145ca22-b2d0-49e5-89ee-0056adf8f54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:252: UserWarning: Device capability of hccl unspecified, assuming `cpu` and `cuda`. Please specify it via the `devices` argument of `register_backend`.\n",
      "  warnings.warn(\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056446876 KB\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Input: summarize: Introduction: The Strategic Arms Limitation Talks II (SALT II) treaty, signed on June 18, 1979, between the United States and the Soviet Union, marked a significant milestone in nuclear arms control efforts during the Cold War era. Building upon its predecessor, SALT I, the treaty aimed to curb the arms race and reduce the risk of nuclear conflict between the superpowers. Key Provisions: SALT II encompassed several crucial provisions. It placed limits on strategic offensive arms, including intercontinental ballistic missiles (ICBMs), submarine-launched ballistic missiles (SLBMs), and heavy bombers. The agreement specified the maximum number of deployed warheads and launchers each party could possess. Verification and Compliance: To ensure compliance, the treaty established comprehensive verification measures. This involved regular exchanges of data, on-site inspections, and monitoring activities by both nations. These measures sought to enhance transparency, foster trust, and prevent either side from gaining a significant advantage in terms of strategic nuclear capabilities. Ratification and Challenges: Although both the United States and the Soviet Union signed the treaty, its ratification faced considerable challenges. The political landscape changed when the Soviet Union invaded Afghanistan in 1979, leading to a deterioration of U.S.-Soviet relations. As a result, the United States never ratified the treaty formally, rendering it non-binding. However, both nations pledged to adhere to its principles, effectively implementing its provisions on a voluntary basis. Legacy and Impact: Despite the treaty's non-ratification, SALT II's legacy and impact were significant. It set the stage for subsequent arms control negotiations, providing a framework for future agreements such as the Intermediate-Range Nuclear Forces (INF) Treaty and the Strategic Arms Reduction Treaty (START). SALT II demonstrated the potential for cooperation between the superpowers and laid the groundwork for continued dialogue aimed at reducing the nuclear threat globally.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Result: [{'summary_text': 'The Strategic Arms Limitation Talks II (SALT II) treaty signed on June 18, 1979 . It aimed to curb the arms race and reduce the risk of conflict between the superpowers . The agreement specified the maximum number of deployed warheads and launchers .'}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load model to fine-tune and its tokenizer\n",
    "model_to_finetune = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_to_finetune)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_finetune)\n",
    "\n",
    "# Point to the ft-summarization folder with the fine-tuned model\n",
    "path_to_local_model = \"/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization/ft-summarization\"\n",
    "\n",
    "# Instantiate pipeline from local repo, if you did not run the fine tuning step above, you can change: model=model_to_finetune\n",
    "pipe = pipeline(task=\"summarization\", model=path_to_local_model, device=\"hpu\", torch_dtype=torch.bfloat16, min_length=50, max_length=150)\n",
    "\n",
    "\n",
    "#text_to_summarize = \"summarize: Photosynthesis involves a series of complex reactions that take place within specialized organelles called chloroplasts in plant cells. It can be broadly divided into two stages: the light-dependent reactions and the light-independent reactions, also known as the Calvin cycle.  Light-Dependent Reactions: During the light-dependent reactions, chlorophyll pigments within the thylakoid membranes of the chloroplasts absorb light energy. This energy is utilized to split water molecules into oxygen, protons (H+), and electrons. Oxygen is released as a byproduct, while protons and electrons are transported through an electron transport chain, generating ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate).  Light-Independent Reactions (Calvin Cycle):  The ATP and NADPH produced in the light-dependent reactions are utilized in the Calvin cycle, which takes place in the stroma of the chloroplasts. In this cycle, carbon dioxide from the atmosphere combines with the stored energy in the form of ATP and NADPH to produce glucose. This glucose serves as a building block for other carbohydrates and organic compounds. Photosynthesis is a complex process that enables plants, algae, and some bacteria to convert light energy into chemical energy, facilitating the sustenance of life on Earth. It involves the interplay of light-dependent reactions, which generate ATP and NADPH, and the light-independent reactions or the Calvin cycle, which utilize the produced energy to fix carbon dioxide and produce glucose. Enhancing our understanding of photosynthesis and its underlying mechanisms holds the key to various applications, including improving crop yields, developing sustainable bioenergy sources, and addressing environmental challenges.\"\n",
    "text_to_summarize = \"summarize: Introduction: The Strategic Arms Limitation Talks II (SALT II) treaty, signed on June 18, 1979, between the United States and the Soviet Union, marked a significant milestone in nuclear arms control efforts during the Cold War era. Building upon its predecessor, SALT I, the treaty aimed to curb the arms race and reduce the risk of nuclear conflict between the superpowers. Key Provisions: SALT II encompassed several crucial provisions. It placed limits on strategic offensive arms, including intercontinental ballistic missiles (ICBMs), submarine-launched ballistic missiles (SLBMs), and heavy bombers. The agreement specified the maximum number of deployed warheads and launchers each party could possess. Verification and Compliance: To ensure compliance, the treaty established comprehensive verification measures. This involved regular exchanges of data, on-site inspections, and monitoring activities by both nations. These measures sought to enhance transparency, foster trust, and prevent either side from gaining a significant advantage in terms of strategic nuclear capabilities. Ratification and Challenges: Although both the United States and the Soviet Union signed the treaty, its ratification faced considerable challenges. The political landscape changed when the Soviet Union invaded Afghanistan in 1979, leading to a deterioration of U.S.-Soviet relations. As a result, the United States never ratified the treaty formally, rendering it non-binding. However, both nations pledged to adhere to its principles, effectively implementing its provisions on a voluntary basis. Legacy and Impact: Despite the treaty's non-ratification, SALT II's legacy and impact were significant. It set the stage for subsequent arms control negotiations, providing a framework for future agreements such as the Intermediate-Range Nuclear Forces (INF) Treaty and the Strategic Arms Reduction Treaty (START). SALT II demonstrated the potential for cooperation between the superpowers and laid the groundwork for continued dialogue aimed at reducing the nuclear threat globally.\"\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Input:\", text_to_summarize)\n",
    "print()\n",
    "\n",
    "result = pipe(text_to_summarize)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Result:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21e4d82-70a4-4de6-855b-1d3b906c538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run additional inference examples, the jupyter notebook requires that the kernel be restarted.  this `exit()` command will restart the kernel and allow another infernece run.\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5c8f0-6622-4823-943c-dc7a633f75fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

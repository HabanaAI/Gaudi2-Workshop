{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6ae3240-aecb-42ae-8e84-4c60c351590c",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b88e7ae8-3888-4cd4-b61b-9a48488e7d01",
   "metadata": {},
   "source": [
    "# Summarization using T5 model from Hugging Face"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "459dcf44-3d9c-4e51-8032-0a5ceb5f3078",
   "metadata": {},
   "source": [
    "### Summarization with T5-3B model\n",
    "We will use the Hugging Face Summariazion example with the T5-3B model to fine tune the model with the CNN-dailymail dataset\n",
    "\n",
    "run_summarization.py is a lightweight example of how to download and preprocess a dataset from the 🤗 Datasets library "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ce72d9-014e-4f37-aad1-8558b8a1839f",
   "metadata": {},
   "source": [
    "#### Initial Setup\n",
    "We start with a Habana PyTorch Docker image and run this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4a58fe9-01fd-412a-84a4-420b76da5d50",
   "metadata": {},
   "source": [
    "#### Install Habana's DeepSpeed Fork\n",
    "Habana's DeepSpeed Fork has implementations specifically for Gaudi and must be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75951895-150d-42d6-a499-1dcf3ec51576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HabanaAI/DeepSpeed.git@1.10.0\n",
      "  Cloning https://github.com/HabanaAI/DeepSpeed.git (to revision 1.10.0) to /tmp/pip-req-build-83mxv2lp\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/DeepSpeed.git /tmp/pip-req-build-83mxv2lp\n",
      "  Running command git checkout -b 1.10.0 --track origin/1.10.0\n",
      "  Switched to a new branch '1.10.0'\n",
      "  Branch '1.10.0' set up to track remote branch '1.10.0' from 'origin'.\n",
      "  Resolved https://github.com/HabanaAI/DeepSpeed.git to commit 141faf783dac331bc3852590d05190dc0e883e51\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ninja in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.10.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (5.9.5)\n",
      "Collecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.10.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (2.0.1a0+git37b7ddc)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic->deepspeed==0.7.7+hpu.synapse.v1.10.0) (4.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (3.12.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (3.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.3.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.7.7+hpu.synapse.v1.10.0-py3-none-any.whl size=738067 sha256=e3f5181280755acba4d3a6bf14527769d1e19c29278ccb4ab3a1b0b0ec4d2958\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ggfes22l/wheels/a1/1d/50/40a4b5d2dcc3710881387fe9c3712cfbc73f4b77a513f3cea6\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, hjson, pydantic, deepspeed\n",
      "Successfully installed deepspeed-0.7.7+hpu.synapse.v1.10.0 hjson-3.1.0 py-cpuinfo-9.0.0 pydantic-1.10.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.10.0  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b632625f-98e3-4be3-87a2-90f39af24e34",
   "metadata": {},
   "source": [
    "#### Install the Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0391992b-28d5-4a71-9eaa-b384aedb158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum[habana]\n",
      "  Downloading optimum-1.8.7.tar.gz (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.9/243.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (0.15.1a0+42759b1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (23.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.23.5)\n",
      "Collecting huggingface-hub>=0.8.0\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]<4.30.0,>=4.26.0\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.12)\n",
      "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (2.0.1a0+git37b7ddc)\n",
      "Collecting optimum-habana\n",
      "  Downloading optimum_habana-1.5.1-py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m326.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers<4.29.0\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (3.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2.31.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.29.0->optimum[habana]) (2020.10.28)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]<4.30.0,>=4.26.0\n",
      "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers<4.29.0->optimum[habana]) (3.19.5)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m321.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m346.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (1.4.1)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m344.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (3.8.4)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m391.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diffusers>=0.12.0\n",
      "  Downloading diffusers-0.17.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum[habana]) (1.3.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers>=0.12.0->optimum-habana->optimum[habana]) (6.6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (3.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->optimum-habana->optimum[habana]) (5.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.9->optimum[habana]) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum[habana]) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers>=0.12.0->optimum-habana->optimum[habana]) (3.15.0)\n",
      "Building wheels for collected packages: optimum\n",
      "  Building wheel for optimum (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum: filename=optimum-1.8.7-py3-none-any.whl size=318191 sha256=d0d8525cd468d7169353fddab117f59f8d3a16cc19e22c320fd0d7139e69ced9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zzio70ky/wheels/ba/76/56/1f4416526a025b3e4116c0866e68ff6dc0ab81ad44fac327d8\n",
      "Successfully built optimum\n",
      "Installing collected packages: tokenizers, sentencepiece, xxhash, pyarrow, pillow, humanfriendly, dill, multiprocess, huggingface-hub, coloredlogs, transformers, diffusers, accelerate, datasets, optimum, optimum-habana\n",
      "Successfully installed accelerate-0.20.3 coloredlogs-15.0.1 datasets-2.13.0 diffusers-0.17.1 dill-0.3.6 huggingface-hub-0.15.1 humanfriendly-10.0 multiprocess-0.70.14 optimum-1.8.7 optimum-habana-1.5.1 pillow-9.5.0 pyarrow-12.0.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install optimum[habana]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81f3f414-2d51-49ff-b12f-f5f3f521459c",
   "metadata": {},
   "source": [
    "#### Clone the Hugging Face Model Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbe4154-8b24-47f5-a431-da74635e701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n",
      "remote: Enumerating objects: 4007, done.\u001b[K\n",
      "remote: Counting objects: 100% (1160/1160), done.\u001b[K\n",
      "remote: Compressing objects: 100% (512/512), done.\u001b[K\n",
      "remote: Total 4007 (delta 791), reused 870 (delta 612), pack-reused 2847\u001b[K\n",
      "Receiving objects: 100% (4007/4007), 2.08 MiB | 5.25 MiB/s, done.\n",
      "Resolving deltas: 100% (2593/2593), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone  https://github.com/huggingface/optimum-habana"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c85e29e-30be-4288-801d-4e819b49b748",
   "metadata": {},
   "source": [
    "#### Go the Summarization example model and install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e27db3ec-af54-41f3-81f7-4c0d4d54a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization\n"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab13f8a-5cb6-4676-b2db-52bbb6aff816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "455c3b87-a11b-44ff-85f9-db5daa9f5d78",
   "metadata": {},
   "source": [
    "### Setup for DeepSpeed\n",
    "Since we are using DeepSpeed, we have to confirm that the model has been configured properly.  We look for the following:\n",
    "\n",
    "* model, optimizer, ... = deepspeed.initialize(args=args, model=model, optimizer=optimizer, ...)\n",
    "* deepspeed.init_distributed(dist_backend=“hccl”, init_method=init_method)\n",
    "* Create a ds_config.json file to set the DS training parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b622e99-4b35-4ee9-b9b0-903524fc6ce1",
   "metadata": {},
   "source": [
    "#### DeepSpeed Initialization\n",
    "Look in deepspeed.py and we see the model being passed to the DeepSpeed engine\n",
    "\n",
    "```\n",
    "    import deepspeed\n",
    "    from deepspeed.utils import logger as ds_logger\n",
    "\n",
    "    model = trainer.model\n",
    "    args = trainer.args\n",
    "    ...\n",
    "\n",
    "    kwargs = {\n",
    "        \"args\": habana_args,\n",
    "        \"model\": model,\n",
    "        \"model_parameters\": model_parameters,\n",
    "        \"config_params\": config,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_scheduler\": lr_scheduler,\n",
    "    }\n",
    "\n",
    "    deepspeedengine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "146c64e9-b31f-4d84-ad0f-ee48d05cd34f",
   "metadata": {},
   "source": [
    "#### DeepSpeed Distributed\n",
    "Look in training_args.py and we see the DeepSpeed Distribution initialization\n",
    "\n",
    "```\n",
    "    from habana_frameworks.torch.distributed.hccl import initialize_distributed_hpu\n",
    "    world_size, rank, self.local_rank = initialize_distributed_hpu()\n",
    "\n",
    "    import deepspeed\n",
    "    deepspeed.init_distributed(dist_backend=\"hccl\", timeout=timedelta(seconds=self.ddp_timeout))\n",
    "       logger.info(\"DeepSpeed is enabled.\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4454d898-f2ff-4e6f-9171-db5f8bfc07f3",
   "metadata": {},
   "source": [
    "#### Create DeepSpeed Config file with ZeRO preferences\n",
    "The ds_config.json file will configure the parameters to run DeepSpeed\n",
    "\n",
    "In this case, we will run the ZeRO3 optimizer and BF16 mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "779b48d2-a098-424d-8756-3362ffa74cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b9f3bf-d0f1-4787-8d7c-5b9fde57f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"steps_per_print\": 64,\n",
      "    \"train_batch_size\": \"auto\",\n",
      "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
      "    \"gradient_accumulation_steps\": \"auto\",\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    },\n",
      "    \"gradient_clipping\": 1.0,\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3,\n",
      "        \"overlap_comm\": false,\n",
      "        \"reduce_scatter\": false,\n",
      "        \"contiguous_gradients\": false,\n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tee ./ds_config.json <<EOF\n",
    "{\n",
    "    \"steps_per_print\": 64,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"overlap_comm\": false,\n",
    "        \"reduce_scatter\": false,\n",
    "        \"contiguous_gradients\": false,\n",
    "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "    }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f8d68d-53ab-452d-a1cc-d93038951609",
   "metadata": {},
   "source": [
    "#### Fine Tuning T5-3b with the cnn_dailymail dataset\n",
    "The T5-3b model is a large language model that was originally trained on the C4 dataset and in this case will be fined tuned on the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset that is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail.\n",
    "\n",
    "For use of this model on First-Gen Gaudi, users should update the model to \"T5-large\"\n",
    "\n",
    "This is run by `gaudi_spawn.py`, a simple launcher script to collect arguments and send them to `distributed_runner.py` for training on multiple HPUs, which then calls the `run_summarization.py` model.\n",
    "\n",
    "Notice the Habana specific commands to use here:\n",
    "\n",
    "-- use_habana  - allows training to run on Habana Gaudi  \n",
    "-- use_hpu_graphs - reduces recompilation by replaying the graph  \n",
    "-- gaudi_config_name Habana/t5 - mapping to Hugging Face T5 Model  \n",
    "\n",
    "**Even though a Billion parameter T5 model can be used for Fine Tuning, this fine tuning still takes many hours to complete.  \n",
    "For users that wish to execute the example Fine Tuning, they should modify the `model_name_or_path` to \"t5-small\", which only takes a few hours to complete.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "565cc518-a016-4dc3-9b9c-07f749106c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3707f195-b154-4612-a210-2956a7ed2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ft-summarization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c5fbafb-c218-43bd-bc6f-0d993f0858f8",
   "metadata": {},
   "source": [
    "**Run the command below in a terminal window** from the optimum-habana/examples/summarization folder, this is most likely here: \n",
    "/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization/\n",
    "\n",
    "```\n",
    "python ../gaudi_spawn.py \\\n",
    "--world_size 8 --use_deepspeed run_summarization.py \\\n",
    "--model_name_or_path t5-3b \\\n",
    "--do_train \\\n",
    "--dataset_name cnn_dailymail \\\n",
    "--dataset_config '\"3.0.0\"' \\\n",
    "--source_prefix '\"summarize: \"' \\\n",
    "--output_dir ./ft-summarization \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--per_device_eval_batch_size 4 \\\n",
    "--overwrite_output_dir \\\n",
    "--predict_with_generate \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs \\\n",
    "--gaudi_config_name Habana/t5 \\\n",
    "--ignore_pad_token_for_loss False \\\n",
    "--pad_to_max_length \\\n",
    "--save_strategy epoch \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--deepspeed ./ds_config.json\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f93c0b-04bb-4fba-adb8-e18af01fd324",
   "metadata": {},
   "source": [
    "### After fine tuning, let's look at the results\n",
    "This fine tuned model has created the new `pytorch_model.bin` and the global_step.. folder contains the checkpoints that will be used in the infernece in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f34cdf3d-118e-4f80-a83d-2237c52f19c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization/ft-summarization\n"
     ]
    }
   ],
   "source": [
    "%cd ./ft-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b035a826-201f-4a42-9222-5a9639cc44ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 217716\n",
      "drwxr-xr-x 6 root root      4096 Jun 14 18:44 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 5 root root      4096 Jun 14 06:58 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root       312 Jun 14 18:44 all_results.json\n",
      "drwxr-xr-x 3 root root      4096 Jun 14 17:48 \u001b[01;34mcheckpoint-17946\u001b[0m/\n",
      "drwxr-xr-x 3 root root      4096 Jun 14 18:44 \u001b[01;34mcheckpoint-26919\u001b[0m/\n",
      "drwxr-xr-x 3 root root      4096 Jun 14 16:53 \u001b[01;34mcheckpoint-8973\u001b[0m/\n",
      "-rw-r--r-- 1 root root      1474 Jun 14 18:44 config.json\n",
      "-rw-r--r-- 1 root root       506 Jun 14 18:44 gaudi_config.json\n",
      "-rw-r--r-- 1 root root       142 Jun 14 18:44 generation_config.json\n",
      "-rw-r--r-- 1 root root 219639653 Jun 14 18:44 pytorch_model.bin\n",
      "drwxr-xr-x 3 root root      4096 Jun 14 15:56 \u001b[01;34mruns\u001b[0m/\n",
      "-rw-r--r-- 1 root root      2201 Jun 14 18:44 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root    791656 Jun 14 18:44 spiece.model\n",
      "-rw-r--r-- 1 root root      2324 Jun 14 18:44 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root   2422095 Jun 14 18:44 tokenizer.json\n",
      "-rw-r--r-- 1 root root     13541 Jun 14 18:44 trainer_state.json\n",
      "-rw-r--r-- 1 root root      4475 Jun 14 18:44 training_args.bin\n",
      "-rw-r--r-- 1 root root       312 Jun 14 18:44 train_results.json\n"
     ]
    }
   ],
   "source": [
    "%ls -al"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f75cd58-cca9-4152-889a-75fbfdc24b07",
   "metadata": {},
   "source": [
    "#### Summarization using the Pipeline\n",
    "Now we can run the summarization using Hugging Face Pipeline call with the fine tuned model.  In this case we will point to the model that we fine tuned.   Remember that if you used t5-small to do the Fine Tuning, be sure to change the `model_to_finetune` to \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e145ca22-b2d0-49e5-89ee-0056adf8f54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84338d41b464ba9a28abf71fc9c44ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cc9dd80f2340b9ba14eb79b84cfee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/11.4G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bf36526c1447a3a861537b6bf8cd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e0a314072d4c1581fb2530a7ea49d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 160\n",
      "CPU RAM = 1056427508 KB \n",
      "============================================================================================ \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Input: Photosynthesis involves a series of complex reactions that take place within specialized organelles called chloroplasts in plant cells. It can be broadly divided into two stages: the light-dependent reactions and the light-independent reactions, also known as the Calvin cycle.  Light-Dependent Reactions: During the light-dependent reactions, chlorophyll pigments within the thylakoid membranes of the chloroplasts absorb light energy. This energy is utilized to split water molecules into oxygen, protons (H+), and electrons. Oxygen is released as a byproduct, while protons and electrons are transported through an electron transport chain, generating ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate).  Light-Independent Reactions (Calvin Cycle):  The ATP and NADPH produced in the light-dependent reactions are utilized in the Calvin cycle, which takes place in the stroma of the chloroplasts. In this cycle, carbon dioxide from the atmosphere combines with the stored energy in the form of ATP and NADPH to produce glucose. This glucose serves as a building block for other carbohydrates and organic compounds. Photosynthesis is a complex process that enables plants, algae, and some bacteria to convert light energy into chemical energy, facilitating the sustenance of life on Earth. It involves the interplay of light-dependent reactions, which generate ATP and NADPH, and the light-independent reactions or the Calvin cycle, which utilize the produced energy to fix carbon dioxide and produce glucose. Enhancing our understanding of photosynthesis and its underlying mechanisms holds the key to various applications, including improving crop yields, developing sustainable bioenergy sources, and addressing environmental challenges.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Result: [{'summary_text': 'Photosynthesis is a complex process that enables plants, algae, and some bacteria to convert light energy into chemical energy . It involves the interplay of light-dependent reactions, which generate ATP and NADPH, and the light-independent reactions or the Calvin cycle . Enhancing our understanding of photosynthesis holds the key to various applications, including improving crop yields and developing sustainable bioenergy sources .'}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load model to fine-tune and its tokenizer\n",
    "model_to_finetune = \"t5-3b\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_to_finetune)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_finetune)\n",
    "\n",
    "# Point to the ft-summarization folder with the fine-tuned model\n",
    "path_to_local_model = \"/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization/ft-summarization\"\n",
    "\n",
    "# Instantiate pipeline from local repo, if you did not run the fine tuning step above, you can change: model=model_to_finetune\n",
    "pipe = pipeline(task=\"summarization\", model=path_to_local_model, device=\"hpu\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "text_to_summarize = \"summarize: Photosynthesis involves a series of complex reactions that take place within specialized organelles called chloroplasts in plant cells. It can be broadly divided into two stages: the light-dependent reactions and the light-independent reactions, also known as the Calvin cycle.  Light-Dependent Reactions: During the light-dependent reactions, chlorophyll pigments within the thylakoid membranes of the chloroplasts absorb light energy. This energy is utilized to split water molecules into oxygen, protons (H+), and electrons. Oxygen is released as a byproduct, while protons and electrons are transported through an electron transport chain, generating ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate).  Light-Independent Reactions (Calvin Cycle):  The ATP and NADPH produced in the light-dependent reactions are utilized in the Calvin cycle, which takes place in the stroma of the chloroplasts. In this cycle, carbon dioxide from the atmosphere combines with the stored energy in the form of ATP and NADPH to produce glucose. This glucose serves as a building block for other carbohydrates and organic compounds. Photosynthesis is a complex process that enables plants, algae, and some bacteria to convert light energy into chemical energy, facilitating the sustenance of life on Earth. It involves the interplay of light-dependent reactions, which generate ATP and NADPH, and the light-independent reactions or the Calvin cycle, which utilize the produced energy to fix carbon dioxide and produce glucose. Enhancing our understanding of photosynthesis and its underlying mechanisms holds the key to various applications, including improving crop yields, developing sustainable bioenergy sources, and addressing environmental challenges.\"\n",
    "#text_to_summarize = \"summarize: Introduction: The Strategic Arms Limitation Talks II (SALT II) treaty, signed on June 18, 1979, between the United States and the Soviet Union, marked a significant milestone in nuclear arms control efforts during the Cold War era. Building upon its predecessor, SALT I, the treaty aimed to curb the arms race and reduce the risk of nuclear conflict between the superpowers. Key Provisions: SALT II encompassed several crucial provisions. It placed limits on strategic offensive arms, including intercontinental ballistic missiles (ICBMs), submarine-launched ballistic missiles (SLBMs), and heavy bombers. The agreement specified the maximum number of deployed warheads and launchers each party could possess. Verification and Compliance: To ensure compliance, the treaty established comprehensive verification measures. This involved regular exchanges of data, on-site inspections, and monitoring activities by both nations. These measures sought to enhance transparency, foster trust, and prevent either side from gaining a significant advantage in terms of strategic nuclear capabilities. Ratification and Challenges: Although both the United States and the Soviet Union signed the treaty, its ratification faced considerable challenges. The political landscape changed when the Soviet Union invaded Afghanistan in 1979, leading to a deterioration of U.S.-Soviet relations. As a result, the United States never ratified the treaty formally, rendering it non-binding. However, both nations pledged to adhere to its principles, effectively implementing its provisions on a voluntary basis. Legacy and Impact: Despite the treaty's non-ratification, SALT II's legacy and impact were significant. It set the stage for subsequent arms control negotiations, providing a framework for future agreements such as the Intermediate-Range Nuclear Forces (INF) Treaty and the Strategic Arms Reduction Treaty (START). SALT II demonstrated the potential for cooperation between the superpowers and laid the groundwork for continued dialogue aimed at reducing the nuclear threat globally.\"\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Input:\", text_to_summarize)\n",
    "print()\n",
    "\n",
    "result = pipe(text_to_summarize)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Result:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a21e4d82-70a4-4de6-855b-1d3b906c538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run additional inference examples, the jupyter notebook requires that the kernel be restarted.  this `exit()` command will restart the kernel and allow another infernece run.\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b88e7ae8-3888-4cd4-b61b-9a48488e7d01",
   "metadata": {},
   "source": [
    "# Hugging Face Optimum Habana Large Language Model Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "459dcf44-3d9c-4e51-8032-0a5ceb5f3078",
   "metadata": {},
   "source": [
    "### Summarization with T5-3B model\n",
    "we will use the Hugging Face Summariazion example with the T531B model to fine tune with TBD Dataset\n",
    "\n",
    "run_summarization.py is a lightweight example of how to download and preprocess a dataset from the ü§ó Datasets library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ce72d9-014e-4f37-aad1-8558b8a1839f",
   "metadata": {},
   "source": [
    "#### Initial Setup\n",
    "we start with a Habana PyTorch Docker image and run this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4a58fe9-01fd-412a-84a4-420b76da5d50",
   "metadata": {},
   "source": [
    "#### Install Habana's DeepSpeed Fork\n",
    "Habana's DeepSpeed Fork has implementations specifically for Gaudi and must be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75951895-150d-42d6-a499-1dcf3ec51576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HabanaAI/DeepSpeed.git@1.10.0\n",
      "  Cloning https://github.com/HabanaAI/DeepSpeed.git (to revision 1.10.0) to /tmp/pip-req-build-nxc0q7t7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/DeepSpeed.git /tmp/pip-req-build-nxc0q7t7\n",
      "  Running command git checkout -b 1.10.0 --track origin/1.10.0\n",
      "  Switched to a new branch '1.10.0'\n",
      "  Branch '1.10.0' set up to track remote branch '1.10.0' from 'origin'.\n",
      "  Resolved https://github.com/HabanaAI/DeepSpeed.git to commit ebed48dcdec0b20602af5097182f68c60bdbddaf\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: hjson in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (3.1.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.10.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.10.9)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (2.0.1a0+git37b7ddc)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic->deepspeed==0.7.7+hpu.synapse.v1.10.0) (4.6.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (3.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (3.12.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.10.0  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b632625f-98e3-4be3-87a2-90f39af24e34",
   "metadata": {},
   "source": [
    "#### Install the Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0391992b-28d5-4a71-9eaa-b384aedb158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum[habana] in /usr/local/lib/python3.8/dist-packages (1.8.6)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (2.12.0)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (15.0.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (4.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (0.15.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (23.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (0.15.1a0+42759b1)\n",
      "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (2.0.1a0+git37b7ddc)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.23.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.12)\n",
      "Requirement already satisfied: optimum-habana in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.6.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2.31.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2023.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (3.12.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (2023.6.3)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (3.20.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.26.0->optimum[habana]) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum[habana]) (10.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (3.8.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (1.4.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (12.0.0)\n",
      "Requirement already satisfied: diffusers>=0.12.0 in /usr/local/lib/python3.8/dist-packages (from optimum-habana->optimum[habana]) (0.12.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (from optimum-habana->optimum[habana]) (0.20.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum[habana]) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->optimum[habana]) (9.5.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers>=0.12.0->optimum-habana->optimum[habana]) (6.6.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (3.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (23.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (3.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->optimum-habana->optimum[habana]) (5.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.9->optimum[habana]) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum[habana]) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers>=0.12.0->optimum-habana->optimum[habana]) (3.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install optimum[habana]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81f3f414-2d51-49ff-b12f-f5f3f521459c",
   "metadata": {},
   "source": [
    "#### Clone the Hugging Face Model Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbe4154-8b24-47f5-a431-da74635e701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'optimum-habana' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone  https://github.com/huggingface/optimum-habana"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c85e29e-30be-4288-801d-4e819b49b748",
   "metadata": {},
   "source": [
    "#### Go the Summarization example model and install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e27db3ec-af54-41f3-81f7-4c0d4d54a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/work/hf_examples/optimum-habana/examples/summarization\n"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab13f8a-5cb6-4676-b2db-52bbb6aff816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (2.12.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (3.20.2)\n",
      "Requirement already satisfied: rouge-score in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (3.8.1)\n",
      "Requirement already satisfied: py7zr in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (0.20.5)\n",
      "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (2.0.1a0+git37b7ddc)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (2023.5.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (12.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.15.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (3.8.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge-score->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge-score->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->-r requirements.txt (line 5)) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->-r requirements.txt (line 5)) (2023.6.3)\n",
      "Requirement already satisfied: texttable in /usr/local/lib/python3.8/dist-packages (from py7zr->-r requirements.txt (line 6)) (1.6.7)\n",
      "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from py7zr->-r requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from py7zr->-r requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from py7zr->-r requirements.txt (line 6)) (5.9.5)\n",
      "Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from py7zr->-r requirements.txt (line 6)) (0.3.1)\n",
      "Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.8/dist-packages (from py7zr->-r requirements.txt (line 6)) (1.0.9)\n",
      "Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.8/dist-packages (from py7zr->-r requirements.txt (line 6)) (0.15.7)\n",
      "Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from py7zr->-r requirements.txt (line 6)) (3.18.0)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.8/dist-packages (from py7zr->-r requirements.txt (line 6)) (0.2.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.12.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (4.6.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (1.12)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (1.9.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=2.4.0->-r requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=2.4.0->-r requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=2.4.0->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 7)) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "455c3b87-a11b-44ff-85f9-db5daa9f5d78",
   "metadata": {},
   "source": [
    "### Setup for DeepSpeed\n",
    "Since we are using DeepSpeed, we have to confirm that the model has been configured properly.  We look for the following:\n",
    "\n",
    "* deepspeed.initialize(model, ...) model, optimizer, ... =¬†deepspeed.initialize(args=args,¬†model=model,¬†optimizer=optimizer, ...)\n",
    "* deepspeed.init_distributed(dist_backend=‚Äúhccl‚Äù, init_method=init_method)\n",
    "* Create a ds_config.json file to set the DS training parameters\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b622e99-4b35-4ee9-b9b0-903524fc6ce1",
   "metadata": {},
   "source": [
    "#### DeepSpeed Initialization\n",
    "Look in deepspeed.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7620afe-25d9-46db-a22d-ceebc178eef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   101\t    import deepspeed\n",
      "   102\t    from deepspeed.utils import logger as ds_logger\n",
      "   103\t\n",
      "   104\t    model = trainer.model\n",
      "   105\t    args = trainer.args\n",
      "   106\t\n",
      "   150\t    kwargs = {\n",
      "   151\t        \"args\": habana_args,\n",
      "   152\t        \"model\": model,\n",
      "   153\t        \"model_parameters\": model_parameters,\n",
      "   154\t        \"config_params\": config,\n",
      "   155\t        \"optimizer\": optimizer,\n",
      "   156\t        \"lr_scheduler\": lr_scheduler,\n",
      "   157\t    }\n",
      "   158\t\n",
      "   159\t    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "   160\t\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cd ../../optimum/habana/transformers\n",
    "cat -n deepspeed.py | head -n 106 | tail -n 6\n",
    "cat -n deepspeed.py | head -n 160 | tail -n 11"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "146c64e9-b31f-4d84-ad0f-ee48d05cd34f",
   "metadata": {},
   "source": [
    "#### DeepSpeed Distrbuted\n",
    "Look in training_args.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f45310-d0f8-416e-ba1b-3ff11ea5d870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   530\t            from habana_frameworks.torch.distributed.hccl import initialize_distributed_hpu\n",
      "   532\t            world_size, rank, self.local_rank = initialize_distributed_hpu()\n",
      "   542\t                    )\n",
      "   543\t                import deepspeed\n",
      "   548\t\n",
      "   549\t                deepspeed.init_distributed(dist_backend=\"hccl\", timeout=timedelta(seconds=self.ddp_timeout))\n",
      "   550\t                logger.info(\"DeepSpeed is enabled.\")\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cd ../../optimum/habana/transformers\n",
    "cat -n training_args.py | head -n 530 | tail -n 1\n",
    "cat -n training_args.py | head -n 532 | tail -n 1\n",
    "cat -n training_args.py | head -n 543 | tail -n 2\n",
    "cat -n training_args.py | head -n 550 | tail -n 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4454d898-f2ff-4e6f-9171-db5f8bfc07f3",
   "metadata": {},
   "source": [
    "#### Create DeepSpeed Config file with ZeRO preferences\n",
    "The ds_config.json file will configure the parameters to run DeepSpeed\n",
    "\n",
    "In this case, we will run the ZeRO3 optimier and BF16 mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b9f3bf-d0f1-4787-8d7c-5b9fde57f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"steps_per_print\": 64,\n",
      "    \"train_batch_size\": \"auto\",\n",
      "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
      "    \"gradient_accumulation_steps\": \"auto\",\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    },\n",
      "    \"gradient_clipping\": 1.0,\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3,\n",
      "        \"overlap_comm\": false,\n",
      "        \"reduce_scatter\": false,\n",
      "        \"contiguous_gradients\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tee ./ds_config.json <<EOF\n",
    "{\n",
    "    \"steps_per_print\": 64,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"overlap_comm\": false,\n",
    "        \"reduce_scatter\": false,\n",
    "        \"contiguous_gradients\": false\n",
    "    }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f8d68d-53ab-452d-a1cc-d93038951609",
   "metadata": {},
   "source": [
    "#### Fine Tuning T5-3b with the cnn_dailymail dataset\n",
    "The T5-3b model is a large language model that was origianlly trained on the C4 dataset and in this case will be fined tuned on the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset that is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail.\n",
    "\n",
    "This is run by `gaudi_spawn.py`, a simple launcher script to collect arguments and send them to `distributed_runner.py` for training on multiple HPUs, which then calls the `run_summarization.py` model.\n",
    "\n",
    "Notice the Habana specific commands to use here:\n",
    "\n",
    "-- use_habana  - allows training to run on Habana Gaudi  \n",
    "-- use_hpu_graphs - reduces recompilation by replaying the graph  \n",
    "-- gaudi_config_name Habana/t5 - mapping to Hugging Face T5 Model  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707f195-b154-4612-a210-2956a7ed2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85af00-f5a3-4d45-a661-53d3ea287c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "mkdir ft-summarization\n",
    "python ../gaudi_spawn.py \\\n",
    "--world_size 8 --use_deepspeed run_summarization.py \\\n",
    "--model_name_or_path t5-3b \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--dataset_name cnn_dailymail \\\n",
    "--dataset_config '\"3.0.0\"' \\\n",
    "--source_prefix '\"summarize: \"' \\\n",
    "--output_dir ./ft-summarization \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--per_device_eval_batch_size 4 \\\n",
    "--overwrite_output_dir \\\n",
    "--predict_with_generate \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs \\\n",
    "--gaudi_config_name Habana/t5 \\\n",
    "--ignore_pad_token_for_loss False \\\n",
    "--pad_to_max_length \\\n",
    "--save_strategy epoch \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--deepspeed ./ds_config.json\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f93c0b-04bb-4fba-adb8-e18af01fd324",
   "metadata": {},
   "source": [
    "### After fine tuning, let's look at the results\n",
    "This fine tuned model has created the new `pytorch_model.bin` and the global_step26919 folder contain the checkpoints that will be used in the infernece in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34cdf3d-118e-4f80-a83d-2237c52f19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /ft-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b035a826-201f-4a42-9222-5a9639cc44ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 37712840\n",
      "drwxr-xr-x 5 root root        4096 Jun  7 21:16 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxrwxr-x 7 1052 1052        4096 Jun  7 22:06 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 1052 1052 38372376576 Jun  7 05:20 \u001b[01;31mcheckpoint-26919.tar.gz\u001b[0m\n",
      "-rw-r--r-- 1 root root        1473 Jun  7 21:35 config.json\n",
      "-rw-r--r-- 1 root root         142 Jun  7 21:35 generation_config.json\n",
      "drwxr-xr-x 2 root root        4096 Jun  5 08:06 \u001b[01;34mglobal_step26919\u001b[0m/\n",
      "drwxr-x--- 2 root root        4096 Jun  7 21:16 \u001b[01;34m.graph_dumps\u001b[0m/\n",
      "-rw-r--r-- 1 root root          16 Jun  5 08:06 latest\n",
      "-rw-r--r-- 1 root root   242069785 Jun  7 21:35 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root       18871 Jun  5 08:06 rng_state_0.pth\n",
      "-rw-r--r-- 1 root root       18871 Jun  5 08:06 rng_state_1.pth\n",
      "-rw-r--r-- 1 root root       18871 Jun  5 08:06 rng_state_3.pth\n",
      "-rw-r--r-- 1 root root       18871 Jun  5 08:06 rng_state_4.pth\n",
      "-rw-r--r-- 1 root root       18871 Jun  5 08:06 rng_state_5.pth\n",
      "-rw-r--r-- 1 root root       18871 Jun  5 08:06 rng_state_6.pth\n",
      "-rw-r--r-- 1 root root       18871 Jun  5 08:06 rng_state_7.pth\n",
      "drwxr-xr-x 3 root root        4096 Jun  7 18:59 \u001b[01;34mroot\u001b[0m/\n",
      "-rw-r--r-- 1 root root        2201 Jun  7 21:35 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root      791656 Jun  7 21:35 spiece.model\n",
      "-rw-r--r-- 1 root root        2324 Jun  7 21:35 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root     2422095 Jun  7 21:35 tokenizer.json\n",
      "-rw-r--r-- 1 root root       13283 Jun  5 08:06 trainer_state.json\n",
      "-rw-r--r-- 1 root root        4603 Jun  5 08:06 training_args.bin\n",
      "-rwxr--r-- 1 root root       19002 Jun  5 08:06 \u001b[01;32mzero_to_fp32.py\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls -al"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f75cd58-cca9-4152-889a-75fbfdc24b07",
   "metadata": {},
   "source": [
    "#### Summarization using the Pipeline\n",
    "Now we can run the summarization using Hugging Face Pipeline call with the fine tuned model.  In this case we will point to the mdoel that we fine tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e145ca22-b2d0-49e5-89ee-0056adf8f54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-3b automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "fopen of /sys/class/accel/accel6 (deleted)/pci_addr failed\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 160\n",
      "CPU RAM = 1056461308 KB \n",
      "============================================================================================ \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Input: summarize: Introduction: The Strategic Arms Limitation Talks II (SALT II) treaty, signed on June 18, 1979, between the United States and the Soviet Union, marked a significant milestone in nuclear arms control efforts during the Cold War era. Building upon its predecessor, SALT I, the treaty aimed to curb the arms race and reduce the risk of nuclear conflict between the superpowers. Key Provisions: SALT II encompassed several crucial provisions. It placed limits on strategic offensive arms, including intercontinental ballistic missiles (ICBMs), submarine-launched ballistic missiles (SLBMs), and heavy bombers. The agreement specified the maximum number of deployed warheads and launchers each party could possess. Verification and Compliance: To ensure compliance, the treaty established comprehensive verification measures. This involved regular exchanges of data, on-site inspections, and monitoring activities by both nations. These measures sought to enhance transparency, foster trust, and prevent either side from gaining a significant advantage in terms of strategic nuclear capabilities. Ratification and Challenges: Although both the United States and the Soviet Union signed the treaty, its ratification faced considerable challenges. The political landscape changed when the Soviet Union invaded Afghanistan in 1979, leading to a deterioration of U.S.-Soviet relations. As a result, the United States never ratified the treaty formally, rendering it non-binding. However, both nations pledged to adhere to its principles, effectively implementing its provisions on a voluntary basis. Legacy and Impact: Despite the treaty's non-ratification, SALT II's legacy and impact were significant. It set the stage for subsequent arms control negotiations, providing a framework for future agreements such as the Intermediate-Range Nuclear Forces (INF) Treaty and the Strategic Arms Reduction Treaty (START). SALT II demonstrated the potential for cooperation between the superpowers and laid the groundwork for continued dialogue aimed at reducing the nuclear threat globally.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Result: [{'summary_text': 'The Strategic Arms Limitation Talks II (SALT II) treaty was signed on June 18, 1979 . It aimed to curb the arms race and reduce the risk of nuclear conflict . The United States never ratified the treaty formally, rendering it non-binding .'}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load model to fine-tune and its tokenizer\n",
    "model_to_finetune = \"t5-3b\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_to_finetune)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_finetune)\n",
    "\n",
    "# Save model and tokenizer on disk\n",
    "path_to_local_model = \"./ft-summarization\"\n",
    "\n",
    "# Instantiate pipeline from local repo\n",
    "pipe = pipeline(task=\"summarization\", model=path_to_local_model, device=\"hpu\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "text_to_summarize = \"summarize: Introduction: The Strategic Arms Limitation Talks II (SALT II) treaty, signed on June 18, 1979, between the United States and the Soviet Union, marked a significant milestone in nuclear arms control efforts during the Cold War era. Building upon its predecessor, SALT I, the treaty aimed to curb the arms race and reduce the risk of nuclear conflict between the superpowers. Key Provisions: SALT II encompassed several crucial provisions. It placed limits on strategic offensive arms, including intercontinental ballistic missiles (ICBMs), submarine-launched ballistic missiles (SLBMs), and heavy bombers. The agreement specified the maximum number of deployed warheads and launchers each party could possess. Verification and Compliance: To ensure compliance, the treaty established comprehensive verification measures. This involved regular exchanges of data, on-site inspections, and monitoring activities by both nations. These measures sought to enhance transparency, foster trust, and prevent either side from gaining a significant advantage in terms of strategic nuclear capabilities. Ratification and Challenges: Although both the United States and the Soviet Union signed the treaty, its ratification faced considerable challenges. The political landscape changed when the Soviet Union invaded Afghanistan in 1979, leading to a deterioration of U.S.-Soviet relations. As a result, the United States never ratified the treaty formally, rendering it non-binding. However, both nations pledged to adhere to its principles, effectively implementing its provisions on a voluntary basis. Legacy and Impact: Despite the treaty's non-ratification, SALT II's legacy and impact were significant. It set the stage for subsequent arms control negotiations, providing a framework for future agreements such as the Intermediate-Range Nuclear Forces (INF) Treaty and the Strategic Arms Reduction Treaty (START). SALT II demonstrated the potential for cooperation between the superpowers and laid the groundwork for continued dialogue aimed at reducing the nuclear threat globally.\"\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Input:\", text_to_summarize)\n",
    "print()\n",
    "\n",
    "result = pipe(text_to_summarize)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Result:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21e4d82-70a4-4de6-855b-1d3b906c538c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

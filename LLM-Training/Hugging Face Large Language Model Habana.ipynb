{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ae3240-aecb-42ae-8e84-4c60c351590c",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e7ae8-3888-4cd4-b61b-9a48488e7d01",
   "metadata": {},
   "source": [
    "# Summarization using T5 model from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459dcf44-3d9c-4e51-8032-0a5ceb5f3078",
   "metadata": {},
   "source": [
    "### Summarization with T5-3B model\n",
    "We will use the Hugging Face Summariazion example with the T5-3B model to fine tune the model with the CNN-dailymail dataset\n",
    "\n",
    "run_summarization.py is a lightweight example of how to download and preprocess a dataset from the ğŸ¤— Datasets library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce72d9-014e-4f37-aad1-8558b8a1839f",
   "metadata": {},
   "source": [
    "#### Initial Setup\n",
    "we start with a Habana PyTorch Docker image and run this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a58fe9-01fd-412a-84a4-420b76da5d50",
   "metadata": {},
   "source": [
    "#### Install Habana's DeepSpeed Fork\n",
    "Habana's DeepSpeed Fork has implementations specifically for Gaudi and must be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75951895-150d-42d6-a499-1dcf3ec51576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HabanaAI/DeepSpeed.git@1.10.0\n",
      "  Cloning https://github.com/HabanaAI/DeepSpeed.git (to revision 1.10.0) to /tmp/pip-req-build-lu_nb_iq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/DeepSpeed.git /tmp/pip-req-build-lu_nb_iq\n",
      "  Running command git checkout -b 1.10.0 --track origin/1.10.0\n",
      "  Switched to a new branch '1.10.0'\n",
      "  Branch '1.10.0' set up to track remote branch '1.10.0' from 'origin'.\n",
      "  Resolved https://github.com/HabanaAI/DeepSpeed.git to commit 141faf783dac331bc3852590d05190dc0e883e51\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m149.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: ninja in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.10.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.23.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (23.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (5.9.5)\n",
      "Collecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.10.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m213.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (2.0.1a0+git37b7ddc)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from deepspeed==0.7.7+hpu.synapse.v1.10.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic->deepspeed==0.7.7+hpu.synapse.v1.10.0) (4.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (3.12.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (3.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch->deepspeed==0.7.7+hpu.synapse.v1.10.0) (1.3.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.7.7+hpu.synapse.v1.10.0-py3-none-any.whl size=738065 sha256=c3b2e254d4de2e9111f0fa619dc8cda0ef62b63b88c2fbee5090200ba8a7c66b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qlcbq8cj/wheels/a1/1d/50/40a4b5d2dcc3710881387fe9c3712cfbc73f4b77a513f3cea6\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, hjson, pydantic, deepspeed\n",
      "Successfully installed deepspeed-0.7.7+hpu.synapse.v1.10.0 hjson-3.1.0 py-cpuinfo-9.0.0 pydantic-1.10.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.10.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632625f-98e3-4be3-87a2-90f39af24e34",
   "metadata": {},
   "source": [
    "#### Install the Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0391992b-28d5-4a71-9eaa-b384aedb158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum[habana]\n",
      "  Downloading optimum-1.8.7.tar.gz (243 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.9/243.9 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (0.15.1a0+42759b1)\n",
      "Collecting transformers[sentencepiece]<4.30.0,>=4.26.0\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m156.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.8.0\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m208.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (23.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.23.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.12)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m278.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m368.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (2.0.1a0+git37b7ddc)\n",
      "Collecting transformers<4.29.0\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m197.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting optimum-habana\n",
      "  Downloading optimum_habana-1.5.1-py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.3/112.3 kB\u001b[0m \u001b[31m345.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (3.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.6.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2.31.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2023.5.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.29.0->optimum[habana]) (2020.10.28)\n",
      "Collecting transformers[sentencepiece]<4.30.0,>=4.26.0\n",
      "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m217.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m134.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers<4.29.0->optimum[habana]) (3.19.5)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m373.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m316.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m334.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (3.8.4)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m336.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m346.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m218.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (1.4.1)\n",
      "Collecting diffusers>=0.12.0\n",
      "  Downloading diffusers-0.17.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m342.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m341.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum[habana]) (1.3.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m340.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers>=0.12.0->optimum-habana->optimum[habana]) (6.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (3.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (6.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (2023.5.7)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->optimum-habana->optimum[habana]) (5.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.9->optimum[habana]) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum[habana]) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers>=0.12.0->optimum-habana->optimum[habana]) (3.15.0)\n",
      "Building wheels for collected packages: optimum\n",
      "  Building wheel for optimum (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum: filename=optimum-1.8.7-py3-none-any.whl size=318191 sha256=0ae3c779b0c8ed6b0b06e4575885ce3bbb4504be559bccb3b4fd610f3513b80f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-a4ou138_/wheels/ba/76/56/1f4416526a025b3e4116c0866e68ff6dc0ab81ad44fac327d8\n",
      "Successfully built optimum\n",
      "Installing collected packages: tokenizers, sentencepiece, xxhash, pyarrow, pillow, humanfriendly, dill, responses, multiprocess, huggingface-hub, coloredlogs, transformers, diffusers, accelerate, datasets, optimum, optimum-habana\n",
      "Successfully installed accelerate-0.20.3 coloredlogs-15.0.1 datasets-2.12.0 diffusers-0.17.1 dill-0.3.6 huggingface-hub-0.15.1 humanfriendly-10.0 multiprocess-0.70.14 optimum-1.8.7 optimum-habana-1.5.1 pillow-9.5.0 pyarrow-12.0.1 responses-0.18.0 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install optimum[habana]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f3f414-2d51-49ff-b12f-f5f3f521459c",
   "metadata": {},
   "source": [
    "#### Clone the Hugging Face Model Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbe4154-8b24-47f5-a431-da74635e701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n",
      "remote: Enumerating objects: 4001, done.\u001b[K\n",
      "remote: Counting objects: 100% (1154/1154), done.\u001b[K\n",
      "remote: Compressing objects: 100% (479/479), done.\u001b[K\n",
      "remote: Total 4001 (delta 803), reused 900 (delta 642), pack-reused 2847\u001b[K\n",
      "Receiving objects: 100% (4001/4001), 2.05 MiB | 30.38 MiB/s, done.\n",
      "Resolving deltas: 100% (2605/2605), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone  https://github.com/huggingface/optimum-habana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85e29e-30be-4288-801d-4e819b49b748",
   "metadata": {},
   "source": [
    "#### Go the Summarization example model and install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e27db3ec-af54-41f3-81f7-4c0d4d54a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/optimum-habana/examples/summarization\n"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab13f8a-5cb6-4676-b2db-52bbb6aff816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (2.12.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (3.19.5)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m202.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting py7zr\n",
      "  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m281.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (2.0.1a0+git37b7ddc)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m295.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (12.0.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (3.8.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (2023.5.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.3.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge-score->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge-score->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m310.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m343.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.6.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m354.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting texttable\n",
      "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pyppmd<1.1.0,>=0.18.1\n",
      "  Downloading pyppmd-1.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.7/139.7 kB\u001b[0m \u001b[31m314.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyzstd>=0.14.4\n",
      "  Downloading pyzstd-0.15.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m412.0/412.0 kB\u001b[0m \u001b[31m336.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting inflate64>=0.3.1\n",
      "  Downloading inflate64-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m309.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from py7zr->-r requirements.txt (line 6)) (5.9.5)\n",
      "Collecting pybcj>=0.6.0\n",
      "  Downloading pybcj-1.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m271.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycryptodomex>=3.6.6\n",
      "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m338.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting brotli>=1.0.9\n",
      "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m364.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.12.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (1.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (4.6.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3->-r requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=2.4.0->-r requirements.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=2.4.0->-r requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=2.4.0->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 7)) (1.3.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=9fa6f150248d3537e40d07231ec2dd48e0efb2820d11911c95b56006ab25a1b4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-utak6iq2/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: texttable, brotli, regex, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, joblib, inflate64, click, py7zr, nltk, rouge-score, evaluate\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2020.10.28\n",
      "    Uninstalling regex-2020.10.28:\n",
      "      Successfully uninstalled regex-2020.10.28\n",
      "Successfully installed brotli-1.0.9 click-8.1.3 evaluate-0.4.0 inflate64-0.3.1 joblib-1.2.0 multivolumefile-0.2.3 nltk-3.8.1 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.8 regex-2023.6.3 rouge-score-0.1.2 texttable-1.6.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "455c3b87-a11b-44ff-85f9-db5daa9f5d78",
   "metadata": {},
   "source": [
    "### Setup for DeepSpeed\n",
    "Since we are using DeepSpeed, we have to confirm that the model has been configured properly.  We look for the following:\n",
    "\n",
    "* deepspeed.initialize(model, ...) model, optimizer, ... =Â deepspeed.initialize(args=args,Â model=model,Â optimizer=optimizer, ...)\n",
    "* deepspeed.init_distributed(dist_backend=â€œhcclâ€, init_method=init_method)\n",
    "* Create a ds_config.json file to set the DS training parameters\n",
    "  \n",
    "  \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b622e99-4b35-4ee9-b9b0-903524fc6ce1",
   "metadata": {},
   "source": [
    "#### DeepSpeed Initialization\n",
    "Look in deepspeed.py and we see the model being passed to the DeepSpeed engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7620afe-25d9-46db-a22d-ceebc178eef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   101\t    import deepspeed\n",
      "   102\t    from deepspeed.utils import logger as ds_logger\n",
      "   103\t\n",
      "   104\t    model = trainer.model\n",
      "   105\t    args = trainer.args\n",
      "   106\t\n",
      "   150\t    kwargs = {\n",
      "   151\t        \"args\": habana_args,\n",
      "   152\t        \"model\": model,\n",
      "   153\t        \"model_parameters\": model_parameters,\n",
      "   154\t        \"config_params\": config,\n",
      "   155\t        \"optimizer\": optimizer,\n",
      "   156\t        \"lr_scheduler\": lr_scheduler,\n",
      "   157\t    }\n",
      "   158\t\n",
      "   159\t    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
      "   160\t\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cd ../../optimum/habana/transformers\n",
    "cat -n deepspeed.py | head -n 106 | tail -n 6\n",
    "cat -n deepspeed.py | head -n 160 | tail -n 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c64e9-b31f-4d84-ad0f-ee48d05cd34f",
   "metadata": {},
   "source": [
    "#### DeepSpeed Distrbuted\n",
    "Look in training_args.py and we see the DeepSpeed Distribution initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44f45310-d0f8-416e-ba1b-3ff11ea5d870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "   538\t            if self.deepspeed:\n",
      "   539\t                # deepspeed inits torch.distributed internally\n",
      "   540\t                from transformers.deepspeed import is_deepspeed_available\n",
      "   541\t\n",
      "   548\t\n",
      "   552\t\n",
      "   553\t                deepspeed.init_distributed(dist_backend=\"hccl\", timeout=timedelta(seconds=self.ddp_timeout))\n",
      "   554\t                logger.info(\"DeepSpeed is enabled.\")\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cd ../../optimum/habana/transformers\n",
    "cat -n training_args.py | head -n 541 | tail -n 4\n",
    "cat -n training_args.py | head -n 548 | tail -n 1\n",
    "cat -n training_args.py | head -n 554 | tail -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454d898-f2ff-4e6f-9171-db5f8bfc07f3",
   "metadata": {},
   "source": [
    "#### Create DeepSpeed Config file with ZeRO preferences\n",
    "The ds_config.json file will configure the parameters to run DeepSpeed\n",
    "\n",
    "In this case, we will run the ZeRO3 optimier and BF16 mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4b9f3bf-d0f1-4787-8d7c-5b9fde57f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"steps_per_print\": 64,\n",
      "    \"train_batch_size\": \"auto\",\n",
      "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
      "    \"gradient_accumulation_steps\": \"auto\",\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    },\n",
      "    \"gradient_clipping\": 1.0,\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3,\n",
      "        \"overlap_comm\": false,\n",
      "        \"reduce_scatter\": false,\n",
      "        \"contiguous_gradients\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tee ./ds_config.json <<EOF\n",
    "{\n",
    "    \"steps_per_print\": 64,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"overlap_comm\": false,\n",
    "        \"reduce_scatter\": false,\n",
    "        \"contiguous_gradients\": false\n",
    "    }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f8d68d-53ab-452d-a1cc-d93038951609",
   "metadata": {},
   "source": [
    "#### Fine Tuning T5-3b with the cnn_dailymail dataset\n",
    "The T5-3b model is a large language model that was origianlly trained on the C4 dataset and in this case will be fined tuned on the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset that is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail.\n",
    "\n",
    "For use of this model on First-Gen Gaudi, users should update the model to \"T5-large\"\n",
    "\n",
    "This is run by `gaudi_spawn.py`, a simple launcher script to collect arguments and send them to `distributed_runner.py` for training on multiple HPUs, which then calls the `run_summarization.py` model.\n",
    "\n",
    "Notice the Habana specific commands to use here:\n",
    "\n",
    "-- use_habana  - allows training to run on Habana Gaudi  \n",
    "-- use_hpu_graphs - reduces recompilation by replaying the graph  \n",
    "-- gaudi_config_name Habana/t5 - mapping to Hugging Face T5 Model  \n",
    "\n",
    "**Even though a Billion parameter T5 model can be used for Fine Tuning, this fine tuning still takes many hours to complete.  \n",
    "For users that wish to execute the example Fine Tuning, they should modify the model to run \"t5-small\", which only takes a few hours to complete.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707f195-b154-4612-a210-2956a7ed2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85af00-f5a3-4d45-a661-53d3ea287c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "mkdir ft-summarization\n",
    "python ../gaudi_spawn.py \\\n",
    "--world_size 8 --use_deepspeed run_summarization.py \\\n",
    "--model_name_or_path t5-3b \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--dataset_name cnn_dailymail \\\n",
    "--dataset_config '\"3.0.0\"' \\\n",
    "--source_prefix '\"summarize: \"' \\\n",
    "--output_dir ./ft-summarization \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--per_device_eval_batch_size 4 \\\n",
    "--overwrite_output_dir \\\n",
    "--predict_with_generate \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs \\\n",
    "--gaudi_config_name Habana/t5 \\\n",
    "--ignore_pad_token_for_loss False \\\n",
    "--pad_to_max_length \\\n",
    "--save_strategy epoch \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--deepspeed ./ds_config.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f93c0b-04bb-4fba-adb8-e18af01fd324",
   "metadata": {},
   "source": [
    "### After fine tuning, let's look at the results\n",
    "This fine tuned model has created the new `pytorch_model.bin` and the global_step26919 folder contain the checkpoints that will be used in the infernece in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34cdf3d-118e-4f80-a83d-2237c52f19c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./ft-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b035a826-201f-4a42-9222-5a9639cc44ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3248\n",
      "drwxrwxr-x 7 1000 1000    6144 Jun 10 10:38 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwx------ 1 root root    4096 Jun 14 04:11 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root     312 Jun 10 10:38 all_results.json\n",
      "drwxr-xr-x 3 root root    6144 Jun 10 09:23 \u001b[01;34mcheckpoint-17946\u001b[0m/\n",
      "drwxr-xr-x 3 root root    6144 Jun 10 10:38 \u001b[01;34mcheckpoint-26919\u001b[0m/\n",
      "drwxr-xr-x 3 root root    6144 Jun 10 08:08 \u001b[01;34mcheckpoint-8973\u001b[0m/\n",
      "-rw-r--r-- 1 root root    1474 Jun 10 10:38 config.json\n",
      "-rw-r--r-- 1 root root     506 Jun 10 10:38 gaudi_config.json\n",
      "-rw-r--r-- 1 root root     142 Jun 10 10:38 generation_config.json\n",
      "drwxr-xr-x 2 root root    6144 Jun 10 10:38 \u001b[01;34mglobal_step26919\u001b[0m/\n",
      "-rw-r--r-- 1 root root      16 Jun 10 10:38 latest\n",
      "drwxr-xr-x 3 root root    6144 Jun 10 06:53 \u001b[01;34mruns\u001b[0m/\n",
      "-rw-r--r-- 1 root root    2201 Jun 10 10:38 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root  791656 Jun 10 10:38 spiece.model\n",
      "-rw-r--r-- 1 root root    2324 Jun 10 10:38 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 2422360 Jun 10 10:38 tokenizer.json\n",
      "-rw-r--r-- 1 root root   13541 Jun 10 10:38 trainer_state.json\n",
      "-rw-r--r-- 1 root root    4475 Jun 10 10:38 training_args.bin\n",
      "-rw-r--r-- 1 root root     312 Jun 10 10:38 train_results.json\n",
      "-rwxr--r-- 1 root root   19002 Jun 10 10:38 \u001b[01;32mzero_to_fp32.py\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "%ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a36a34-e7ff-4ac7-998a-e9d01bf157d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f75cd58-cca9-4152-889a-75fbfdc24b07",
   "metadata": {},
   "source": [
    "#### Summarization using the Pipeline\n",
    "Now we can run the summarization using Hugging Face Pipeline call with the fine tuned model.  In this case we will point to the mdoel that we fine tuned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e145ca22-b2d0-49e5-89ee-0056adf8f54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Input: summarize: Introduction: The Strategic Arms Limitation Talks II (SALT II) treaty, signed on June 18, 1979, between the United States and the Soviet Union, marked a significant milestone in nuclear arms control efforts during the Cold War era. Building upon its predecessor, SALT I, the treaty aimed to curb the arms race and reduce the risk of nuclear conflict between the superpowers. Key Provisions: SALT II encompassed several crucial provisions. It placed limits on strategic offensive arms, including intercontinental ballistic missiles (ICBMs), submarine-launched ballistic missiles (SLBMs), and heavy bombers. The agreement specified the maximum number of deployed warheads and launchers each party could possess. Verification and Compliance: To ensure compliance, the treaty established comprehensive verification measures. This involved regular exchanges of data, on-site inspections, and monitoring activities by both nations. These measures sought to enhance transparency, foster trust, and prevent either side from gaining a significant advantage in terms of strategic nuclear capabilities. Ratification and Challenges: Although both the United States and the Soviet Union signed the treaty, its ratification faced considerable challenges. The political landscape changed when the Soviet Union invaded Afghanistan in 1979, leading to a deterioration of U.S.-Soviet relations. As a result, the United States never ratified the treaty formally, rendering it non-binding. However, both nations pledged to adhere to its principles, effectively implementing its provisions on a voluntary basis. Legacy and Impact: Despite the treaty's non-ratification, SALT II's legacy and impact were significant. It set the stage for subsequent arms control negotiations, providing a framework for future agreements such as the Intermediate-Range Nuclear Forces (INF) Treaty and the Strategic Arms Reduction Treaty (START). SALT II demonstrated the potential for cooperation between the superpowers and laid the groundwork for continued dialogue aimed at reducing the nuclear threat globally.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Result: [{'summary_text': \"the Strategic Arms Limitation Talks II (SALT II) treaty, signed on June 18, 1979, between the United States and the Soviet Union, marked a significant milestone in nuclear arms control efforts during the Cold War era . despite the treaty's non-ratification, its legacy and impact were significant .\"}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load model to fine-tune and its tokenizer\n",
    "model_to_finetune = \"t5-3b\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_to_finetune)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_finetune)\n",
    "\n",
    "# Save model and tokenizer on disk\n",
    "path_to_local_model = \"./ft-summarization\"\n",
    "\n",
    "# Instantiate pipeline from local repo\n",
    "pipe = pipeline(task=\"summarization\", model=path_to_local_model, device=\"hpu\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "text_to_summarize = \"summarize: Introduction: The Strategic Arms Limitation Talks II (SALT II) treaty, signed on June 18, 1979, between the United States and the Soviet Union, marked a significant milestone in nuclear arms control efforts during the Cold War era. Building upon its predecessor, SALT I, the treaty aimed to curb the arms race and reduce the risk of nuclear conflict between the superpowers. Key Provisions: SALT II encompassed several crucial provisions. It placed limits on strategic offensive arms, including intercontinental ballistic missiles (ICBMs), submarine-launched ballistic missiles (SLBMs), and heavy bombers. The agreement specified the maximum number of deployed warheads and launchers each party could possess. Verification and Compliance: To ensure compliance, the treaty established comprehensive verification measures. This involved regular exchanges of data, on-site inspections, and monitoring activities by both nations. These measures sought to enhance transparency, foster trust, and prevent either side from gaining a significant advantage in terms of strategic nuclear capabilities. Ratification and Challenges: Although both the United States and the Soviet Union signed the treaty, its ratification faced considerable challenges. The political landscape changed when the Soviet Union invaded Afghanistan in 1979, leading to a deterioration of U.S.-Soviet relations. As a result, the United States never ratified the treaty formally, rendering it non-binding. However, both nations pledged to adhere to its principles, effectively implementing its provisions on a voluntary basis. Legacy and Impact: Despite the treaty's non-ratification, SALT II's legacy and impact were significant. It set the stage for subsequent arms control negotiations, providing a framework for future agreements such as the Intermediate-Range Nuclear Forces (INF) Treaty and the Strategic Arms Reduction Treaty (START). SALT II demonstrated the potential for cooperation between the superpowers and laid the groundwork for continued dialogue aimed at reducing the nuclear threat globally.\"\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Input:\", text_to_summarize)\n",
    "print()\n",
    "\n",
    "result = pipe(text_to_summarize)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Result:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21e4d82-70a4-4de6-855b-1d3b906c538c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

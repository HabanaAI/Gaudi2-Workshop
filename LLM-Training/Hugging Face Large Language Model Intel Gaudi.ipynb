{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6ae3240-aecb-42ae-8e84-4c60c351590c",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b88e7ae8-3888-4cd4-b61b-9a48488e7d01",
   "metadata": {},
   "source": [
    "# Fine Tuning and Inference using Hugging Face and the Optimum Habana Library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "459dcf44-3d9c-4e51-8032-0a5ceb5f3078",
   "metadata": {},
   "source": [
    "### Summarization with T5-3B model on the Intel&reg; Gaudi&reg; 2 AI acclerator\n",
    "We will use the Hugging Face Summariazion example with the T5-3B model to fine tune the model with the CNN-dailymail dataset\n",
    "\n",
    "run_summarization.py is a lightweight example of how to download and preprocess a dataset from the 🤗 Datasets library "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8ce72d9-014e-4f37-aad1-8558b8a1839f",
   "metadata": {},
   "source": [
    "#### Initial Setup\n",
    "We start with a Intel Gaudi PyTorch Docker image and run this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4a58fe9-01fd-412a-84a4-420b76da5d50",
   "metadata": {},
   "source": [
    "#### Install the Intel Gaudi DeepSpeed Fork\n",
    "The Intel Gaudi DeepSpeed Fork has implementations specifically for Gaudi and must be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75951895-150d-42d6-a499-1dcf3ec51576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/HabanaAI/DeepSpeed.git@1.16.2\n",
      "  Cloning https://github.com/HabanaAI/DeepSpeed.git (to revision 1.16.2) to /tmp/pip-req-build-by3bxdk1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/HabanaAI/DeepSpeed.git /tmp/pip-req-build-by3bxdk1\n",
      "  Running command git checkout -b 1.16.2 --track origin/1.16.2\n",
      "  Switched to a new branch '1.16.2'\n",
      "  Branch '1.16.2' set up to track remote branch '1.16.2' from 'origin'.\n",
      "  Resolved https://github.com/HabanaAI/DeepSpeed.git to commit d0420c5fd6b21fcd403538bde078e695a62ddba5\n",
      "  Running command git submodule update --init --recursive -q\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.16.1) (3.1.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.16.1) (1.11.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.16.1) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.16.1) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.16.1) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.16.1) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.16.1) (2.8.2)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.16.1) (8.0.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.16.1) (2.2.2a0+gitb5d0b9b)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.14.0+hpu.synapse.v1.16.1) (4.66.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0+hpu.synapse.v1.16.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0+hpu.synapse.v1.16.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.14.0+hpu.synapse.v1.16.1) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0+hpu.synapse.v1.16.1) (3.15.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0+hpu.synapse.v1.16.1) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0+hpu.synapse.v1.16.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0+hpu.synapse.v1.16.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed==0.14.0+hpu.synapse.v1.16.1) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed==0.14.0+hpu.synapse.v1.16.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed==0.14.0+hpu.synapse.v1.16.1) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.16.2  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b632625f-98e3-4be3-87a2-90f39af24e34",
   "metadata": {},
   "source": [
    "#### Install the Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0391992b-28d5-4a71-9eaa-b384aedb158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum-habana==v1.12.0 in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
      "Requirement already satisfied: transformers<4.41.0,>=4.40.0 in /usr/local/lib/python3.10/dist-packages (from optimum-habana==v1.12.0) (4.40.2)\n",
      "Requirement already satisfied: optimum in /usr/local/lib/python3.10/dist-packages (from optimum-habana==v1.12.0) (1.21.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from optimum-habana==v1.12.0) (2.2.2a0+gitb5d0b9b)\n",
      "Requirement already satisfied: accelerate<0.28.0 in /usr/local/lib/python3.10/dist-packages (from optimum-habana==v1.12.0) (0.27.2)\n",
      "Requirement already satisfied: diffusers<0.27.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from optimum-habana==v1.12.0) (0.26.3)\n",
      "Requirement already satisfied: huggingface-hub<0.23.0 in /usr/local/lib/python3.10/dist-packages (from optimum-habana==v1.12.0) (0.22.2)\n",
      "Requirement already satisfied: datasets<2.20.0 in /usr/local/lib/python3.10/dist-packages (from optimum-habana==v1.12.0) (2.19.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==v1.12.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==v1.12.0) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==v1.12.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==v1.12.0) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate<0.28.0->optimum-habana==v1.12.0) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0->optimum-habana==v1.12.0) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0->optimum-habana==v1.12.0) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0->optimum-habana==v1.12.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0->optimum-habana==v1.12.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0->optimum-habana==v1.12.0) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0->optimum-habana==v1.12.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0->optimum-habana==v1.12.0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0->optimum-habana==v1.12.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0->optimum-habana==v1.12.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets<2.20.0->optimum-habana==v1.12.0) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<2.20.0->optimum-habana==v1.12.0) (3.9.5)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers<0.27.0,>=0.26.0->optimum-habana==v1.12.0) (8.0.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers<0.27.0,>=0.26.0->optimum-habana==v1.12.0) (2023.5.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers<0.27.0,>=0.26.0->optimum-habana==v1.12.0) (10.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.23.0->optimum-habana==v1.12.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->optimum-habana==v1.12.0) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->optimum-habana==v1.12.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->optimum-habana==v1.12.0) (3.1.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<4.41.0,>=4.40.0->optimum-habana==v1.12.0) (0.19.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from optimum->optimum-habana==v1.12.0) (15.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0->optimum-habana==v1.12.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0->optimum-habana==v1.12.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0->optimum-habana==v1.12.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0->optimum-habana==v1.12.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0->optimum-habana==v1.12.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<2.20.0->optimum-habana==v1.12.0) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets<2.20.0->optimum-habana==v1.12.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets<2.20.0->optimum-habana==v1.12.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets<2.20.0->optimum-habana==v1.12.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets<2.20.0->optimum-habana==v1.12.0) (2024.6.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum->optimum-habana==v1.12.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum->optimum-habana==v1.12.0) (3.20.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->optimum->optimum-habana==v1.12.0) (10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers<0.27.0,>=0.26.0->optimum-habana==v1.12.0) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->optimum-habana==v1.12.0) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<2.20.0->optimum-habana==v1.12.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<2.20.0->optimum-habana==v1.12.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets<2.20.0->optimum-habana==v1.12.0) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->optimum-habana==v1.12.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<2.20.0->optimum-habana==v1.12.0) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.11 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.11 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.3 jupyterlab-widgets-3.0.11 widgetsnbextension-4.0.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install optimum-habana==v1.12.0\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81f3f414-2d51-49ff-b12f-f5f3f521459c",
   "metadata": {},
   "source": [
    "#### Clone the Hugging Face Model Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbe4154-8b24-47f5-a431-da74635e701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n",
      "remote: Enumerating objects: 15245, done.\u001b[K\n",
      "remote: Counting objects: 100% (5011/5011), done.\u001b[K\n",
      "remote: Compressing objects: 100% (807/807), done.\u001b[K\n",
      "remote: Total 15245 (delta 4653), reused 4321 (delta 4180), pack-reused 10234\u001b[K\n",
      "Receiving objects: 100% (15245/15245), 9.11 MiB | 21.70 MiB/s, done.\n",
      "Resolving deltas: 100% (10588/10588), done.\n",
      "Note: switching to '6adad1651566ffb761ce47f8d671b73a3bbb0ec2'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by switching back to a branch.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -c with the switch command. Example:\n",
      "\n",
      "  git switch -c <new-branch-name>\n",
      "\n",
      "Or undo this operation with:\n",
      "\n",
      "  git switch -\n",
      "\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone -b v1.12.0 https://github.com/huggingface/optimum-habana.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c85e29e-30be-4288-801d-4e819b49b748",
   "metadata": {},
   "source": [
    "#### Go the Summarization example model and install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e27db3ec-af54-41f3-81f7-4c0d4d54a609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab13f8a-5cb6-4676-b2db-52bbb6aff816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "455c3b87-a11b-44ff-85f9-db5daa9f5d78",
   "metadata": {},
   "source": [
    "### Setup for DeepSpeed\n",
    "Since we are using DeepSpeed, we have to confirm that the model has been configured properly.  We look for the following:\n",
    "\n",
    "* model, optimizer, ... = deepspeed.initialize(args=args, model=model, optimizer=optimizer, ...)\n",
    "* deepspeed.init_distributed(dist_backend=“hccl”, init_method=init_method)\n",
    "* Create a ds_config.json file to set the DS training parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b622e99-4b35-4ee9-b9b0-903524fc6ce1",
   "metadata": {},
   "source": [
    "#### DeepSpeed Initialization\n",
    "Look in deepspeed.py and we see the model being passed to the DeepSpeed engine\n",
    "\n",
    "```\n",
    "    import deepspeed\n",
    "    from deepspeed.utils import logger as ds_logger\n",
    "\n",
    "    model = trainer.model\n",
    "    args = trainer.args\n",
    "    ...\n",
    "\n",
    "    kwargs = {\n",
    "        \"args\": habana_args,\n",
    "        \"model\": model,\n",
    "        \"model_parameters\": model_parameters,\n",
    "        \"config_params\": config,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr_scheduler\": lr_scheduler,\n",
    "    }\n",
    "\n",
    "    deepspeedengine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "146c64e9-b31f-4d84-ad0f-ee48d05cd34f",
   "metadata": {},
   "source": [
    "#### DeepSpeed Distributed\n",
    "Look in training_args.py and we see the DeepSpeed Distribution initialization\n",
    "\n",
    "```\n",
    "    from habana_frameworks.torch.distributed.hccl import initialize_distributed_hpu\n",
    "    world_size, rank, self.local_rank = initialize_distributed_hpu()\n",
    "\n",
    "    import deepspeed\n",
    "    deepspeed.init_distributed(dist_backend=\"hccl\", timeout=timedelta(seconds=self.ddp_timeout))\n",
    "       logger.info(\"DeepSpeed is enabled.\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4454d898-f2ff-4e6f-9171-db5f8bfc07f3",
   "metadata": {},
   "source": [
    "#### Create DeepSpeed Config file with ZeRO preferences\n",
    "The ds_config.json file will configure the parameters to run DeepSpeed\n",
    "\n",
    "In this case, we will run the ZeRO2 optimizer and BF16 mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "779b48d2-a098-424d-8756-3362ffa74cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b9f3bf-d0f1-4787-8d7c-5b9fde57f36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"steps_per_print\": 64,\n",
      "    \"train_batch_size\": \"auto\",\n",
      "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
      "    \"gradient_accumulation_steps\": \"auto\",\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    },\n",
      "    \"gradient_clipping\": 1.0,\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2,\n",
      "        \"overlap_comm\": false,\n",
      "        \"reduce_scatter\": false,\n",
      "        \"contiguous_gradients\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "tee ./ds_config.json <<EOF\n",
    "{\n",
    "    \"steps_per_print\": 64,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"bf16\": {\n",
    "        \"enabled\": true\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"overlap_comm\": false,\n",
    "        \"reduce_scatter\": false,\n",
    "        \"contiguous_gradients\": false\n",
    "    }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f8d68d-53ab-452d-a1cc-d93038951609",
   "metadata": {},
   "source": [
    "#### Fine Tuning T5-3b with the cnn_dailymail dataset\n",
    "The T5-3b model is a large language model that was originally trained on the C4 dataset and in this case will be fined tuned on the [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) dataset that is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail.\n",
    "\n",
    "For use of this model on Intel Gaudi First-Gen, users should update the model to \"T5-large\"\n",
    "\n",
    "This is run by `gaudi_spawn.py`, a simple launcher script to collect arguments and send them to `distributed_runner.py` for training on multiple HPUs, which then calls the `run_summarization.py` model.\n",
    "\n",
    "Notice the Habana specific commands to use here:\n",
    "\n",
    "-- use_habana  - allows training to run on Intel Gaudi cards\n",
    "-- use_hpu_graphs - reduces recompilation by replaying the graph  \n",
    "-- gaudi_config_name Habana/t5 - mapping to Hugging Face T5 Model  \n",
    "\n",
    "**Even though a Billion parameter T5 model can be used for Fine Tuning, this fine tuning still takes many hours to complete.  \n",
    "For users that wish to execute the example Fine Tuning, they should modify the `model_name_or_path` to \"t5-small\", which takes about 30 minutes to complete.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3707f195-b154-4612-a210-2956a7ed2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ft-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c5fbafb-c218-43bd-bc6f-0d993f0858f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributedRunner run(): command = deepspeed --num_nodes 1 --num_gpus 8 --no_local_rank --master_port 29500 run_summarization.py --model_name_or_path t5-small --do_train --dataset_name cnn_dailymail --dataset_config \"3.0.0\" --source_prefix \"summarize: \" --output_dir ./ft-summarization --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --overwrite_output_dir --predict_with_generate --use_habana --use_lazy_mode --use_hpu_graphs_for_training --gaudi_config_name Habana/t5 --ignore_pad_token_for_loss False --pad_to_max_length --save_strategy epoch --report_to none --throughput_warmup_steps 3 --deepspeed ./ds_config.json\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:22:58,627] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-07-17 06:22:59,842] [WARNING] [runner.py:205:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-07-17 06:22:59,900] [INFO] [runner.py:583:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank --enable_each_rank_log=None run_summarization.py --model_name_or_path t5-small --do_train --dataset_name cnn_dailymail --dataset_config 3.0.0 --source_prefix summarize:  --output_dir ./ft-summarization --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --overwrite_output_dir --predict_with_generate --use_habana --use_lazy_mode --use_hpu_graphs_for_training --gaudi_config_name Habana/t5 --ignore_pad_token_for_loss False --pad_to_max_length --save_strategy epoch --report_to none --throughput_warmup_steps 3 --deepspeed ./ds_config.json\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:23:01,419] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-07-17 06:23:02,648] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2024-07-17 06:23:02,648] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2024-07-17 06:23:02,648] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2024-07-17 06:23:02,648] [INFO] [launch.py:163:main] dist_world_size=8\n",
      "[2024-07-17 06:23:02,648] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "[2024-07-17 06:23:02,649] [INFO] [launch.py:253:main] process 7341 spawned with command: ['/usr/bin/python3', '-u', 'run_summarization.py', '--model_name_or_path', 't5-small', '--do_train', '--dataset_name', 'cnn_dailymail', '--dataset_config', '3.0.0', '--source_prefix', 'summarize: ', '--output_dir', './ft-summarization', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--save_strategy', 'epoch', '--report_to', 'none', '--throughput_warmup_steps', '3', '--deepspeed', './ds_config.json']\n",
      "[2024-07-17 06:23:02,649] [INFO] [launch.py:253:main] process 7342 spawned with command: ['/usr/bin/python3', '-u', 'run_summarization.py', '--model_name_or_path', 't5-small', '--do_train', '--dataset_name', 'cnn_dailymail', '--dataset_config', '3.0.0', '--source_prefix', 'summarize: ', '--output_dir', './ft-summarization', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--save_strategy', 'epoch', '--report_to', 'none', '--throughput_warmup_steps', '3', '--deepspeed', './ds_config.json']\n",
      "[2024-07-17 06:23:02,649] [INFO] [launch.py:253:main] process 7343 spawned with command: ['/usr/bin/python3', '-u', 'run_summarization.py', '--model_name_or_path', 't5-small', '--do_train', '--dataset_name', 'cnn_dailymail', '--dataset_config', '3.0.0', '--source_prefix', 'summarize: ', '--output_dir', './ft-summarization', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--save_strategy', 'epoch', '--report_to', 'none', '--throughput_warmup_steps', '3', '--deepspeed', './ds_config.json']\n",
      "[2024-07-17 06:23:02,650] [INFO] [launch.py:253:main] process 7344 spawned with command: ['/usr/bin/python3', '-u', 'run_summarization.py', '--model_name_or_path', 't5-small', '--do_train', '--dataset_name', 'cnn_dailymail', '--dataset_config', '3.0.0', '--source_prefix', 'summarize: ', '--output_dir', './ft-summarization', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--save_strategy', 'epoch', '--report_to', 'none', '--throughput_warmup_steps', '3', '--deepspeed', './ds_config.json']\n",
      "[2024-07-17 06:23:02,650] [INFO] [launch.py:253:main] process 7345 spawned with command: ['/usr/bin/python3', '-u', 'run_summarization.py', '--model_name_or_path', 't5-small', '--do_train', '--dataset_name', 'cnn_dailymail', '--dataset_config', '3.0.0', '--source_prefix', 'summarize: ', '--output_dir', './ft-summarization', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--save_strategy', 'epoch', '--report_to', 'none', '--throughput_warmup_steps', '3', '--deepspeed', './ds_config.json']\n",
      "[2024-07-17 06:23:02,651] [INFO] [launch.py:253:main] process 7346 spawned with command: ['/usr/bin/python3', '-u', 'run_summarization.py', '--model_name_or_path', 't5-small', '--do_train', '--dataset_name', 'cnn_dailymail', '--dataset_config', '3.0.0', '--source_prefix', 'summarize: ', '--output_dir', './ft-summarization', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--save_strategy', 'epoch', '--report_to', 'none', '--throughput_warmup_steps', '3', '--deepspeed', './ds_config.json']\n",
      "[2024-07-17 06:23:02,651] [INFO] [launch.py:253:main] process 7347 spawned with command: ['/usr/bin/python3', '-u', 'run_summarization.py', '--model_name_or_path', 't5-small', '--do_train', '--dataset_name', 'cnn_dailymail', '--dataset_config', '3.0.0', '--source_prefix', 'summarize: ', '--output_dir', './ft-summarization', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--save_strategy', 'epoch', '--report_to', 'none', '--throughput_warmup_steps', '3', '--deepspeed', './ds_config.json']\n",
      "[2024-07-17 06:23:02,651] [INFO] [launch.py:253:main] process 7348 spawned with command: ['/usr/bin/python3', '-u', 'run_summarization.py', '--model_name_or_path', 't5-small', '--do_train', '--dataset_name', 'cnn_dailymail', '--dataset_config', '3.0.0', '--source_prefix', 'summarize: ', '--output_dir', './ft-summarization', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--overwrite_output_dir', '--predict_with_generate', '--use_habana', '--use_lazy_mode', '--use_hpu_graphs_for_training', '--gaudi_config_name', 'Habana/t5', '--ignore_pad_token_for_loss', 'False', '--pad_to_max_length', '--save_strategy', 'epoch', '--report_to', 'none', '--throughput_warmup_steps', '3', '--deepspeed', './ds_config.json']\n",
      "gaudi_config.json: 100%|██████████████████████| 90.0/90.0 [00:00<00:00, 769kB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:23:09,026] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:23:09,080] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:23:09,126] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:23:09,127] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:23:09,168] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:23:09,218] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:23:09,219] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/hpu/__init__.py:158: UserWarning: torch.hpu.setDeterministic is deprecated and will be removed in next release. Please use torch.use_deterministic_algorithms instead.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:23:09,363] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
      "[2024-07-17 06:23:10,015] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-07-17 06:23:10,054] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-07-17 06:23:10,098] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-07-17 06:23:10,108] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-07-17 06:23:10,128] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-07-17 06:23:10,187] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-07-17 06:23:10,188] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-07-17 06:23:10,333] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-07-17 06:23:10,333] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl\n",
      "07/17/2024 06:23:11 - WARNING - __main__ - Process rank: 3, device: hpu, distributed training: True, mixed-precision training: False\n",
      "07/17/2024 06:23:11 - WARNING - __main__ - Process rank: 7, device: hpu, distributed training: True, mixed-precision training: False\n",
      "07/17/2024 06:23:11 - WARNING - __main__ - Process rank: 2, device: hpu, distributed training: True, mixed-precision training: False\n",
      "07/17/2024 06:23:11 - WARNING - __main__ - Process rank: 6, device: hpu, distributed training: True, mixed-precision training: False\n",
      "07/17/2024 06:23:11 - WARNING - __main__ - Process rank: 4, device: hpu, distributed training: True, mixed-precision training: False\n",
      "07/17/2024 06:23:11 - WARNING - __main__ - Process rank: 1, device: hpu, distributed training: True, mixed-precision training: False\n",
      "07/17/2024 06:23:11 - WARNING - __main__ - Process rank: 5, device: hpu, distributed training: True, mixed-precision training: False\n",
      "07/17/2024 06:23:11 - WARNING - __main__ - Process rank: 0, device: hpu, distributed training: True, mixed-precision training: False\n",
      "07/17/2024 06:23:11 - INFO - __main__ - Training/evaluation parameters GaudiSeq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=hccl,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=./ds_config.json,\n",
      "disable_tensor_cache_hpu_graphs=False,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp8=False,\n",
      "fp8_recipe_format=E5M2,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/t5,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "ignore_eos=True,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./ft-summarization/runs/Jul17_06-23-08_sc09super17-klb2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_hpu_graphs=None,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./ft-summarization,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "pipelining_fwd_bwd=False,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "profiling_record_shapes=True,\n",
      "profiling_steps=0,\n",
      "profiling_warmup_steps=0,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./ft-summarization,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "throughput_warmup_steps=3,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_cpu=False,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=False,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=True,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Downloading readme: 100%|██████████████████| 15.6k/15.6k [00:00<00:00, 65.1MB/s]\n",
      "Downloading data: 100%|██████████████████████| 257M/257M [00:09<00:00, 26.9MB/s]\n",
      "Downloading data: 100%|██████████████████████| 257M/257M [00:10<00:00, 25.4MB/s]\n",
      "Downloading data: 100%|██████████████████████| 259M/259M [00:09<00:00, 27.0MB/s]\n",
      "Downloading data: 100%|████████████████████| 34.7M/34.7M [00:01<00:00, 18.7MB/s]\n",
      "Downloading data: 100%|████████████████████| 30.0M/30.0M [00:01<00:00, 22.7MB/s]\n",
      "Generating train split: 100%|█| 287113/287113 [00:03<00:00, 91655.21 examples/s]\n",
      "Generating validation split: 100%|█| 13368/13368 [00:00<00:00, 88336.75 examples\n",
      "Generating test split: 100%|████| 11490/11490 [00:00<00:00, 94653.14 examples/s]\n",
      "Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\n",
      "07/17/2024 06:23:55 - INFO - datasets.builder - Found cached dataset cnn_dailymail (/root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "07/17/2024 06:23:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d\n",
      "config.json: 100%|█████████████████████████| 1.21k/1.21k [00:00<00:00, 11.8MB/s]\n",
      "[INFO|configuration_utils.py:726] 2024-07-17 06:23:55,558 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-07-17 06:23:55,559 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.40.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100%|███████████████| 2.32k/2.32k [00:00<00:00, 23.0MB/s]\n",
      "spiece.model: 100%|██████████████████████████| 792k/792k [00:00<00:00, 4.93MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 1.39M/1.39M [00:00<00:00, 5.61MB/s]\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-07-17 06:23:56,744 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-07-17 06:23:56,744 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-07-17 06:23:56,744 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-07-17 06:23:56,744 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2087] 2024-07-17 06:23:56,744 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
      "model.safetensors: 100%|█████████████████████| 242M/242M [00:09<00:00, 26.9MB/s]\n",
      "[INFO|modeling_utils.py:3429] 2024-07-17 06:24:05,975 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n",
      "[INFO|configuration_utils.py:928] 2024-07-17 06:24:05,980 >> Generate config GaudiGenerationConfig {\n",
      "  \"attn_softmax_bf16\": null,\n",
      "  \"bucket_internal\": null,\n",
      "  \"bucket_size\": -1,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"flash_attention_causal_mask\": null,\n",
      "  \"flash_attention_fast_softmax\": null,\n",
      "  \"flash_attention_recompute\": null,\n",
      "  \"ignore_eos\": null,\n",
      "  \"limit_hpu_graphs\": null,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reduce_recompile\": null,\n",
      "  \"reuse_cache\": null,\n",
      "  \"static_shapes\": null,\n",
      "  \"trim_logits\": null,\n",
      "  \"use_flash_attention\": null,\n",
      "  \"use_fused_rope\": null\n",
      "}\n",
      "\n",
      "generation_config.json: 100%|███████████████████| 147/147 [00:00<00:00, 938kB/s]\n",
      "[INFO|modeling_utils.py:4170] 2024-07-17 06:24:09,607 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4178] 2024-07-17 06:24:09,607 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:883] 2024-07-17 06:24:09,729 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n",
      "[INFO|configuration_utils.py:928] 2024-07-17 06:24:09,729 >> Generate config GaudiGenerationConfig {\n",
      "  \"attn_softmax_bf16\": null,\n",
      "  \"bucket_internal\": null,\n",
      "  \"bucket_size\": -1,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"flash_attention_causal_mask\": null,\n",
      "  \"flash_attention_fast_softmax\": null,\n",
      "  \"flash_attention_recompute\": null,\n",
      "  \"ignore_eos\": null,\n",
      "  \"limit_hpu_graphs\": null,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reduce_recompile\": null,\n",
      "  \"reuse_cache\": null,\n",
      "  \"static_shapes\": null,\n",
      "  \"trim_logits\": null,\n",
      "  \"use_flash_attention\": null,\n",
      "  \"use_fused_rope\": null\n",
      "}\n",
      "\n",
      "Running tokenizer on train dataset:   0%|     | 0/287113 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-eb424b1989e36a7d.arrow\n",
      "07/17/2024 06:24:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cache-eb424b1989e36a7d.arrow\n",
      "Running tokenizer on train dataset: 100%|█| 287113/287113 [02:58<00:00, 1611.67 \n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 0\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056433664 KB\n",
      "------------------------------------------------------------------------------\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 33.0MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 35.2MB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:57: UserWarning: \"hpu:X\" notation is not supported by Gaudi PyTorch intergration bridge. Please change to \"hpu\" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)\n",
      "  return super().__torch_function__(func, types, new_args, kwargs)\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 29.9MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 20.9MB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:57: UserWarning: \"hpu:X\" notation is not supported by Gaudi PyTorch intergration bridge. Please change to \"hpu\" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)\n",
      "  return super().__torch_function__(func, types, new_args, kwargs)\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 27.1MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 17.9MB/s]\n",
      "[2024-07-17 06:27:12,027] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0+hpu.synapse.v1.16.1, git-hash=d0420c5, git-branch=1.16.2\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 19.4MB/s]\n",
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 22.7MB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:57: UserWarning: \"hpu:X\" notation is not supported by Gaudi PyTorch intergration bridge. Please change to \"hpu\" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)\n",
      "  return super().__torch_function__(func, types, new_args, kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:57: UserWarning: \"hpu:X\" notation is not supported by Gaudi PyTorch intergration bridge. Please change to \"hpu\" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)\n",
      "  return super().__torch_function__(func, types, new_args, kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:57: UserWarning: \"hpu:X\" notation is not supported by Gaudi PyTorch intergration bridge. Please change to \"hpu\" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)\n",
      "  return super().__torch_function__(func, types, new_args, kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:57: UserWarning: \"hpu:X\" notation is not supported by Gaudi PyTorch intergration bridge. Please change to \"hpu\" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)\n",
      "  return super().__torch_function__(func, types, new_args, kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:57: UserWarning: \"hpu:X\" notation is not supported by Gaudi PyTorch intergration bridge. Please change to \"hpu\" without index (Triggered internally at /npu-stack/pytorch-integration/pytorch_helpers/lazy_to_backend.cpp:53.)\n",
      "  return super().__torch_function__(func, types, new_args, kwargs)\n",
      "[2024-07-17 06:27:14,656] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-07-17 06:27:14,657] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-07-17 06:27:14,657] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-07-17 06:27:14,659] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdamW\n",
      "[2024-07-17 06:27:14,659] [INFO] [utils.py:63:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdamW type=<class 'habana_frameworks.torch.hpex.optimizers.FusedAdamW.FusedAdamW'>\n",
      "[2024-07-17 06:27:14,659] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-07-17 06:27:14,660] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000\n",
      "[2024-07-17 06:27:14,660] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000\n",
      "[2024-07-17 06:27:14,660] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False\n",
      "[2024-07-17 06:27:14,660] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "[2024-07-17 06:27:18,875] [INFO] [utils.py:775:see_memory_usage] Before initializing optimizer states\n",
      "[2024-07-17 06:27:18,881] [INFO] [utils.py:776:see_memory_usage] MA 0.12 GB         Max_MA 0.12 GB         CA 94.62 GB         Max_CA 95 GB \n",
      "[2024-07-17 06:27:18,881] [INFO] [utils.py:783:see_memory_usage] CPU Virtual Memory:  used = 65.37 GB, percent = 6.5%\n",
      "[2024-07-17 06:27:18,959] [INFO] [utils.py:775:see_memory_usage] After initializing optimizer states\n",
      "[2024-07-17 06:27:18,965] [INFO] [utils.py:776:see_memory_usage] MA 0.12 GB         Max_MA 0.0 GB         CA 94.62 GB         Max_CA 95 GB \n",
      "[2024-07-17 06:27:18,965] [INFO] [utils.py:783:see_memory_usage] CPU Virtual Memory:  used = 65.37 GB, percent = 6.5%\n",
      "[2024-07-17 06:27:18,966] [INFO] [stage_1_and_2.py:538:__init__] optimizer state initialized\n",
      "[2024-07-17 06:27:19,040] [INFO] [utils.py:775:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-07-17 06:27:19,046] [INFO] [utils.py:776:see_memory_usage] MA 0.12 GB         Max_MA 0.0 GB         CA 94.62 GB         Max_CA 95 GB \n",
      "[2024-07-17 06:27:19,046] [INFO] [utils.py:783:see_memory_usage] CPU Virtual Memory:  used = 65.38 GB, percent = 6.5%\n",
      "[2024-07-17 06:27:19,049] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2024-07-17 06:27:19,049] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-07-17 06:27:19,049] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-07-17 06:27:19,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1007:print] DeepSpeedEngine configuration:\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   amp_enabled .................. False\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   amp_params ................... False\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   bfloat16_enabled ............. True\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7d66afd2afb0>\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   communication_data_type ...... None\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   curriculum_enabled_legacy .... False\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   curriculum_params_legacy ..... False\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   data_efficiency_enabled ...... False\n",
      "[2024-07-17 06:27:19,050] [INFO] [config.py:1011:print]   dataloader_drop_last ......... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   disable_allgather ............ False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   dump_state ................... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   eigenvalue_enabled ........... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   eigenvalue_verbose ........... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   elasticity_enabled ........... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   fp16_auto_cast ............... None\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   fp16_enabled ................. False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   fp8_optimizer_enabled ........ False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   global_rank .................. 0\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   grad_accum_dtype ............. None\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   gradient_accumulation_steps .. 1\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   gradient_clipping ............ 1.0\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   graph_harvesting ............. False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   initial_dynamic_scale ........ 1\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   load_universal_checkpoint .... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   loss_scale ................... 1.0\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   memory_breakdown ............. False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   mics_hierarchial_params_gather  False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   mics_shard_size .............. -1\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   optimizer_name ............... None\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   optimizer_params ............. None\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   pld_enabled .................. False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   pld_params ................... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   prescale_gradients ........... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   scheduler_name ............... None\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   scheduler_params ............. None\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   sparse_attention ............. None\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   sparse_gradients_enabled ..... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   steps_per_print .............. inf\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   train_batch_size ............. 32\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   train_micro_batch_size_per_gpu  4\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   use_data_before_expert_parallel_  False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   use_node_local_storage ....... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   wall_clock_breakdown ......... False\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   weight_quantization_config ... None\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   world_size ................... 8\n",
      "[2024-07-17 06:27:19,051] [INFO] [config.py:1011:print]   zero_allow_untested_optimizer  True\n",
      "[2024-07-17 06:27:19,052] [INFO] [config.py:1011:print]   zero_config .................. stage=2 contiguous_gradients=False reduce_scatter=False reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False use_all_reduce_for_fetch_params=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-07-17 06:27:19,052] [INFO] [config.py:1011:print]   zero_enabled ................. True\n",
      "[2024-07-17 06:27:19,052] [INFO] [config.py:1011:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-07-17 06:27:19,052] [INFO] [config.py:1011:print]   zero_optimization_stage ...... 2\n",
      "[2024-07-17 06:27:19,052] [INFO] [config.py:997:print_user_config]   json = {\n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 4, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"overlap_comm\": false, \n",
      "        \"reduce_scatter\": false, \n",
      "        \"contiguous_gradients\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "[INFO|trainer.py:766] 2024-07-17 06:27:19,052 >> ***** Running training *****\n",
      "[INFO|trainer.py:767] 2024-07-17 06:27:19,052 >>   Num examples = 287,113\n",
      "[INFO|trainer.py:768] 2024-07-17 06:27:19,052 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:769] 2024-07-17 06:27:19,052 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:772] 2024-07-17 06:27:19,052 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:773] 2024-07-17 06:27:19,052 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:774] 2024-07-17 06:27:19,052 >>   Total optimization steps = 26,919\n",
      "[INFO|trainer.py:775] 2024-07-17 06:27:19,054 >>   Number of trainable parameters = 60,506,624\n",
      "{'loss': 1.5275, 'grad_norm': 0.6536413431167603, 'learning_rate': 4.907128793788774e-05, 'epoch': 0.06, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0874, 'grad_norm': 0.7808430194854736, 'learning_rate': 4.814257587577547e-05, 'epoch': 0.11, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0834, 'grad_norm': 0.5406067967414856, 'learning_rate': 4.7213863813663214e-05, 'epoch': 0.17, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0784, 'grad_norm': 0.5303201675415039, 'learning_rate': 4.6285151751550956e-05, 'epoch': 0.22, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0779, 'grad_norm': 0.6340585947036743, 'learning_rate': 4.535643968943869e-05, 'epoch': 0.28, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0654, 'grad_norm': 0.6415128707885742, 'learning_rate': 4.4427727627326426e-05, 'epoch': 0.33, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0689, 'grad_norm': 0.704521656036377, 'learning_rate': 4.349901556521416e-05, 'epoch': 0.39, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0647, 'grad_norm': 0.5324937105178833, 'learning_rate': 4.2570303503101897e-05, 'epoch': 0.45, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0624, 'grad_norm': 0.5152311325073242, 'learning_rate': 4.164159144098964e-05, 'epoch': 0.5, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0633, 'grad_norm': 0.548210084438324, 'learning_rate': 4.0712879378877373e-05, 'epoch': 0.56, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0614, 'grad_norm': 0.4761773943901062, 'learning_rate': 3.9784167316765115e-05, 'epoch': 0.61, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0649, 'grad_norm': 0.5343759655952454, 'learning_rate': 3.885545525465285e-05, 'epoch': 0.67, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0604, 'grad_norm': 0.5335255265235901, 'learning_rate': 3.7926743192540585e-05, 'epoch': 0.72, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0569, 'grad_norm': 0.4993593990802765, 'learning_rate': 3.699803113042832e-05, 'epoch': 0.78, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0546, 'grad_norm': 0.6568120718002319, 'learning_rate': 3.606931906831606e-05, 'epoch': 0.84, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0522, 'grad_norm': 0.5919902324676514, 'learning_rate': 3.51406070062038e-05, 'epoch': 0.89, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0525, 'grad_norm': 0.6011993288993835, 'learning_rate': 3.421189494409154e-05, 'epoch': 0.95, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      " 33%|████████████▋                         | 8973/26919 [07:29<16:30, 18.12it/s][INFO|trainer.py:1610] 2024-07-17 06:34:48,803 >> Saving model checkpoint to ./ft-summarization/checkpoint-8973\n",
      "[INFO|configuration_utils.py:471] 2024-07-17 06:34:48,804 >> Configuration saved in ./ft-summarization/checkpoint-8973/config.json\n",
      "[INFO|configuration_utils.py:697] 2024-07-17 06:34:48,805 >> Configuration saved in ./ft-summarization/checkpoint-8973/generation_config.json\n",
      "[INFO|modeling_utils.py:2590] 2024-07-17 06:34:49,032 >> Model weights saved in ./ft-summarization/checkpoint-8973/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-07-17 06:34:49,033 >> tokenizer config file saved in ./ft-summarization/checkpoint-8973/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-07-17 06:34:49,033 >> Special tokens file saved in ./ft-summarization/checkpoint-8973/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-07-17 06:34:49,034 >> Copy vocab file to ./ft-summarization/checkpoint-8973/spiece.model\n",
      "[INFO|configuration_utils.py:125] 2024-07-17 06:34:49,042 >> Configuration saved in ./ft-summarization/checkpoint-8973/gaudi_config.json\n",
      "[2024-07-17 06:34:49,043] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8973 is about to be saved!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:34:49,062] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./ft-summarization/checkpoint-8973/global_step8973/mp_rank_00_model_states.pt\n",
      "[2024-07-17 06:34:49,063] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-8973/global_step8973/mp_rank_00_model_states.pt...\n",
      "[2024-07-17 06:34:49,289] [INFO] [torch_checkpoint_engine.py:24:save] [Torch] Saved ./ft-summarization/checkpoint-8973/global_step8973/mp_rank_00_model_states.pt.\n",
      "[2024-07-17 06:34:49,308] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-8973/global_step8973/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-07-17 06:34:49,447] [INFO] [torch_checkpoint_engine.py:24:save] [Torch] Saved ./ft-summarization/checkpoint-8973/global_step8973/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-07-17 06:34:49,448] [INFO] [engine.py:3531:_save_zero_checkpoint] zero checkpoint saved ./ft-summarization/checkpoint-8973/global_step8973/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-07-17 06:34:49,448] [INFO] [torch_checkpoint_engine.py:34:commit] [Torch] Checkpoint global_step8973 is ready now!\n",
      "{'loss': 1.0502, 'grad_norm': 0.5743988752365112, 'learning_rate': 3.3283182881979274e-05, 'epoch': 1.0, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0475, 'grad_norm': 0.5528340339660645, 'learning_rate': 3.235447081986701e-05, 'epoch': 1.06, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0472, 'grad_norm': 0.5046667456626892, 'learning_rate': 3.1425758757754745e-05, 'epoch': 1.11, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0491, 'grad_norm': 2.701096296310425, 'learning_rate': 3.0497046695642483e-05, 'epoch': 1.17, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.05, 'grad_norm': 0.4796208441257477, 'learning_rate': 2.956833463353022e-05, 'epoch': 1.23, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0408, 'grad_norm': 0.4241269528865814, 'learning_rate': 2.8639622571417957e-05, 'epoch': 1.28, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0463, 'grad_norm': 0.5631932020187378, 'learning_rate': 2.77109105093057e-05, 'epoch': 1.34, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0466, 'grad_norm': 0.44895726442337036, 'learning_rate': 2.6782198447193433e-05, 'epoch': 1.39, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.048, 'grad_norm': 0.6052603721618652, 'learning_rate': 2.5853486385081172e-05, 'epoch': 1.45, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0397, 'grad_norm': 0.5063062310218811, 'learning_rate': 2.4924774322968907e-05, 'epoch': 1.5, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0447, 'grad_norm': 0.49111056327819824, 'learning_rate': 2.3996062260856645e-05, 'epoch': 1.56, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0463, 'grad_norm': 0.478216290473938, 'learning_rate': 2.3067350198744384e-05, 'epoch': 1.62, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0432, 'grad_norm': 0.5446385145187378, 'learning_rate': 2.213863813663212e-05, 'epoch': 1.67, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0483, 'grad_norm': 0.5860661268234253, 'learning_rate': 2.1209926074519858e-05, 'epoch': 1.73, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0395, 'grad_norm': 0.792944610118866, 'learning_rate': 2.0281214012407593e-05, 'epoch': 1.78, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0383, 'grad_norm': 0.579601526260376, 'learning_rate': 1.935250195029533e-05, 'epoch': 1.84, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0405, 'grad_norm': 0.4773712754249573, 'learning_rate': 1.842378988818307e-05, 'epoch': 1.89, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0463, 'grad_norm': 0.4904231131076813, 'learning_rate': 1.7495077826070805e-05, 'epoch': 1.95, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      " 67%|████████████████████████▋            | 17945/26919 [14:53<08:10, 18.29it/s][INFO|trainer.py:1610] 2024-07-17 06:42:13,116 >> Saving model checkpoint to ./ft-summarization/checkpoint-17946\n",
      "[INFO|configuration_utils.py:471] 2024-07-17 06:42:13,118 >> Configuration saved in ./ft-summarization/checkpoint-17946/config.json\n",
      "[INFO|configuration_utils.py:697] 2024-07-17 06:42:13,118 >> Configuration saved in ./ft-summarization/checkpoint-17946/generation_config.json\n",
      "[INFO|modeling_utils.py:2590] 2024-07-17 06:42:13,260 >> Model weights saved in ./ft-summarization/checkpoint-17946/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-07-17 06:42:13,261 >> tokenizer config file saved in ./ft-summarization/checkpoint-17946/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-07-17 06:42:13,262 >> Special tokens file saved in ./ft-summarization/checkpoint-17946/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-07-17 06:42:13,262 >> Copy vocab file to ./ft-summarization/checkpoint-17946/spiece.model\n",
      "[INFO|configuration_utils.py:125] 2024-07-17 06:42:13,268 >> Configuration saved in ./ft-summarization/checkpoint-17946/gaudi_config.json\n",
      "[2024-07-17 06:42:13,341] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step17946 is about to be saved!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:42:13,352] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./ft-summarization/checkpoint-17946/global_step17946/mp_rank_00_model_states.pt\n",
      "[2024-07-17 06:42:13,352] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-17946/global_step17946/mp_rank_00_model_states.pt...\n",
      "[2024-07-17 06:42:13,575] [INFO] [torch_checkpoint_engine.py:24:save] [Torch] Saved ./ft-summarization/checkpoint-17946/global_step17946/mp_rank_00_model_states.pt.\n",
      "[2024-07-17 06:42:13,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-17946/global_step17946/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-07-17 06:42:13,709] [INFO] [torch_checkpoint_engine.py:24:save] [Torch] Saved ./ft-summarization/checkpoint-17946/global_step17946/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-07-17 06:42:13,710] [INFO] [engine.py:3531:_save_zero_checkpoint] zero checkpoint saved ./ft-summarization/checkpoint-17946/global_step17946/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-07-17 06:42:13,710] [INFO] [torch_checkpoint_engine.py:34:commit] [Torch] Checkpoint global_step17946 is ready now!\n",
      "{'loss': 1.0405, 'grad_norm': 0.4648004472255707, 'learning_rate': 1.6566365763958543e-05, 'epoch': 2.01, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0419, 'grad_norm': 0.47865530848503113, 'learning_rate': 1.563765370184628e-05, 'epoch': 2.06, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0324, 'grad_norm': 0.47634246945381165, 'learning_rate': 1.4708941639734017e-05, 'epoch': 2.12, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0375, 'grad_norm': 0.4815964698791504, 'learning_rate': 1.3780229577621757e-05, 'epoch': 2.17, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0426, 'grad_norm': 0.48767441511154175, 'learning_rate': 1.2851517515509492e-05, 'epoch': 2.23, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0443, 'grad_norm': 0.5688492059707642, 'learning_rate': 1.1922805453397229e-05, 'epoch': 2.28, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.027, 'grad_norm': 0.5737971663475037, 'learning_rate': 1.0994093391284967e-05, 'epoch': 2.34, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.031, 'grad_norm': 0.6082027554512024, 'learning_rate': 1.0065381329172704e-05, 'epoch': 2.4, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0412, 'grad_norm': 0.5090895891189575, 'learning_rate': 9.13666926706044e-06, 'epoch': 2.45, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0346, 'grad_norm': 0.4776914119720459, 'learning_rate': 8.207957204948179e-06, 'epoch': 2.51, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0331, 'grad_norm': 0.48565298318862915, 'learning_rate': 7.279245142835915e-06, 'epoch': 2.56, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0429, 'grad_norm': 0.5141371488571167, 'learning_rate': 6.350533080723653e-06, 'epoch': 2.62, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0312, 'grad_norm': 0.4711679518222809, 'learning_rate': 5.42182101861139e-06, 'epoch': 2.67, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0342, 'grad_norm': 0.49372974038124084, 'learning_rate': 4.493108956499127e-06, 'epoch': 2.73, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0349, 'grad_norm': 0.5454219579696655, 'learning_rate': 3.564396894386864e-06, 'epoch': 2.79, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0413, 'grad_norm': 0.5650849342346191, 'learning_rate': 2.6356848322746018e-06, 'epoch': 2.84, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.037, 'grad_norm': 0.4788304269313812, 'learning_rate': 1.7069727701623388e-06, 'epoch': 2.9, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "{'loss': 1.0359, 'grad_norm': 0.4854757487773895, 'learning_rate': 7.782607080500762e-07, 'epoch': 2.95, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "100%|████████████████████████████████████▉| 26918/26919 [22:19<00:00, 18.10it/s][INFO|trainer.py:1610] 2024-07-17 06:49:38,198 >> Saving model checkpoint to ./ft-summarization/checkpoint-26919\n",
      "[INFO|configuration_utils.py:471] 2024-07-17 06:49:38,199 >> Configuration saved in ./ft-summarization/checkpoint-26919/config.json\n",
      "[INFO|configuration_utils.py:697] 2024-07-17 06:49:38,200 >> Configuration saved in ./ft-summarization/checkpoint-26919/generation_config.json\n",
      "[INFO|modeling_utils.py:2590] 2024-07-17 06:49:38,340 >> Model weights saved in ./ft-summarization/checkpoint-26919/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-07-17 06:49:38,341 >> tokenizer config file saved in ./ft-summarization/checkpoint-26919/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-07-17 06:49:38,341 >> Special tokens file saved in ./ft-summarization/checkpoint-26919/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-07-17 06:49:38,342 >> Copy vocab file to ./ft-summarization/checkpoint-26919/spiece.model\n",
      "[INFO|configuration_utils.py:125] 2024-07-17 06:49:38,347 >> Configuration saved in ./ft-summarization/checkpoint-26919/gaudi_config.json\n",
      "[2024-07-17 06:49:38,348] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step26919 is about to be saved!\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1880: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "[2024-07-17 06:49:38,360] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ./ft-summarization/checkpoint-26919/global_step26919/mp_rank_00_model_states.pt\n",
      "[2024-07-17 06:49:38,360] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-26919/global_step26919/mp_rank_00_model_states.pt...\n",
      "[2024-07-17 06:49:38,582] [INFO] [torch_checkpoint_engine.py:24:save] [Torch] Saved ./ft-summarization/checkpoint-26919/global_step26919/mp_rank_00_model_states.pt.\n",
      "[2024-07-17 06:49:38,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./ft-summarization/checkpoint-26919/global_step26919/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2024-07-17 06:49:38,724] [INFO] [torch_checkpoint_engine.py:24:save] [Torch] Saved ./ft-summarization/checkpoint-26919/global_step26919/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2024-07-17 06:49:38,724] [INFO] [engine.py:3531:_save_zero_checkpoint] zero checkpoint saved ./ft-summarization/checkpoint-26919/global_step26919/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2024-07-17 06:49:38,725] [INFO] [torch_checkpoint_engine.py:34:commit] [Torch] Checkpoint global_step26919 is ready now!\n",
      "[INFO|trainer.py:1067] 2024-07-17 06:49:38,765 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1339.7116, 'train_samples_per_second': 646.017, 'train_steps_per_second': 20.19, 'train_loss': 1.0571916260572807, 'epoch': 3.0, 'memory_allocated (GB)': 4.91, 'max_memory_allocated (GB)': 5.06, 'total_memory_available (GB)': 94.62}\n",
      "100%|█████████████████████████████████████| 26919/26919 [22:19<00:00, 20.09it/s]\n",
      "[INFO|trainer.py:1610] 2024-07-17 06:49:38,840 >> Saving model checkpoint to ./ft-summarization\n",
      "[INFO|configuration_utils.py:471] 2024-07-17 06:49:38,841 >> Configuration saved in ./ft-summarization/config.json\n",
      "[INFO|configuration_utils.py:697] 2024-07-17 06:49:38,841 >> Configuration saved in ./ft-summarization/generation_config.json\n",
      "[INFO|modeling_utils.py:2590] 2024-07-17 06:49:38,993 >> Model weights saved in ./ft-summarization/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2488] 2024-07-17 06:49:38,994 >> tokenizer config file saved in ./ft-summarization/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2497] 2024-07-17 06:49:38,995 >> Special tokens file saved in ./ft-summarization/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-07-17 06:49:38,995 >> Copy vocab file to ./ft-summarization/spiece.model\n",
      "[INFO|configuration_utils.py:125] 2024-07-17 06:49:39,001 >> Configuration saved in ./ft-summarization/gaudi_config.json\n",
      "***** train metrics *****\n",
      "  epoch                       =         3.0\n",
      "  max_memory_allocated (GB)   =        5.06\n",
      "  memory_allocated (GB)       =        4.91\n",
      "  total_flos                  = 217155573GF\n",
      "  total_memory_available (GB) =       94.62\n",
      "  train_loss                  =      1.0572\n",
      "  train_runtime               =  0:22:19.71\n",
      "  train_samples               =      287113\n",
      "  train_samples_per_second    =     646.017\n",
      "  train_steps_per_second      =       20.19\n",
      "[INFO|modelcard.py:450] 2024-07-17 06:49:39,394 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}, 'dataset': {'name': 'cnn_dailymail 3.0.0', 'type': 'cnn_dailymail', 'args': '3.0.0'}}\n",
      "[2024-07-17 06:49:48,290] [INFO] [launch.py:348:main] Process 7348 exits successfully.\n",
      "[2024-07-17 06:49:48,290] [INFO] [launch.py:348:main] Process 7345 exits successfully.\n",
      "[2024-07-17 06:49:48,291] [INFO] [launch.py:348:main] Process 7344 exits successfully.\n",
      "[2024-07-17 06:49:49,292] [INFO] [launch.py:348:main] Process 7341 exits successfully.\n",
      "[2024-07-17 06:49:49,292] [INFO] [launch.py:348:main] Process 7347 exits successfully.\n",
      "[2024-07-17 06:49:49,292] [INFO] [launch.py:348:main] Process 7342 exits successfully.\n",
      "[2024-07-17 06:49:49,292] [INFO] [launch.py:348:main] Process 7343 exits successfully.\n",
      "[2024-07-17 06:49:49,292] [INFO] [launch.py:348:main] Process 7346 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!python ../gaudi_spawn.py \\\n",
    "--world_size 8 --use_deepspeed run_summarization.py \\\n",
    "--model_name_or_path t5-small \\\n",
    "--do_train \\\n",
    "--dataset_name cnn_dailymail \\\n",
    "--dataset_config '\"3.0.0\"' \\\n",
    "--source_prefix '\"summarize: \"' \\\n",
    "--output_dir ./ft-summarization \\\n",
    "--per_device_train_batch_size 4 \\\n",
    "--per_device_eval_batch_size 4 \\\n",
    "--overwrite_output_dir \\\n",
    "--predict_with_generate \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs_for_training \\\n",
    "--gaudi_config_name Habana/t5 \\\n",
    "--ignore_pad_token_for_loss False \\\n",
    "--pad_to_max_length \\\n",
    "--save_strategy epoch \\\n",
    "--report_to none \\\n",
    "--throughput_warmup_steps 3 \\\n",
    "--deepspeed ./ds_config.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60f93c0b-04bb-4fba-adb8-e18af01fd324",
   "metadata": {},
   "source": [
    "### After fine tuning, let's look at the results\n",
    "This fine tuned model has created the new `pytorch_model.bin` and the global_step.. folder contains the checkpoints that will be used in the infernece in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f34cdf3d-118e-4f80-a83d-2237c52f19c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization/ft-summarization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ./ft-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b035a826-201f-4a42-9222-5a9639cc44ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 217824\n",
      "drwxr-xr-x 5 root root      4096 Jul 17 06:49 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 4 root root      4096 Jul 17 06:23 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 root root      1182 Jul 17 06:49 README.md\n",
      "-rw-r--r-- 1 root root       355 Jul 17 06:49 all_results.json\n",
      "drwxr-xr-x 3 root root      4096 Jul 17 06:42 \u001b[01;34mcheckpoint-17946\u001b[0m/\n",
      "drwxr-xr-x 3 root root      4096 Jul 17 06:49 \u001b[01;34mcheckpoint-26919\u001b[0m/\n",
      "drwxr-xr-x 3 root root      4096 Jul 17 06:34 \u001b[01;34mcheckpoint-8973\u001b[0m/\n",
      "-rw-r--r-- 1 root root      1503 Jul 17 06:49 config.json\n",
      "-rw-r--r-- 1 root root       247 Jul 17 06:49 gaudi_config.json\n",
      "-rw-r--r-- 1 root root       588 Jul 17 06:49 generation_config.json\n",
      "-rw-r--r-- 1 root root 219726224 Jul 17 06:49 model.safetensors\n",
      "-rw-r--r-- 1 root root      2543 Jul 17 06:49 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root    791656 Jul 17 06:49 spiece.model\n",
      "-rw-r--r-- 1 root root   2422434 Jul 17 06:49 tokenizer.json\n",
      "-rw-r--r-- 1 root root     20746 Jul 17 06:49 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root       355 Jul 17 06:49 train_results.json\n",
      "-rw-r--r-- 1 root root     16585 Jul 17 06:49 trainer_state.json\n",
      "-rw-r--r-- 1 root root      7480 Jul 17 06:49 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "%ls -al"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f75cd58-cca9-4152-889a-75fbfdc24b07",
   "metadata": {},
   "source": [
    "## Inference Summarization using the Pipeline\n",
    "Now we can run the summarization using Hugging Face Pipeline call with the fine tuned model.  In this case we will point to the model that we fine tuned.   Remember that if you used t5-small to do the Fine Tuning, be sure to change the `model_to_finetune` to \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e145ca22-b2d0-49e5-89ee-0056adf8f54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Input: summarize: Introduction: The Strategic Arms Limitation Talks II (SALT II) treaty, signed on June 18, 1979, between the United States and the Soviet Union, marked a significant milestone in nuclear arms control efforts during the Cold War era. Building upon its predecessor, SALT I, the treaty aimed to curb the arms race and reduce the risk of nuclear conflict between the superpowers. Key Provisions: SALT II encompassed several crucial provisions. It placed limits on strategic offensive arms, including intercontinental ballistic missiles (ICBMs), submarine-launched ballistic missiles (SLBMs), and heavy bombers. The agreement specified the maximum number of deployed warheads and launchers each party could possess. Verification and Compliance: To ensure compliance, the treaty established comprehensive verification measures. This involved regular exchanges of data, on-site inspections, and monitoring activities by both nations. These measures sought to enhance transparency, foster trust, and prevent either side from gaining a significant advantage in terms of strategic nuclear capabilities. Ratification and Challenges: Although both the United States and the Soviet Union signed the treaty, its ratification faced considerable challenges. The political landscape changed when the Soviet Union invaded Afghanistan in 1979, leading to a deterioration of U.S.-Soviet relations. As a result, the United States never ratified the treaty formally, rendering it non-binding. However, both nations pledged to adhere to its principles, effectively implementing its provisions on a voluntary basis. Legacy and Impact: Despite the treaty's non-ratification, SALT II's legacy and impact were significant. It set the stage for subsequent arms control negotiations, providing a framework for future agreements such as the Intermediate-Range Nuclear Forces (INF) Treaty and the Strategic Arms Reduction Treaty (START). SALT II demonstrated the potential for cooperation between the superpowers and laid the groundwork for continued dialogue aimed at reducing the nuclear threat globally.\n",
      "\n",
      "------------------------------------------------------------\n",
      "Result: [{'summary_text': 'The Strategic Arms Limitation Talks II (SALT II) treaty was signed on June 18, 1979 . It aimed to curb the arms race and reduce the risk of conflict between the superpowers . The agreement specified the maximum number of deployed warheads and launchers each party could possess .'}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch\n",
    "\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load model to fine-tune and its tokenizer\n",
    "model_to_finetune = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_to_finetune)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_to_finetune)\n",
    "\n",
    "# Point to the ft-summarization folder with the fine-tuned model\n",
    "path_to_local_model = \"/root/Gaudi2-Workshop/LLM-Training/optimum-habana/examples/summarization/ft-summarization\"\n",
    "\n",
    "# Instantiate pipeline from local repo, if you did not run the fine tuning step above, you can change: model=model_to_finetune\n",
    "summarization_pipeline = pipeline(task=\"summarization\", model=path_to_local_model, device=\"hpu\", torch_dtype=torch.bfloat16, min_length=50, max_length=100)\n",
    "\n",
    "\n",
    "#text_to_summarize = \"summarize: Photosynthesis involves a series of complex reactions that take place within specialized organelles called chloroplasts in plant cells. It can be broadly divided into two stages: the light-dependent reactions and the light-independent reactions, also known as the Calvin cycle.  Light-Dependent Reactions: During the light-dependent reactions, chlorophyll pigments within the thylakoid membranes of the chloroplasts absorb light energy. This energy is utilized to split water molecules into oxygen, protons (H+), and electrons. Oxygen is released as a byproduct, while protons and electrons are transported through an electron transport chain, generating ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate).  Light-Independent Reactions (Calvin Cycle):  The ATP and NADPH produced in the light-dependent reactions are utilized in the Calvin cycle, which takes place in the stroma of the chloroplasts. In this cycle, carbon dioxide from the atmosphere combines with the stored energy in the form of ATP and NADPH to produce glucose. This glucose serves as a building block for other carbohydrates and organic compounds. Photosynthesis is a complex process that enables plants, algae, and some bacteria to convert light energy into chemical energy, facilitating the sustenance of life on Earth. It involves the interplay of light-dependent reactions, which generate ATP and NADPH, and the light-independent reactions or the Calvin cycle, which utilize the produced energy to fix carbon dioxide and produce glucose. Enhancing our understanding of photosynthesis and its underlying mechanisms holds the key to various applications, including improving crop yields, developing sustainable bioenergy sources, and addressing environmental challenges.\"\n",
    "text_to_summarize = \"summarize: Introduction: The Strategic Arms Limitation Talks II (SALT II) treaty, signed on June 18, 1979, between the United States and the Soviet Union, marked a significant milestone in nuclear arms control efforts during the Cold War era. Building upon its predecessor, SALT I, the treaty aimed to curb the arms race and reduce the risk of nuclear conflict between the superpowers. Key Provisions: SALT II encompassed several crucial provisions. It placed limits on strategic offensive arms, including intercontinental ballistic missiles (ICBMs), submarine-launched ballistic missiles (SLBMs), and heavy bombers. The agreement specified the maximum number of deployed warheads and launchers each party could possess. Verification and Compliance: To ensure compliance, the treaty established comprehensive verification measures. This involved regular exchanges of data, on-site inspections, and monitoring activities by both nations. These measures sought to enhance transparency, foster trust, and prevent either side from gaining a significant advantage in terms of strategic nuclear capabilities. Ratification and Challenges: Although both the United States and the Soviet Union signed the treaty, its ratification faced considerable challenges. The political landscape changed when the Soviet Union invaded Afghanistan in 1979, leading to a deterioration of U.S.-Soviet relations. As a result, the United States never ratified the treaty formally, rendering it non-binding. However, both nations pledged to adhere to its principles, effectively implementing its provisions on a voluntary basis. Legacy and Impact: Despite the treaty's non-ratification, SALT II's legacy and impact were significant. It set the stage for subsequent arms control negotiations, providing a framework for future agreements such as the Intermediate-Range Nuclear Forces (INF) Treaty and the Strategic Arms Reduction Treaty (START). SALT II demonstrated the potential for cooperation between the superpowers and laid the groundwork for continued dialogue aimed at reducing the nuclear threat globally.\"\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Input:\", text_to_summarize)\n",
    "print()\n",
    "\n",
    "# Now we call the pipline \n",
    "result = summarization_pipeline(text_to_summarize)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Result:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21e4d82-70a4-4de6-855b-1d3b906c538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run additional inference examples, the jupyter notebook requires that the kernel be restarted.  this `exit()` command will restart the kernel and allow another infernece run.\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5c8f0-6622-4823-943c-dc7a633f75fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

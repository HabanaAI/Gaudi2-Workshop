{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68877607",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.  \n",
    "Copyright (c) 2017, Pytorch contributors All rights reserved.\n",
    "## BSD 3-Clause License\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16169a66",
   "metadata": {},
   "source": [
    "# ResNet50 for PyTorch with GPU Migration\n",
    "\n",
    "In this notebook we will demonstrate ResNet50 model which has been enabled using an experimental feature called GPU migration and it can be trained using Pytorch on 8 HPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666669b",
   "metadata": {},
   "source": [
    "#### Clone the Model-References repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/habanaai/Model-References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bace79a",
   "metadata": {},
   "source": [
    "#### Set the ENV variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41f0f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=/root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision:/usr/lib/habanalabs/:/root\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH=/root/Gaudi2-Workshop/Model-Migration/Model-References:/usr/lib/habanalabs/:/root\n",
    "%set_env PYTHON=/usr/bin/python3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64847f32",
   "metadata": {},
   "source": [
    "#### Naviagte to the model to begin the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2212748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision\n"
     ]
    }
   ],
   "source": [
    "%cd /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a208072",
   "metadata": {},
   "source": [
    "#### Download dataset\n",
    "ImageNet 2012 dataset needs to be organized according to PyTorch requirements, and as specified in the scripts of [imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea291f86",
   "metadata": {},
   "source": [
    "#### Run the following command to start multi-HPU training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b92d4",
   "metadata": {},
   "source": [
    "```bash\n",
    "GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 8 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=/root/software/data/pytorch/imagenet/ILSVRC2012 --workers=8 --epochs=1 --opt=sgd --amp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8607d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-07/16-25-46/gpu_migration_9020.log\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-07/16-25-46/gpu_migration_9021.log\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-07/16-25-46/gpu_migration_9026.log\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-07/16-25-46/gpu_migration_9027.log\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-07/16-25-46/gpu_migration_9025.log\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-07/16-25-46/gpu_migration_9024.log\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-07/16-25-46/gpu_migration_9023.log\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:36: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2023-06-07/16-25-46/gpu_migration_9022.log\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=1, ) --> torch.hpu.set_device(hpu:1)\n",
      "\u001b[0m\n",
      "| distributed init (rank 1): env://\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=1, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=6, ) --> torch.hpu.set_device(hpu:6)\n",
      "\u001b[0m\n",
      "| distributed init (rank 6): env://\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=0, ) --> torch.hpu.set_device(hpu:0)\n",
      "\u001b[0m\n",
      "| distributed init (rank 0): env://\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=6, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=0, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=5, ) --> torch.hpu.set_device(hpu:5)\n",
      "\u001b[0m\n",
      "| distributed init (rank 5): env://\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=5, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=4, ) --> torch.hpu.set_device(hpu:4)\n",
      "\u001b[0m\n",
      "| distributed init (rank 4): env://\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=7, ) --> torch.hpu.set_device(hpu:7)\n",
      "\u001b[0m\n",
      "| distributed init (rank 7): env://\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=4, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=7, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=3, ) --> torch.hpu.set_device(hpu:3)\n",
      "\u001b[0m\n",
      "| distributed init (rank 3): env://\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=3, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=2, ) --> torch.hpu.set_device(hpu:2)\n",
      "\u001b[0m\n",
      "| distributed init (rank 2): env://\n",
      "[2023-06-07 16:25:46] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=0:30:00, world_size=8, rank=2, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "Namespace(amp=True, augmix_severity=3, auto_augment=None, batch_size=256, bias_weight_decay=None, cache_dataset=False, clip_grad_norm=None, cutmix_alpha=0.0, data_path='/root/software/data/pytorch/imagenet/ILSVRC2012', device='cuda', dist_backend='nccl', dist_url='env://', distributed=True, dl_worker_type='HABANA', epochs=1, gpu=0, interpolation='bilinear', label_smoothing=0.0, lr=0.1, lr_gamma=0.1, lr_min=0.0, lr_scheduler='custom_lr', lr_step_size=30, lr_warmup_decay=0.01, lr_warmup_epochs=0, lr_warmup_method='constant', mixup_alpha=0.0, model='resnet50', model_ema=False, model_ema_decay=0.99998, model_ema_steps=32, momentum=0.9, norm_weight_decay=None, opt='sgd', output_dir='.', print_freq=10, ra_magnitude=9, ra_reps=3, ra_sampler=False, random_erase=0.0, rank=0, resume='', seed=123, start_epoch=0, sync_bn=False, test_only=False, train_crop_size=224, transformer_embedding_decay=None, use_deterministic_algorithms=False, val_crop_size=224, val_resize_size=256, weight_decay=0.0001, weights=None, workers=8, world_size=8)\n",
      "[2023-06-07 16:25:46] /usr/local/lib/python3.8/dist-packages/torch/random.py:40\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=123, ) --> torch.hpu.random.manual_seed_all(123)\n",
      "\u001b[0m\n",
      "Loading data\n",
      "Loading training data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 4.0025529861450195\n",
      "Loading validation data\n",
      "Creating data loaders\n",
      "MediaDataloader 1/8 seed : 760445125\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 160\n",
      "CPU RAM = 1056438708 KB \n",
      "============================================================================================ \n",
      "HabanaDataLoader device type  4\n",
      "MediaDataloader 3/8 seed : 770065667\n",
      "MediaDataloader 2/8 seed : 789914671\n",
      "MediaDataloader 5/8 seed : 839157043\n",
      "Warning: Updated shuffle to True as sampler is DistributedSampler with shuffle True\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x7f27a1961b20>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform RandomResizedCrop: Random Crop,Resize w:h  224 224  scale:  (0.08, 1.0)  ratio:  (0.75, 1.3333333333333333)  interpolation:  InterpolationMode.BILINEAR\n",
      "transform RandomHorizontalFlip: probability  0.5\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 8 instance id 0\n",
      "MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name HPUMediaPipe:1\n",
      "MediaDataloader 0/8 seed : 846816242\n",
      "Decode ResizedCrop w:h 224 224\n",
      "MediaDataloader shuffle is  True\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... MediaDataloader 7/8 seed : 852193832\n",
      "Done!\n",
      "MediaDataloader 6/8 seed : 968172507\n",
      "MediaDataloader 4/8 seed : 1087207275\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 1273840 classes 996\n",
      "num_slices 8 slice_index 0\n",
      "random seed used  846816242\n",
      "sliced media files/labels 159230\n",
      "Finding largest file ...\n",
      "largest file is  /root/software/data/pytorch/imagenet/ILSVRC2012/train/n07717556/n07717556_14716.JPEG\n",
      "Running with Habana media DataLoader with num_instances = 8, instance_id = 0.\n",
      "HabanaDataLoader device type  4\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x7f27a19c55e0>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform Resize: w:h  256 256  interpolation:  InterpolationMode.BILINEAR  max_size:  None\n",
      "transform CenterCrop: w:h  224 224\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 8 instance id 0\n",
      "MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name HPUMediaPipe:2\n",
      "MediaDataloader 0/8 seed : 1740130358\n",
      "Decode w:h  256 256  , Crop disabled\n",
      "MediaDataloader shuffle is  False\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... MediaDataloader 7/8 seed : 1982086831\n",
      "MediaDataloader 5/8 seed : 2015342167\n",
      "Done!\n",
      "MediaDataloader 4/8 seed : 42062127\n",
      "MediaDataloader 3/8 seed : 1069894516\n",
      "MediaDataloader 1/8 seed : 1109235316\n",
      "MediaDataloader 2/8 seed : 1154518119\n",
      "MediaDataloader 6/8 seed : 1236864741\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 50000 classes 1000\n",
      "num_slices 8 slice_index 0\n",
      "random seed used  1740130358\n",
      "sliced media files/labels 6250\n",
      "Finding largest file ...\n",
      "largest file is  /root/software/data/pytorch/imagenet/ILSVRC2012/val/n01630670/ILSVRC2012_val_00046430.JPEG\n",
      "Running with Habana media DataLoader with num_instances = 8, instance_id = 0.\n",
      "Creating model\n",
      "[2023-06-07 16:27:22] /usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/core/weight_sharing.py:173\n",
      "    [context]:     result = self.original_to(*args, **kwargs)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'), None, False), kwargs={}, ) --> torch.Tensor.to(args=('hpu', None, False), kwargs={})\n",
      "\u001b[0m\n",
      "[2023-06-07 16:27:22] train.py:306\n",
      "    [context]:     scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.amp.GradScaler.__init__(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True, ) --> set enabled to Flase\n",
      "\u001b[0m\n",
      "[2023-06-07 16:27:22] train.py:349\n",
      "    [context]:         model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], broadcast_buffers=False, gradient_as_bucket_view=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.nn.parallel.DistributedDataParallel.__init__(module=module, device_ids=[0], output_device=None, dim=0, broadcast_buffers=False, process_group=None, bucket_cap_mb=25, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=True, static_graph=False, ) --> change device_ids and output_device to None\n",
      "\u001b[0m\n",
      "Start training\n",
      "[2023-06-07 16:27:24] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:113\n",
      "    [context]:         if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Shuffling ... Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Done!\n",
      "[2023-06-07 16:27:25] train.py:32\n",
      "    [context]:         image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2023-06-07 16:27:25] train.py:33\n",
      "    [context]:         with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.autocast.__init__(device_type=cuda, dtype=torch.float16, enabled=True, cache_enabled=True, ) --> torch.autocast.__init__(device_type=hpu, dtype=None, enabled=True, cache_enabled=True, )\n",
      "\u001b[0m\n",
      "[2023-06-07 16:27:25] /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/common.py:7\n",
      "    [context]:     return not (torch.cuda.is_available() or find_spec('torch_xla'))\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2023-06-07 16:27:36] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:137\n",
      "    [context]:                 if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2023-06-07 16:27:36] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:146\n",
      "    [context]:                             memory=torch.cuda.max_memory_allocated() / MB,\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.max_memory_allocated(device=None, ) --> torch.hpu.max_memory_allocated(device=None)\n",
      "\u001b[0m\n",
      "Epoch: [0]  [  0/622]  eta: 2:05:25  lr: 0.1  img/s: 21.15946231165127  loss: 7.1341 (7.1341)  acc1: 0.0000 (0.0000)  acc5: 1.1719 (1.1719)  time: 12.0986  data: 1.0983  max mem: 15838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 10/622]  eta: 0:30:27  lr: 0.1  img/s: 123.30808523404552  loss: 7.1341 (7.1421)  acc1: 0.0000 (0.1953)  acc5: 1.1719 (1.1719)  time: 2.9867  data: 0.1094  max mem: 16966\n",
      "Epoch: [0]  [ 20/622]  eta: 0:15:55  lr: 0.1  img/s: 5146.506143980567  loss: 7.1502 (7.1801)  acc1: 0.0000 (0.1302)  acc5: 1.1719 (0.9115)  time: 1.0613  data: 0.0073  max mem: 16966\n",
      "Epoch: [0]  [ 30/622]  eta: 0:10:45  lr: 0.1  img/s: 5240.960702867602  loss: 7.1341 (7.1405)  acc1: 0.0000 (0.1953)  acc5: 1.1719 (1.3672)  time: 0.0465  data: 0.0027  max mem: 16966\n",
      "Epoch: [0]  [ 40/622]  eta: 0:08:06  lr: 0.1  img/s: 4989.253908607789  loss: 7.1410 (7.1406)  acc1: 0.3906 (0.2344)  acc5: 1.1719 (1.1719)  time: 0.0472  data: 0.0014  max mem: 16966\n",
      "Epoch: [0]  [ 50/622]  eta: 0:06:29  lr: 0.1  img/s: 4990.452762419641  loss: 7.1341 (7.1110)  acc1: 0.0000 (0.1953)  acc5: 0.3906 (1.0417)  time: 0.0485  data: 0.0036  max mem: 16966\n",
      "Epoch: [0]  [ 60/622]  eta: 0:05:25  lr: 0.1  img/s: 4179.821571352384  loss: 7.1341 (7.0770)  acc1: 0.3906 (0.2232)  acc5: 0.7812 (1.0045)  time: 0.0534  data: 0.0110  max mem: 16966\n",
      "Epoch: [0]  [ 70/622]  eta: 0:04:38  lr: 0.1  img/s: 5264.529348718215  loss: 7.0217 (7.0531)  acc1: 0.3906 (0.2441)  acc5: 0.7812 (1.0254)  time: 0.0522  data: 0.0091  max mem: 16966\n",
      "Epoch: [0]  [ 80/622]  eta: 0:04:02  lr: 0.1  img/s: 5255.0322621061205  loss: 7.0217 (7.0476)  acc1: 0.3906 (0.2170)  acc5: 0.7812 (0.9549)  time: 0.0460  data: 0.0021  max mem: 16966\n",
      "Epoch: [0]  [ 90/622]  eta: 0:03:34  lr: 0.1  img/s: 5236.99094231438  loss: 7.0040 (7.0271)  acc1: 0.3906 (0.3125)  acc5: 0.7812 (1.0938)  time: 0.0460  data: 0.0017  max mem: 16966\n",
      "Epoch: [0]  [100/622]  eta: 0:03:12  lr: 0.1  img/s: 5266.217985564936  loss: 7.0040 (6.9987)  acc1: 0.3906 (0.3196)  acc5: 1.1719 (1.2074)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [110/622]  eta: 0:02:53  lr: 0.1  img/s: 5274.236987087293  loss: 6.9631 (6.9717)  acc1: 0.3906 (0.3906)  acc5: 1.1719 (1.2695)  time: 0.0458  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [120/622]  eta: 0:02:38  lr: 0.1  img/s: 5171.018497337537  loss: 6.9631 (6.9509)  acc1: 0.3906 (0.4207)  acc5: 1.1719 (1.3522)  time: 0.0462  data: 0.0021  max mem: 16966\n",
      "Epoch: [0]  [130/622]  eta: 0:02:24  lr: 0.1  img/s: 4948.267224594285  loss: 6.8854 (6.9306)  acc1: 0.3906 (0.4464)  acc5: 1.1719 (1.4788)  time: 0.0478  data: 0.0025  max mem: 16966\n",
      "Epoch: [0]  [140/622]  eta: 0:02:13  lr: 0.1  img/s: 5163.754538608106  loss: 6.8854 (6.9069)  acc1: 0.3906 (0.4167)  acc5: 1.1719 (1.5104)  time: 0.0478  data: 0.0031  max mem: 16966\n",
      "Epoch: [0]  [150/622]  eta: 0:02:03  lr: 0.1  img/s: 5266.081098511656  loss: 6.8734 (6.8858)  acc1: 0.3906 (0.3906)  acc5: 1.1719 (1.6846)  time: 0.0463  data: 0.0034  max mem: 16966\n",
      "Epoch: [0]  [160/622]  eta: 0:01:54  lr: 0.1  img/s: 5273.040348949018  loss: 6.8734 (6.8669)  acc1: 0.3906 (0.4596)  acc5: 1.9531 (1.8382)  time: 0.0459  data: 0.0021  max mem: 16966\n",
      "Epoch: [0]  [170/622]  eta: 0:01:46  lr: 0.1  img/s: 5016.9740839484575  loss: 6.8418 (6.8447)  acc1: 0.3906 (0.4557)  acc5: 1.9531 (1.9314)  time: 0.0471  data: 0.0010  max mem: 16966\n",
      "Epoch: [0]  [180/622]  eta: 0:01:39  lr: 0.1  img/s: 5247.892146991951  loss: 6.8418 (6.8259)  acc1: 0.3906 (0.4729)  acc5: 1.9531 (1.9942)  time: 0.0471  data: 0.0010  max mem: 16966\n",
      "Epoch: [0]  [190/622]  eta: 0:01:33  lr: 0.1  img/s: 5251.927265328163  loss: 6.7151 (6.8009)  acc1: 0.3906 (0.5078)  acc5: 1.9531 (2.0508)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [200/622]  eta: 0:01:27  lr: 0.1  img/s: 5261.959742699848  loss: 6.7019 (6.7766)  acc1: 0.3906 (0.5208)  acc5: 2.3438 (2.0833)  time: 0.0459  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [210/622]  eta: 0:01:22  lr: 0.1  img/s: 5214.855245686511  loss: 6.6743 (6.7563)  acc1: 0.3906 (0.5504)  acc5: 2.3438 (2.1484)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [220/622]  eta: 0:01:17  lr: 0.1  img/s: 5272.499191012008  loss: 6.6662 (6.7358)  acc1: 0.3906 (0.5774)  acc5: 2.3438 (2.2418)  time: 0.0462  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [230/622]  eta: 0:01:13  lr: 0.1  img/s: 5245.956364059197  loss: 6.5757 (6.7141)  acc1: 0.7812 (0.6673)  acc5: 2.3438 (2.3763)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [240/622]  eta: 0:01:09  lr: 0.1  img/s: 5115.1608280740265  loss: 6.5693 (6.6882)  acc1: 0.7812 (0.6875)  acc5: 2.7344 (2.4844)  time: 0.0467  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [250/622]  eta: 0:01:05  lr: 0.1  img/s: 4917.83015666185  loss: 6.5636 (6.6642)  acc1: 0.7812 (0.6911)  acc5: 3.1250 (2.6142)  time: 0.0485  data: 0.0022  max mem: 16966\n",
      "Epoch: [0]  [260/622]  eta: 0:01:01  lr: 0.1  img/s: 5276.082220536266  loss: 6.4872 (6.6405)  acc1: 0.7812 (0.7523)  acc5: 3.1250 (2.8935)  time: 0.0477  data: 0.0032  max mem: 16966\n",
      "Epoch: [0]  [270/622]  eta: 0:00:58  lr: 0.1  img/s: 4896.985003140023  loss: 6.4688 (6.6182)  acc1: 0.7812 (0.8092)  acc5: 3.1250 (2.9855)  time: 0.0476  data: 0.0023  max mem: 16966\n",
      "Epoch: [0]  [280/622]  eta: 0:00:55  lr: 0.1  img/s: 4978.949485825362  loss: 6.3320 (6.5884)  acc1: 1.1719 (0.9159)  acc5: 3.5156 (3.2732)  time: 0.0490  data: 0.0027  max mem: 16966\n",
      "Epoch: [0]  [290/622]  eta: 0:00:52  lr: 0.1  img/s: 5243.066886661488  loss: 6.3253 (6.5710)  acc1: 1.1719 (0.9896)  acc5: 3.5156 (3.3724)  time: 0.0474  data: 0.0030  max mem: 16966\n",
      "Epoch: [0]  [300/622]  eta: 0:00:49  lr: 0.1  img/s: 4923.846935177147  loss: 6.2896 (6.5507)  acc1: 1.1719 (1.0711)  acc5: 4.2969 (3.5912)  time: 0.0477  data: 0.0032  max mem: 16966\n",
      "Epoch: [0]  [310/622]  eta: 0:00:47  lr: 0.1  img/s: 5171.892741714071  loss: 6.2847 (6.5213)  acc1: 1.1719 (1.1475)  acc5: 4.2969 (3.8208)  time: 0.0479  data: 0.0036  max mem: 16966\n",
      "Epoch: [0]  [320/622]  eta: 0:00:44  lr: 0.1  img/s: 5236.89132837351  loss: 6.2142 (6.4991)  acc1: 1.1719 (1.2547)  acc5: 4.2969 (4.0483)  time: 0.0463  data: 0.0022  max mem: 16966\n",
      "Epoch: [0]  [330/622]  eta: 0:00:42  lr: 0.1  img/s: 5252.960143243627  loss: 6.0666 (6.4746)  acc1: 1.1719 (1.3212)  acc5: 5.0781 (4.2394)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [340/622]  eta: 0:00:40  lr: 0.1  img/s: 5247.327930315932  loss: 6.0664 (6.4517)  acc1: 1.5625 (1.4286)  acc5: 5.4688 (4.5312)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [350/622]  eta: 0:00:37  lr: 0.1  img/s: 5159.102523138661  loss: 6.0642 (6.4323)  acc1: 2.3438 (1.5191)  acc5: 5.4688 (4.7635)  time: 0.0465  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [360/622]  eta: 0:00:35  lr: 0.1  img/s: 5234.394555700719  loss: 6.0256 (6.4128)  acc1: 2.3438 (1.5836)  acc5: 5.8594 (5.0253)  time: 0.0465  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [370/622]  eta: 0:00:33  lr: 0.1  img/s: 5257.958119157133  loss: 6.0138 (6.3935)  acc1: 2.7344 (1.6345)  acc5: 6.2500 (5.2220)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [380/622]  eta: 0:00:32  lr: 0.1  img/s: 3480.8935144827346  loss: 5.9403 (6.3664)  acc1: 3.1250 (1.7127)  acc5: 10.1562 (5.4788)  time: 0.0584  data: 0.0056  max mem: 16966\n",
      "Epoch: [0]  [390/622]  eta: 0:00:30  lr: 0.1  img/s: 5135.380537584576  loss: 5.7880 (6.3433)  acc1: 3.5156 (1.7578)  acc5: 10.1562 (5.7129)  time: 0.0590  data: 0.0056  max mem: 16966\n",
      "Epoch: [0]  [400/622]  eta: 0:00:28  lr: 0.1  img/s: 4909.764018597573  loss: 5.7557 (6.3277)  acc1: 3.5156 (1.8007)  acc5: 10.5469 (5.9165)  time: 0.0482  data: 0.0024  max mem: 16966\n",
      "Epoch: [0]  [410/622]  eta: 0:00:26  lr: 0.1  img/s: 4976.13210590124  loss: 5.7532 (6.3087)  acc1: 3.5156 (1.8973)  acc5: 10.9375 (6.1849)  time: 0.0490  data: 0.0031  max mem: 16966\n",
      "Epoch: [0]  [420/622]  eta: 0:00:25  lr: 0.1  img/s: 5233.789867890582  loss: 5.7125 (6.2874)  acc1: 3.5156 (1.9985)  acc5: 11.3281 (6.4226)  time: 0.0473  data: 0.0017  max mem: 16966\n",
      "Epoch: [0]  [430/622]  eta: 0:00:23  lr: 0.1  img/s: 5272.7089088957355  loss: 5.7035 (6.2681)  acc1: 3.5156 (2.1129)  acc5: 11.3281 (6.6317)  time: 0.0459  data: 0.0010  max mem: 16966\n",
      "Epoch: [0]  [440/622]  eta: 0:00:22  lr: 0.1  img/s: 5277.205023495107  loss: 5.6782 (6.2520)  acc1: 3.5156 (2.1875)  acc5: 12.5000 (6.8490)  time: 0.0458  data: 0.0021  max mem: 16966\n",
      "Epoch: [0]  [450/622]  eta: 0:00:20  lr: 0.1  img/s: 5211.1752909544475  loss: 5.6710 (6.2329)  acc1: 3.9062 (2.3013)  acc5: 12.8906 (7.1247)  time: 0.0460  data: 0.0035  max mem: 16966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [460/622]  eta: 0:00:19  lr: 0.1  img/s: 5259.098979078055  loss: 5.6689 (6.2195)  acc1: 3.9062 (2.3604)  acc5: 14.0625 (7.2889)  time: 0.0461  data: 0.0023  max mem: 16966\n",
      "Epoch: [0]  [470/622]  eta: 0:00:17  lr: 0.1  img/s: 5229.459083523925  loss: 5.6102 (6.2022)  acc1: 4.6875 (2.4984)  acc5: 14.4531 (7.5033)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [480/622]  eta: 0:00:16  lr: 0.1  img/s: 5238.089501154462  loss: 5.6028 (6.1853)  acc1: 4.6875 (2.5351)  acc5: 14.4531 (7.6690)  time: 0.0461  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [490/622]  eta: 0:00:14  lr: 0.1  img/s: 5266.86894213488  loss: 5.5439 (6.1684)  acc1: 4.6875 (2.6172)  acc5: 14.8438 (7.9141)  time: 0.0459  data: 0.0022  max mem: 16966\n",
      "Epoch: [0]  [500/622]  eta: 0:00:13  lr: 0.1  img/s: 5258.596733694798  loss: 5.5297 (6.1512)  acc1: 4.6875 (2.7037)  acc5: 14.8438 (8.1265)  time: 0.0458  data: 0.0015  max mem: 16966\n",
      "Epoch: [0]  [510/622]  eta: 0:00:12  lr: 0.1  img/s: 5264.389968386454  loss: 5.4447 (6.1343)  acc1: 5.0781 (2.7719)  acc5: 15.2344 (8.3834)  time: 0.0458  data: 0.0014  max mem: 16966\n",
      "Epoch: [0]  [520/622]  eta: 0:00:11  lr: 0.1  img/s: 4953.31199298804  loss: 5.4358 (6.1185)  acc1: 5.0781 (2.8376)  acc5: 15.6250 (8.5495)  time: 0.0473  data: 0.0012  max mem: 16966\n",
      "Epoch: [0]  [530/622]  eta: 0:00:09  lr: 0.1  img/s: 5276.31037275236  loss: 5.3939 (6.1030)  acc1: 5.4688 (2.9152)  acc5: 15.6250 (8.7457)  time: 0.0473  data: 0.0016  max mem: 16966\n",
      "Epoch: [0]  [540/622]  eta: 0:00:08  lr: 0.1  img/s: 5263.989936204123  loss: 5.3895 (6.0854)  acc1: 5.8594 (3.0043)  acc5: 16.4062 (8.9205)  time: 0.0458  data: 0.0015  max mem: 16966\n",
      "Epoch: [0]  [550/622]  eta: 0:00:07  lr: 0.1  img/s: 5264.162845880807  loss: 5.3776 (6.0703)  acc1: 6.2500 (3.1110)  acc5: 16.4062 (9.1378)  time: 0.0459  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [560/622]  eta: 0:00:06  lr: 0.1  img/s: 5264.81845738051  loss: 5.3721 (6.0536)  acc1: 6.2500 (3.1798)  acc5: 17.1875 (9.2996)  time: 0.0459  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [570/622]  eta: 0:00:05  lr: 0.1  img/s: 5235.36949434844  loss: 5.3371 (6.0404)  acc1: 6.2500 (3.2058)  acc5: 17.1875 (9.4558)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [580/622]  eta: 0:00:04  lr: 0.1  img/s: 4059.9912428299726  loss: 5.3373 (6.0284)  acc1: 6.2500 (3.2243)  acc5: 17.1875 (9.5869)  time: 0.0533  data: 0.0072  max mem: 16966\n",
      "Epoch: [0]  [590/622]  eta: 0:00:03  lr: 0.1  img/s: 5190.970297029703  loss: 5.3371 (6.0111)  acc1: 6.2500 (3.3008)  acc5: 17.5781 (9.8503)  time: 0.0535  data: 0.0072  max mem: 16966\n",
      "Epoch: [0]  [600/622]  eta: 0:00:02  lr: 0.1  img/s: 5077.374451355663  loss: 5.2985 (5.9974)  acc1: 6.2500 (3.3043)  acc5: 18.3594 (9.9898)  time: 0.0471  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [610/622]  eta: 0:00:01  lr: 0.1  img/s: 5277.246521905593  loss: 5.2919 (5.9801)  acc1: 6.6406 (3.3896)  acc5: 18.3594 (10.2130)  time: 0.0467  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [620/622]  eta: 0:00:00  lr: 0.1  img/s: 5284.726882742892  loss: 5.2848 (5.9662)  acc1: 7.0312 (3.4722)  acc5: 18.3594 (10.4291)  time: 0.0458  data: 0.0008  max mem: 16966\n",
      "Epoch: [0] Total time: 0:01:03\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "[2023-06-07 16:28:27] train.py:82\n",
      "    [context]:             image = image.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2023-06-07 16:28:27] train.py:83\n",
      "    [context]:             target = target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "Test:   [ 0/25]  eta: 0:02:24  loss: 7.8349 (7.8349)  acc1: 0.7812 (0.7812)  acc5: 1.5625 (1.5625)  time: 5.7910  data: 0.0580  max mem: 16966\n",
      "Test:  Total time: 0:00:07\n",
      "[2023-06-07 16:28:34] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=(6400,), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=(6400,), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-07 16:28:34] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([25, 200.49833631515503],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([25, 200.49833631515503],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-07 16:28:34] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 3000.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 3000.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-07 16:28:34] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 10700.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 10700.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "Test:  Acc@1 0.363 Acc@5 1.807\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "[2023-06-07 16:28:34] train.py:415\n",
      "    [context]:                 checkpoint[\"scaler\"] = scaler.state_dict()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.amp.GradScaler.state_dict() --> return state in diable mode\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 0:01:15\r\n"
     ]
    }
   ],
   "source": [
    "!GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 8 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=/root/software/data/pytorch/imagenet/ILSVRC2012 --workers=8 --epochs=1 --opt=sgd --amp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

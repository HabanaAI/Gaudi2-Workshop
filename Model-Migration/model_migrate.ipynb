{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68877607",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.  \n",
    "Copyright (c) 2017, Pytorch contributors All rights reserved.\n",
    "## BSD 3-Clause License\n",
    "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16169a66",
   "metadata": {},
   "source": [
    "# ResNet50 for PyTorch with GPU Migration\n",
    "\n",
    "In this notebook we will demonstrate ResNet50 model which is based on open source implementation of ResNet50. It has been enabled using an experimental feature called GPU migration toolkit and it can be trained using Pytorch on 8 HPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666669b",
   "metadata": {},
   "source": [
    "#### Clone the Model-References repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a26ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Model-References'...\n",
      "remote: Enumerating objects: 18770, done.\u001b[K\n",
      "remote: Counting objects: 100% (4559/4559), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2021/2021), done.\u001b[K\n",
      "remote: Total 18770 (delta 2435), reused 4301 (delta 2238), pack-reused 14211\u001b[K\n",
      "Receiving objects: 100% (18770/18770), 119.22 MiB | 26.76 MiB/s, done.\n",
      "Resolving deltas: 100% (10121/10121), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/habanaai/Model-References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bace79a",
   "metadata": {},
   "source": [
    "#### Set the ENV variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41f0f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=/root/Gaudi2-Workshop/Model-Migration/Model-References:/usr/lib/habanalabs/:/root\n",
      "env: PYTHON=/usr/bin/python3.8\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH=/root/Gaudi2-Workshop/Model-Migration/Model-References:/usr/lib/habanalabs/:/root\n",
    "%set_env PYTHON=/usr/bin/python3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64847f32",
   "metadata": {},
   "source": [
    "#### Naviagte to the model to begin the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2212748b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a208072",
   "metadata": {},
   "source": [
    "#### Download dataset\n",
    "ImageNet 2012 dataset needs to be organized according to PyTorch requirements, and as specified in the scripts of [imagenet-multiGPU.torch](https://github.com/soumith/imagenet-multiGPU.torch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee70dc",
   "metadata": {},
   "source": [
    "#### Import GPU Migration Toolkit package and Habana Torch Library\n",
    "Look into train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c905c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4\timport habana_frameworks.torch.gpu_migration\n",
      "     5\timport datetime\n",
      "     6\timport os\n",
      "     7\timport time\n",
      "     8\timport warnings\n",
      "     9\t\n",
      "    10\timport presets\n",
      "    11\timport torch\n",
      "    12\timport torch.utils.data\n",
      "    13\timport torchvision\n",
      "    14\timport transforms\n",
      "    15\timport utils\n",
      "    16\timport habana_frameworks.torch.utils.experimental as htexp\n",
      "    17\tfrom sampler import RASampler\n",
      "    18\tfrom torch import nn\n",
      "    19\tfrom torch.utils.data.dataloader import default_collate\n",
      "    20\tfrom torchvision.transforms.functional import InterpolationMode\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 20 | tail -n 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed748205",
   "metadata": {},
   "source": [
    "#### Placing mark_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4e1aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    38\t        optimizer.zero_grad(set_to_none=True)\n",
      "    39\t        if scaler is not None:\n",
      "    40\t            scaler.scale(loss).backward()\n",
      "    41\t            htcore.mark_step()\n",
      "    42\t            if args.clip_grad_norm is not None:\n",
      "    43\t                # we should unscale the gradients of optimizer's assigned params if do gradient clipping\n",
      "    44\t                scaler.unscale_(optimizer)\n",
      "    45\t                nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
      "    46\t            scaler.step(optimizer)\n",
      "    47\t            htcore.mark_step()\n",
      "    48\t            scaler.update()\n",
      "    49\t        else:\n",
      "    50\t            loss.backward()\n",
      "    51\t            htcore.mark_step()\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cat -n train.py | head -n 51 | tail -n 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea291f86",
   "metadata": {},
   "source": [
    "#### Run the following command to start multi-HPU training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b92d4",
   "metadata": {},
   "source": [
    "```bash\n",
    "GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 8 train.py --batch-size=256 --model=resnet50 --device=cuda --data-path=/root/software/data/pytorch/imagenet/ILSVRC2012 --workers=8 --epochs=1 --opt=sgd --amp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8607d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-17 05:49:48,504] torch.distributed.run: [WARNING] \n",
      "[2024-07-17 05:49:48,504] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-07-17 05:49:48,504] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-07-17 05:49:48,504] torch.distributed.run: [WARNING] *****************************************\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-07-17/05-49-51/gpu_migration_700.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-07-17/05-49-51/gpu_migration_703.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-07-17/05-49-51/gpu_migration_699.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-07-17/05-49-51/gpu_migration_698.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-07-17/05-49-51/gpu_migration_697.log\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-07-17/05-49-51/gpu_migration_702.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-07-17/05-49-51/gpu_migration_696.log\n",
      "/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/gpu_migration/__init__.py:46: UserWarning: apex not installed, gpu_migration will not swap api for this package.\n",
      "  warnings.warn(\n",
      "gpu migration log will be saved to /var/log/habana_logs/gpu_migration_logs/2024-07-17/05-49-51/gpu_migration_701.log\n",
      "[2024-07-17 05:49:51] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=7, ) --> torch.hpu.set_device(hpu:7)\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:51] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=3, ) --> torch.hpu.set_device(hpu:3)\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:51] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=4, ) --> torch.hpu.set_device(hpu:4)\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:51] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=2, ) --> torch.hpu.set_device(hpu:2)\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:51] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=1, ) --> torch.hpu.set_device(hpu:1)\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:51] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=6, ) --> torch.hpu.set_device(hpu:6)\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:51] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=0, ) --> torch.hpu.set_device(hpu:0)\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:51] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:265\n",
      "    [context]:     torch.cuda.set_device(args.gpu)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.set_device(device=5, ) --> torch.hpu.set_device(hpu:5)\n",
      "\u001b[0m\n",
      "| distributed init (rank 4): env://\n",
      "| distributed init (rank 3): env://\n",
      "| distributed init (rank 7): env://\n",
      "[2024-07-17 05:49:52] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=None, world_size=8, rank=4, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:52] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=None, world_size=8, rank=3, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:52] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=None, world_size=8, rank=7, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "| distributed init (rank 2): env://\n",
      "[2024-07-17 05:49:52] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=None, world_size=8, rank=2, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:52] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:117\n",
      "    [context]:         backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:52] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:117\n",
      "    [context]:         backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:52] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:117\n",
      "    [context]:         backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:52] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:117\n",
      "    [context]:         backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "| distributed init (rank 1): env://\n",
      "[2024-07-17 05:49:52] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=None, world_size=8, rank=1, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "| distributed init (rank 6): env://\n",
      "[2024-07-17 05:49:52] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=None, world_size=8, rank=6, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "| distributed init (rank 5): env://\n",
      "[2024-07-17 05:49:52] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=None, world_size=8, rank=5, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "| distributed init (rank 0): env://\n",
      "[2024-07-17 05:49:52] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:269\n",
      "    [context]:     torch.distributed.init_process_group(\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.init_process_group(backend=nccl, init_method=env://, timeout=None, world_size=8, rank=0, store=None, group_name=, pg_options=None, ) --> change backend to hccl\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:52] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:117\n",
      "    [context]:         backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:52] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:117\n",
      "    [context]:         backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:52] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:117\n",
      "    [context]:         backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "[2024-07-17 05:49:52] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/torch_overwrites.py:117\n",
      "    [context]:         backend_name = group._get_backend_name() if group is not None else torch.distributed.get_backend()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.distributed.get_backend() --> change return value from hccl to nccl\n",
      "\u001b[0m\n",
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 1056433664 KB\n",
      "------------------------------------------------------------------------------\n",
      "Namespace(use_torch_compile=False, data_path='/root/data/huggingface/datasets/imagenet/ILSVRC2012', model='resnet50', device='cuda', batch_size=256, epochs=1, dl_worker_type='HABANA', workers=8, opt='sgd', lr=0.1, momentum=0.9, weight_decay=0.0001, norm_weight_decay=None, bias_weight_decay=None, transformer_embedding_decay=None, label_smoothing=0.0, mixup_alpha=0.0, cutmix_alpha=0.0, lr_scheduler='custom_lr', lr_warmup_epochs=0, lr_warmup_method='constant', lr_warmup_decay=0.01, lr_step_size=30, lr_gamma=0.1, lr_min=0.0, print_freq=10, output_dir='.', resume='', start_epoch=0, seed=123, cache_dataset=False, sync_bn=False, test_only=False, auto_augment=None, ra_magnitude=9, augmix_severity=3, random_erase=0.0, amp=True, world_size=8, dist_url='env://', model_ema=False, model_ema_steps=32, model_ema_decay=0.99998, use_deterministic_algorithms=False, interpolation='bilinear', val_resize_size=256, val_crop_size=224, train_crop_size=224, clip_grad_norm=None, ra_sampler=False, ra_reps=3, weights=None, save_checkpoint=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')\n",
      "[2024-07-17 05:49:58] /usr/local/lib/python3.10/dist-packages/torch/random.py:40\n",
      "    [context]:         torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.manual_seed_all(seed=123, ) --> torch.hpu.random.manual_seed_all(123)\n",
      "\u001b[0m\n",
      "Loading data\n",
      "Loading training data\n",
      "Took 109.72813725471497\n",
      "Loading validation data\n",
      "Creating data loaders\n",
      "HabanaDataLoader device type  4\n",
      "Warning: Updated shuffle to True as sampler is DistributedSampler with shuffle True\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x717dd1133c70>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform RandomResizedCrop: Random Crop,Resize w:h  224 224  scale:  (0.08, 1.0)  ratio:  (0.75, 1.3333333333333333)  interpolation:  InterpolationMode.BILINEAR\n",
      "transform RandomHorizontalFlip: probability  0.5\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 8 instance id 0\n",
      "MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name HPUMediaPipe:1\n",
      "MediaDataloader 1/8 seed : 1846299962\n",
      "MediaDataloader 5/8 seed : 1846375070\n",
      "MediaDataloader 6/8 seed : 1846417620\n",
      "MediaDataloader 4/8 seed : 1846511038\n",
      "MediaDataloader 2/8 seed : 1846688554\n",
      "MediaDataloader 7/8 seed : 1847083640\n",
      "MediaDataloader 3/8 seed : 1847248224\n",
      "MediaDataloader 0/8 seed : 1847337800\n",
      "Decode ResizedCrop w:h 224 224\n",
      "MediaDataloader shuffle is  True\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... Done!\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 492337 classes 386\n",
      "num_slices 8 slice_index 0\n",
      "random seed used  1847337800\n",
      "sliced media files/labels 61543\n",
      "Finding largest file ...\n",
      "MediaDataloader 2/8 seed : 1777164216\n",
      "largest file is  /root/data/huggingface/datasets/imagenet/ILSVRC2012/train/n01756291/n01756291_6037.JPEG\n",
      "Running with Habana media DataLoader with num_instances = 8, instance_id = 0.\n",
      "MediaDataloader 7/8 seed : 1856700492\n",
      "MediaDataloader 4/8 seed : 1884745836\n",
      "HabanaDataLoader device type  4\n",
      "Warning: sampler is not supported by MediaDataLoader, ignoring sampler:  <torch.utils.data.distributed.DistributedSampler object at 0x717dd1131ed0>\n",
      "Warning: num_workers is not supported by MediaDataLoader, ignoring num_workers:  8\n",
      "Warning: MediaDataLoader using drop_last: False, round up of last batch will be done\n",
      "Warning: MediaDataLoader using prefetch_factor 3\n",
      "transform Resize: w:h  256 256  interpolation:  InterpolationMode.BILINEAR  max_size:  None\n",
      "transform CenterCrop: w:h  224 224\n",
      "transform ToTensor\n",
      "transform Normalize: mean:std [0.485, 0.456, 0.406] [0.229, 0.224, 0.225]\n",
      "MediaDataloader num instances 8 instance id 0\n",
      "MediaPipe device gaudi2 device_type GAUDI2 device_id 0 pipe_name HPUMediaPipe:2\n",
      "MediaDataloader 0/8 seed : 1943996129\n",
      "Decode w:h  256 256  , Crop disabled\n",
      "MediaDataloader shuffle is  False\n",
      "MediaDataloader output type is  float32\n",
      "Finding classes ... Done!\n",
      "MediaDataloader 1/8 seed : 1961570092\n",
      "MediaDataloader 6/8 seed : 1966675621\n",
      "MediaDataloader 5/8 seed : 1986705345\n",
      "MediaDataloader 3/8 seed : 2023481935\n",
      "Done!\n",
      "Generating labels ... Done!\n",
      "Total media files/labels 50000 classes 1000\n",
      "num_slices 8 slice_index 0\n",
      "random seed used  1943996129\n",
      "sliced media files/labels 6250\n",
      "Finding largest file ...\n",
      "largest file is  /root/data/huggingface/datasets/imagenet/ILSVRC2012/val/n01630670/ILSVRC2012_val_00046430.JPEG\n",
      "Running with Habana media DataLoader with num_instances = 8, instance_id = 0.\n",
      "Creating model\n",
      "[2024-07-17 05:52:15] /usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/core/weight_sharing.py:179\n",
      "    [context]:     result = self.original_to(*args, **kwargs)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'), None, False), kwargs={}, ) --> torch.Tensor.to(args=('hpu', None, False), kwargs={})\n",
      "\u001b[0m\n",
      "Using HPU Graphs on Gaudi2 for reducing operator accumulation time.\n",
      "[2024-07-17 05:52:15] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:316\n",
      "    [context]:     scaler = torch.cuda.amp.GradScaler() if args.amp else None\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.amp.GradScaler.__init__(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True, ) --> set enabled to Flase\n",
      "\u001b[0m\n",
      "[2024-07-17 05:52:15] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:359\n",
      "    [context]:         model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu], broadcast_buffers=False, gradient_as_bucket_view=True, bucket_cap_mb=1024)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.nn.parallel.DistributedDataParallel.__init__(module=module, device_ids=[0], output_device=None, dim=0, broadcast_buffers=False, process_group=None, bucket_cap_mb=1024, find_unused_parameters=False, check_reduction=False, gradient_as_bucket_view=True, static_graph=False, delay_all_reduce_named_params=None, param_to_hook_all_reduce=None, mixed_precision=None, device_mesh=None, ) --> change device_ids and output_device to None\n",
      "\u001b[0m\n",
      "Start training\n",
      "[2024-07-17 05:52:16] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:113\n",
      "    [context]:         if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Shuffling ... Done!\n",
      "[2024-07-17 05:52:18] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:33\n",
      "    [context]:         image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2024-07-17 05:52:18] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:34\n",
      "    [context]:         with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.autocast.__init__(device_type=cuda, dtype=torch.float16, enabled=True, cache_enabled=True, ) --> torch.autocast.__init__(device_type=hpu, dtype=None, enabled=True, cache_enabled=True, )\n",
      "\u001b[0m\n",
      "[2024-07-17 05:52:18] /usr/local/lib/python3.10/dist-packages/torch/cuda/amp/common.py:9\n",
      "    [context]:     return not (torch.cuda.is_available() or find_spec(\"torch_xla\"))\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2024-07-17 05:52:27] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:137\n",
      "    [context]:                 if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2024-07-17 05:52:27] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:146\n",
      "    [context]:                             memory=torch.cuda.max_memory_allocated() / MB,\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.max_memory_allocated(device=None, ) --> torch.hpu.max_memory_allocated(device=None)\n",
      "\u001b[0m\n",
      "Epoch: [0]  [  0/241]  eta: 0:43:36  lr: 0.1  img/s: 23.58343985588415  loss: 6.1690 (6.1690)  acc1: 0.0000 (0.0000)  acc5: 0.3906 (0.3906)  time: 10.8551  data: 1.6163  max mem: 18636\n",
      "Epoch: [0]  [ 10/241]  eta: 0:04:37  lr: 0.1  img/s: 1074.0387957270186  loss: 6.1690 (6.2516)  acc1: 0.0000 (0.0000)  acc5: 0.3906 (0.5859)  time: 1.2031  data: 0.2747  max mem: 40182\n",
      "Epoch: [0]  [ 20/241]  eta: 0:02:52  lr: 0.1  img/s: 812.4342804705656  loss: 6.3218 (6.2750)  acc1: 0.0000 (0.0000)  acc5: 0.7812 (0.9115)  time: 0.2756  data: 0.2122  max mem: 40182\n",
      "Epoch: [0]  [ 30/241]  eta: 0:02:12  lr: 0.1  img/s: 810.3124784921243  loss: 6.1947 (6.2549)  acc1: 0.0000 (0.0000)  acc5: 0.7812 (0.8789)  time: 0.3136  data: 0.2854  max mem: 40182\n",
      "Epoch: [0]  [ 40/241]  eta: 0:01:51  lr: 0.1  img/s: 803.2592217105097  loss: 6.1947 (6.2071)  acc1: 0.0000 (0.2344)  acc5: 0.7812 (1.2500)  time: 0.3154  data: 0.2876  max mem: 40182\n",
      "Epoch: [0]  [ 50/241]  eta: 0:01:36  lr: 0.1  img/s: 803.1895819365458  loss: 6.1690 (6.1715)  acc1: 0.0000 (0.1953)  acc5: 0.7812 (1.4323)  time: 0.3168  data: 0.2891  max mem: 40182\n",
      "Epoch: [0]  [ 60/241]  eta: 0:01:25  lr: 0.1  img/s: 815.9025143514451  loss: 6.1690 (6.1477)  acc1: 0.0000 (0.1674)  acc5: 0.7812 (1.2277)  time: 0.3144  data: 0.2871  max mem: 40182\n",
      "Epoch: [0]  [ 70/241]  eta: 0:01:17  lr: 0.1  img/s: 805.1089955297459  loss: 6.1690 (6.1535)  acc1: 0.0000 (0.1465)  acc5: 0.7812 (1.2207)  time: 0.3139  data: 0.2859  max mem: 40182\n",
      "Epoch: [0]  [ 80/241]  eta: 0:01:10  lr: 0.1  img/s: 797.836128598417  loss: 6.1690 (6.1305)  acc1: 0.0000 (0.1302)  acc5: 1.1719 (1.2153)  time: 0.3174  data: 0.2854  max mem: 40182\n",
      "Epoch: [0]  [ 90/241]  eta: 0:01:03  lr: 0.1  img/s: 813.8676863530537  loss: 6.0160 (6.1124)  acc1: 0.0000 (0.1562)  acc5: 1.1719 (1.2109)  time: 0.3157  data: 0.2818  max mem: 40182\n",
      "Epoch: [0]  [100/241]  eta: 0:00:58  lr: 0.1  img/s: 793.9821382051881  loss: 6.0160 (6.0979)  acc1: 0.0000 (0.1420)  acc5: 1.1719 (1.3494)  time: 0.3165  data: 0.2839  max mem: 40182\n",
      "Epoch: [0]  [110/241]  eta: 0:00:52  lr: 0.1  img/s: 795.1700569742295  loss: 6.0049 (6.0847)  acc1: 0.0000 (0.1302)  acc5: 1.1719 (1.2695)  time: 0.3202  data: 0.2855  max mem: 40182\n",
      "Epoch: [0]  [120/241]  eta: 0:00:47  lr: 0.1  img/s: 811.8878557135564  loss: 6.0049 (6.0749)  acc1: 0.0000 (0.1502)  acc5: 1.1719 (1.3221)  time: 0.3166  data: 0.2817  max mem: 40182\n",
      "Epoch: [0]  [130/241]  eta: 0:00:43  lr: 0.1  img/s: 807.2556077532614  loss: 5.9936 (6.0627)  acc1: 0.0000 (0.2232)  acc5: 1.1719 (1.3951)  time: 0.3141  data: 0.2787  max mem: 40182\n",
      "Epoch: [0]  [140/241]  eta: 0:00:38  lr: 0.1  img/s: 808.027906994245  loss: 5.9936 (6.0508)  acc1: 0.0000 (0.2604)  acc5: 1.1719 (1.5365)  time: 0.3149  data: 0.2792  max mem: 40182\n",
      "Epoch: [0]  [150/241]  eta: 0:00:34  lr: 0.1  img/s: 822.4350374776639  loss: 5.9575 (6.0405)  acc1: 0.0000 (0.2686)  acc5: 1.1719 (1.7090)  time: 0.3119  data: 0.2794  max mem: 40182\n",
      "Epoch: [0]  [160/241]  eta: 0:00:30  lr: 0.1  img/s: 829.2483859363346  loss: 5.9575 (6.0278)  acc1: 0.0000 (0.2987)  acc5: 1.5625 (1.7923)  time: 0.3079  data: 0.2766  max mem: 40182\n",
      "Epoch: [0]  [170/241]  eta: 0:00:26  lr: 0.1  img/s: 819.1593770145371  loss: 5.9533 (6.0141)  acc1: 0.0000 (0.3255)  acc5: 1.5625 (2.0399)  time: 0.3086  data: 0.2780  max mem: 40182\n",
      "Epoch: [0]  [180/241]  eta: 0:00:22  lr: 0.1  img/s: 820.7957272890304  loss: 5.9533 (6.0022)  acc1: 0.0000 (0.3906)  acc5: 1.9531 (2.2204)  time: 0.3102  data: 0.2790  max mem: 40182\n",
      "Epoch: [0]  [190/241]  eta: 0:00:18  lr: 0.1  img/s: 807.9743396683017  loss: 5.9494 (5.9855)  acc1: 0.0000 (0.4688)  acc5: 1.9531 (2.4805)  time: 0.3124  data: 0.2768  max mem: 40182\n",
      "Epoch: [0]  [200/241]  eta: 0:00:14  lr: 0.1  img/s: 818.3736469103349  loss: 5.9467 (5.9687)  acc1: 0.3906 (0.4650)  acc5: 2.3438 (2.5670)  time: 0.3129  data: 0.2743  max mem: 40182\n",
      "Epoch: [0]  [210/241]  eta: 0:00:11  lr: 0.1  img/s: 822.0892795372898  loss: 5.9395 (5.9598)  acc1: 0.3906 (0.4972)  acc5: 2.3438 (2.6278)  time: 0.3102  data: 0.2707  max mem: 40182\n",
      "Epoch: [0]  [220/241]  eta: 0:00:07  lr: 0.1  img/s: 815.1159625630614  loss: 5.9032 (5.9474)  acc1: 0.3906 (0.5095)  acc5: 2.7344 (2.7514)  time: 0.3107  data: 0.2706  max mem: 40182\n",
      "Epoch: [0]  [230/241]  eta: 0:00:03  lr: 0.1  img/s: 823.473048087511  loss: 5.8855 (5.9349)  acc1: 0.3906 (0.5371)  acc5: 2.7344 (2.8320)  time: 0.3104  data: 0.2710  max mem: 40182\n",
      "Epoch: [0]  [240/241]  eta: 0:00:00  lr: 0.1  img/s: 863.5250313183512  loss: 5.8846 (5.9192)  acc1: 0.3906 (0.6250)  acc5: 3.1250 (3.0625)  time: 0.3016  data: 0.2661  max mem: 40182\n",
      "Epoch: [0] Total time: 0:01:25\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "[2024-07-17 05:53:42] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:83\n",
      "    [context]:             image = image.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2024-07-17 05:53:42] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/train.py:84\n",
      "    [context]:             target = target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "Test:   [ 0/25]  eta: 0:02:26  loss: 6.2604 (6.2604)  acc1: 0.0000 (0.0000)  acc5: 0.3906 (0.3906)  time: 5.8488  data: 0.5608  max mem: 40182\n",
      "Test:  Total time: 0:00:12\n",
      "[2024-07-17 05:53:55] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=(6400,), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=(6400,), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2024-07-17 05:53:55] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([25, 60.69567537307739],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([25, 60.69567537307739],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2024-07-17 05:53:55] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 1000.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 1000.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2024-07-17 05:53:55] /root/Gaudi2-Workshop/Model-Migration/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 3900.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 3900.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "Test:  Acc@1 0.125 Acc@5 0.666\n",
      "Training time 0:01:38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-09 23:17:26] train.py:32\n",
      "    [context]:         image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:26] train.py:33\n",
      "    [context]:         with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.autocast.__init__(device_type=cuda, dtype=torch.float16, enabled=True, cache_enabled=True, ) --> torch.autocast.__init__(device_type=hpu, dtype=None, enabled=True, cache_enabled=True, )\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:26] /usr/local/lib/python3.8/dist-packages/torch/cuda/amp/common.py:7\n",
      "    [context]:     return not (torch.cuda.is_available() or find_spec('torch_xla'))\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:37] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:137\n",
      "    [context]:                 if torch.cuda.is_available():\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.is_available() --> torch.hpu.is_available()\n",
      "\u001b[0m\n",
      "[2023-06-09 23:17:37] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:146\n",
      "    [context]:                             memory=torch.cuda.max_memory_allocated() / MB,\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.max_memory_allocated(device=None, ) --> torch.hpu.max_memory_allocated(device=None)\n",
      "\u001b[0m\n",
      "Epoch: [0]  [  0/622]  eta: 2:06:32  lr: 0.1  img/s: 20.973556319305924  loss: 7.1048 (7.1048)  acc1: 0.3906 (0.3906)  acc5: 0.3906 (0.3906)  time: 12.2059  data: 1.2269  max mem: 15838\n",
      "Epoch: [0]  [ 10/622]  eta: 0:31:50  lr: 0.1  img/s: 115.61183054130028  loss: 7.1048 (7.1195)  acc1: 0.3906 (0.3906)  acc5: 0.3906 (0.7812)  time: 3.1221  data: 0.1968  max mem: 16966\n",
      "Epoch: [0]  [ 20/622]  eta: 0:16:40  lr: 0.1  img/s: 4432.052451134573  loss: 7.1133 (7.1174)  acc1: 0.3906 (0.3906)  acc5: 1.1719 (0.9115)  time: 1.1344  data: 0.0488  max mem: 16966\n",
      "Epoch: [0]  [ 30/622]  eta: 0:11:16  lr: 0.1  img/s: 4516.90829523116  loss: 7.1133 (7.1285)  acc1: 0.3906 (0.4883)  acc5: 0.7812 (0.8789)  time: 0.0546  data: 0.0051  max mem: 16966\n",
      "Epoch: [0]  [ 40/622]  eta: 0:08:30  lr: 0.1  img/s: 4853.195422435741  loss: 7.1133 (7.0867)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.8594)  time: 0.0521  data: 0.0047  max mem: 16966\n",
      "Epoch: [0]  [ 50/622]  eta: 0:06:48  lr: 0.1  img/s: 5000.998223147521  loss: 7.1048 (7.0657)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.8464)  time: 0.0493  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [ 60/622]  eta: 0:05:39  lr: 0.1  img/s: 5282.966572527873  loss: 7.1048 (7.0459)  acc1: 0.3906 (0.3348)  acc5: 0.7812 (0.7812)  time: 0.0472  data: 0.0015  max mem: 16966\n",
      "Epoch: [0]  [ 70/622]  eta: 0:04:50  lr: 0.1  img/s: 4933.84042494532  loss: 6.9607 (7.0270)  acc1: 0.3906 (0.2930)  acc5: 0.7812 (0.7812)  time: 0.0475  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [ 80/622]  eta: 0:04:13  lr: 0.1  img/s: 4933.219318243425  loss: 6.9607 (7.0097)  acc1: 0.3906 (0.3038)  acc5: 0.7812 (0.7812)  time: 0.0492  data: 0.0014  max mem: 16966\n",
      "Epoch: [0]  [ 90/622]  eta: 0:03:44  lr: 0.1  img/s: 5296.3007443733  loss: 6.9274 (6.9888)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.9766)  time: 0.0475  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [100/622]  eta: 0:03:20  lr: 0.1  img/s: 5273.366650950809  loss: 6.9274 (6.9741)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.9233)  time: 0.0458  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [110/622]  eta: 0:03:01  lr: 0.1  img/s: 5202.998810872521  loss: 6.9192 (6.9543)  acc1: 0.3906 (0.3581)  acc5: 0.7812 (1.0417)  time: 0.0462  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [120/622]  eta: 0:02:44  lr: 0.1  img/s: 5171.708403494103  loss: 6.9192 (6.9406)  acc1: 0.3906 (0.3606)  acc5: 0.7812 (1.1118)  time: 0.0467  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [130/622]  eta: 0:02:30  lr: 0.1  img/s: 5266.486614806604  loss: 6.8949 (6.9260)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (1.3393)  time: 0.0464  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [140/622]  eta: 0:02:20  lr: 0.1  img/s: 2805.1565217414022  loss: 6.8949 (6.9064)  acc1: 0.3906 (0.4167)  acc5: 0.7812 (1.4583)  time: 0.0673  data: 0.0086  max mem: 16966\n",
      "Epoch: [0]  [150/622]  eta: 0:02:09  lr: 0.1  img/s: 5234.665052013101  loss: 6.8713 (6.8888)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (1.4893)  time: 0.0675  data: 0.0095  max mem: 16966\n",
      "Epoch: [0]  [160/622]  eta: 0:02:00  lr: 0.1  img/s: 5222.205370535011  loss: 6.8713 (6.8671)  acc1: 0.3906 (0.4136)  acc5: 1.1719 (1.5165)  time: 0.0463  data: 0.0023  max mem: 16966\n",
      "Epoch: [0]  [170/622]  eta: 0:01:52  lr: 0.1  img/s: 5223.231671485778  loss: 6.8270 (6.8482)  acc1: 0.3906 (0.4774)  acc5: 1.1719 (1.6059)  time: 0.0463  data: 0.0014  max mem: 16966\n",
      "Epoch: [0]  [180/622]  eta: 0:01:44  lr: 0.1  img/s: 4716.0108590957225  loss: 6.8270 (6.8283)  acc1: 0.3906 (0.4729)  acc5: 1.1719 (1.6653)  time: 0.0489  data: 0.0041  max mem: 16966\n",
      "Epoch: [0]  [190/622]  eta: 0:01:38  lr: 0.1  img/s: 5201.287478013948  loss: 6.8005 (6.8103)  acc1: 0.3906 (0.5078)  acc5: 1.1719 (1.8164)  time: 0.0490  data: 0.0040  max mem: 16966\n",
      "Epoch: [0]  [200/622]  eta: 0:01:32  lr: 0.1  img/s: 5223.7449732474115  loss: 6.7767 (6.7917)  acc1: 0.3906 (0.5580)  acc5: 1.9531 (1.9903)  time: 0.0465  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [210/622]  eta: 0:01:26  lr: 0.1  img/s: 5242.224721909762  loss: 6.7362 (6.7727)  acc1: 0.3906 (0.6037)  acc5: 1.9531 (2.1662)  time: 0.0463  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [220/622]  eta: 0:01:21  lr: 0.1  img/s: 5272.3490329002  loss: 6.7361 (6.7572)  acc1: 0.3906 (0.6624)  acc5: 1.9531 (2.2418)  time: 0.0460  data: 0.0010  max mem: 16966\n",
      "Epoch: [0]  [230/622]  eta: 0:01:16  lr: 0.1  img/s: 5257.62599791603  loss: 6.6319 (6.7398)  acc1: 0.3906 (0.6673)  acc5: 2.3438 (2.2786)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [240/622]  eta: 0:01:12  lr: 0.1  img/s: 5265.554278899224  loss: 6.6249 (6.7193)  acc1: 0.7812 (0.7188)  acc5: 2.7344 (2.3438)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [250/622]  eta: 0:01:08  lr: 0.1  img/s: 5245.37462640924  loss: 6.5275 (6.6944)  acc1: 0.7812 (0.7662)  acc5: 2.7344 (2.5090)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [260/622]  eta: 0:01:04  lr: 0.1  img/s: 5223.676357764838  loss: 6.5193 (6.6699)  acc1: 0.7812 (0.7668)  acc5: 3.1250 (2.5608)  time: 0.0463  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [270/622]  eta: 0:01:01  lr: 0.1  img/s: 4976.946303265318  loss: 6.4692 (6.6476)  acc1: 0.7812 (0.7673)  acc5: 3.1250 (2.6925)  time: 0.0475  data: 0.0031  max mem: 16966\n",
      "Epoch: [0]  [280/622]  eta: 0:00:58  lr: 0.1  img/s: 4616.917909177062  loss: 6.4686 (6.6303)  acc1: 0.7812 (0.8082)  acc5: 3.1250 (2.8691)  time: 0.0506  data: 0.0069  max mem: 16966\n",
      "Epoch: [0]  [290/622]  eta: 0:00:54  lr: 0.1  img/s: 4638.213659670236  loss: 6.4193 (6.6123)  acc1: 0.7812 (0.8464)  acc5: 3.9062 (3.0990)  time: 0.0526  data: 0.0075  max mem: 16966\n",
      "Epoch: [0]  [300/622]  eta: 0:00:52  lr: 0.1  img/s: 4683.1702654478195  loss: 6.4160 (6.5903)  acc1: 0.7812 (0.9451)  acc5: 3.9062 (3.2006)  time: 0.0523  data: 0.0051  max mem: 16966\n",
      "Epoch: [0]  [310/622]  eta: 0:00:49  lr: 0.1  img/s: 5126.348193291654  loss: 6.3754 (6.5707)  acc1: 1.1719 (1.0010)  acc5: 3.9062 (3.4302)  time: 0.0497  data: 0.0023  max mem: 16966\n",
      "Epoch: [0]  [320/622]  eta: 0:00:46  lr: 0.1  img/s: 5302.313549730277  loss: 6.3376 (6.5510)  acc1: 1.5625 (1.0535)  acc5: 4.2969 (3.5748)  time: 0.0465  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [330/622]  eta: 0:00:44  lr: 0.1  img/s: 5269.187344039455  loss: 6.2278 (6.5302)  acc1: 1.5625 (1.1029)  acc5: 4.6875 (3.8143)  time: 0.0458  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [340/622]  eta: 0:00:41  lr: 0.1  img/s: 5273.364061093401  loss: 6.1463 (6.5111)  acc1: 1.5625 (1.1719)  acc5: 5.4688 (4.0290)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [350/622]  eta: 0:00:39  lr: 0.1  img/s: 5061.5752637813775  loss: 6.0904 (6.4893)  acc1: 1.5625 (1.1719)  acc5: 5.8594 (4.1775)  time: 0.0470  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [360/622]  eta: 0:00:37  lr: 0.1  img/s: 4311.871557253055  loss: 6.0738 (6.4701)  acc1: 1.9531 (1.2141)  acc5: 6.2500 (4.3919)  time: 0.0524  data: 0.0061  max mem: 16966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [370/622]  eta: 0:00:35  lr: 0.1  img/s: 4708.559641186386  loss: 6.0441 (6.4522)  acc1: 1.9531 (1.2747)  acc5: 6.2500 (4.5539)  time: 0.0542  data: 0.0080  max mem: 16966\n",
      "Epoch: [0]  [380/622]  eta: 0:00:33  lr: 0.1  img/s: 3969.727520264712  loss: 6.0329 (6.4277)  acc1: 1.9531 (1.3822)  acc5: 6.6406 (4.7676)  time: 0.0568  data: 0.0114  max mem: 16966\n",
      "Epoch: [0]  [390/622]  eta: 0:00:31  lr: 0.1  img/s: 5295.522353315996  loss: 5.9631 (6.4052)  acc1: 1.9531 (1.4258)  acc5: 7.8125 (5.0781)  time: 0.0538  data: 0.0095  max mem: 16966\n",
      "Epoch: [0]  [400/622]  eta: 0:00:29  lr: 0.1  img/s: 3096.166358897938  loss: 5.9305 (6.3836)  acc1: 1.9531 (1.4863)  acc5: 8.2031 (5.3163)  time: 0.0628  data: 0.0150  max mem: 16966\n",
      "Epoch: [0]  [410/622]  eta: 0:00:28  lr: 0.1  img/s: 2408.498940252397  loss: 5.9192 (6.3629)  acc1: 2.7344 (1.5532)  acc5: 9.3750 (5.4874)  time: 0.0918  data: 0.0395  max mem: 16966\n",
      "Epoch: [0]  [420/622]  eta: 0:00:26  lr: 0.1  img/s: 3030.912933452077  loss: 5.8614 (6.3425)  acc1: 2.7344 (1.6443)  acc5: 9.7656 (5.6777)  time: 0.0927  data: 0.0413  max mem: 16966\n",
      "Epoch: [0]  [430/622]  eta: 0:00:25  lr: 0.1  img/s: 2110.917490856389  loss: 5.8441 (6.3240)  acc1: 2.7344 (1.6779)  acc5: 10.5469 (5.8683)  time: 0.1002  data: 0.0546  max mem: 16966\n",
      "Epoch: [0]  [440/622]  eta: 0:00:23  lr: 0.1  img/s: 2323.6120469949924  loss: 5.7918 (6.3044)  acc1: 2.7344 (1.7535)  acc5: 10.5469 (6.1024)  time: 0.1130  data: 0.0703  max mem: 16966\n",
      "Epoch: [0]  [450/622]  eta: 0:00:22  lr: 0.1  img/s: 2431.6198912664154  loss: 5.7758 (6.2853)  acc1: 3.1250 (1.8512)  acc5: 11.3281 (6.2500)  time: 0.1050  data: 0.0635  max mem: 16966\n",
      "Epoch: [0]  [460/622]  eta: 0:00:21  lr: 0.1  img/s: 3050.051113422713  loss: 5.7286 (6.2633)  acc1: 3.1250 (1.9448)  acc5: 11.7188 (6.4910)  time: 0.0919  data: 0.0475  max mem: 16966\n",
      "Epoch: [0]  [470/622]  eta: 0:00:19  lr: 0.1  img/s: 2714.830569262903  loss: 5.5314 (6.2427)  acc1: 3.5156 (2.0589)  acc5: 12.1094 (6.7139)  time: 0.0864  data: 0.0403  max mem: 16966\n",
      "Epoch: [0]  [480/622]  eta: 0:00:18  lr: 0.1  img/s: 2607.0844864483993  loss: 5.5276 (6.2233)  acc1: 3.5156 (2.1923)  acc5: 12.5000 (6.9276)  time: 0.0935  data: 0.0487  max mem: 16966\n",
      "Epoch: [0]  [490/622]  eta: 0:00:16  lr: 0.1  img/s: 2673.644608111059  loss: 5.5198 (6.2070)  acc1: 3.9062 (2.2656)  acc5: 12.8906 (7.1484)  time: 0.0943  data: 0.0500  max mem: 16966\n",
      "Epoch: [0]  [500/622]  eta: 0:00:15  lr: 0.1  img/s: 3352.976666539677  loss: 5.5159 (6.1900)  acc1: 3.9062 (2.3667)  acc5: 12.8906 (7.3529)  time: 0.0833  data: 0.0405  max mem: 16966\n",
      "Epoch: [0]  [510/622]  eta: 0:00:14  lr: 0.1  img/s: 2582.536135938522  loss: 5.4965 (6.1737)  acc1: 4.2969 (2.4114)  acc5: 13.6719 (7.5646)  time: 0.0850  data: 0.0419  max mem: 16966\n",
      "Epoch: [0]  [520/622]  eta: 0:00:12  lr: 0.1  img/s: 2566.7180229319924  loss: 5.4821 (6.1604)  acc1: 4.6875 (2.4690)  acc5: 14.0625 (7.6946)  time: 0.0967  data: 0.0539  max mem: 16966\n",
      "Epoch: [0]  [530/622]  eta: 0:00:11  lr: 0.1  img/s: 3633.3280794191505  loss: 5.4670 (6.1458)  acc1: 5.0781 (2.5174)  acc5: 14.4531 (7.8559)  time: 0.0824  data: 0.0380  max mem: 16966\n",
      "Epoch: [0]  [540/622]  eta: 0:00:10  lr: 0.1  img/s: 5268.827948578792  loss: 5.4391 (6.1269)  acc1: 5.0781 (2.5994)  acc5: 14.8438 (8.1108)  time: 0.0568  data: 0.0120  max mem: 16966\n",
      "Epoch: [0]  [550/622]  eta: 0:00:08  lr: 0.1  img/s: 5298.454265664423  loss: 5.4279 (6.1127)  acc1: 5.4688 (2.6507)  acc5: 16.4062 (8.2729)  time: 0.0458  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [560/622]  eta: 0:00:07  lr: 0.1  img/s: 5227.8957480268955  loss: 5.4072 (6.0961)  acc1: 5.4688 (2.7618)  acc5: 16.4062 (8.5115)  time: 0.0460  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [570/622]  eta: 0:00:06  lr: 0.1  img/s: 5281.303546308593  loss: 5.3751 (6.0820)  acc1: 5.4688 (2.8354)  acc5: 17.1875 (8.7150)  time: 0.0461  data: 0.0009  max mem: 16966\n",
      "Epoch: [0]  [580/622]  eta: 0:00:04  lr: 0.1  img/s: 5121.330912278744  loss: 5.3453 (6.0645)  acc1: 5.4688 (2.9396)  acc5: 17.1875 (8.9645)  time: 0.0466  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [590/622]  eta: 0:00:03  lr: 0.1  img/s: 5291.805950031172  loss: 5.3375 (6.0483)  acc1: 5.8594 (3.0859)  acc5: 17.1875 (9.1862)  time: 0.0465  data: 0.0013  max mem: 16966\n",
      "Epoch: [0]  [600/622]  eta: 0:00:02  lr: 0.1  img/s: 5286.735640525827  loss: 5.3339 (6.0343)  acc1: 6.2500 (3.1762)  acc5: 17.1875 (9.3814)  time: 0.0458  data: 0.0015  max mem: 16966\n",
      "Epoch: [0]  [610/622]  eta: 0:00:01  lr: 0.1  img/s: 5235.686044663241  loss: 5.2932 (6.0187)  acc1: 6.2500 (3.2258)  acc5: 17.1875 (9.5010)  time: 0.0460  data: 0.0020  max mem: 16966\n",
      "Epoch: [0]  [620/622]  eta: 0:00:00  lr: 0.1  img/s: 5294.193348618199  loss: 5.2774 (6.0045)  acc1: 6.2500 (3.2738)  acc5: 17.5781 (9.6726)  time: 0.0460  data: 0.0017  max mem: 16966\n",
      "Epoch: [0] Total time: 0:01:11\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "Warning: Decoder updated User configured Interpolation from Bilinear to Bicubic\n",
      "[2023-06-09 23:18:37] train.py:82\n",
      "    [context]:             image = image.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:37] train.py:83\n",
      "    [context]:             target = target.to(device, non_blocking=True)\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.Tensor.to(args=(device(type='cuda'),), kwargs={'non_blocking': True}, ) --> torch.Tensor.to(args=('hpu',), kwargs={non_blocking=True, })\n",
      "\u001b[0m\n",
      "Test:   [ 0/25]  eta: 0:02:25  loss: 7.6194 (7.6194)  acc1: 0.3906 (0.3906)  acc5: 0.7812 (0.7812)  time: 5.8004  data: 0.0764  max mem: 16966\n",
      "Test:  Total time: 0:00:07\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=(6400,), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=(6400,), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([25, 195.1865692138672],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([25, 195.1865692138672],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 2200.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 2200.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "[2023-06-09 23:18:44] /root/tf/Model-References/PyTorch/examples/gpu_migration/computer_vision/classification/torchvision/utils.py:404\n",
      "    [context]:     t = torch.tensor(val, device=\"cuda\")\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.tensor(args=([6400, 10400.0],), kwargs={'device': 'hpu'}, ) --> torch.tensor(args=([6400, 10400.0],), kwargs={device=hpu, })\n",
      "\u001b[0m\n",
      "Test:  Acc@1 0.330 Acc@5 1.725\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/gpu_migration/torch/cuda/amp/grad_scaler.py:65: UserWarning: GradScaler is not applicable to HPU. If this instance if disabled, the states of the scaler are values in disable mode.\n",
      "  warnings.warn(\n",
      "[2023-06-09 23:18:44] train.py:415\n",
      "    [context]:                 checkpoint[\"scaler\"] = scaler.state_dict()\n",
      "\n",
      "\u001b[92m    [hpu_match]: torch.cuda.amp.GradScaler.state_dict() --> return state in diable mode\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 0:01:23\r\n"
     ]
    }
   ],
   "source": [
    "!GPU_MIGRATION_LOG_LEVEL=1 torchrun --nproc_per_node 8 train.py --batch-size=256 --model=resnet50 --device=cuda \\\n",
    "--data-path=/root/data/huggingface/datasets/imagenet/ILSVRC2012 \\\n",
    "--workers=8 --epochs=1 --opt=sgd --amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b97bbd-285d-4778-a725-8e9be1b3e56f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

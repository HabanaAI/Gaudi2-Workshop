{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "# Model Profiling and optimization using Swin Transformer from HuggingFace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Setup\n",
    "We start with a Habana PyTorch Docker image and run this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the Optimum Habana Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum[habana]\n",
      "  Downloading optimum-1.8.8.tar.gz (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.1/244.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (0.15.1a0+42759b1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.12)\n",
      "Collecting huggingface-hub>=0.8.0\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (23.1)\n",
      "Collecting transformers[sentencepiece]>=4.26.0\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (1.23.5)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m277.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from optimum[habana]) (2.0.1a0+git37b7ddc)\n",
      "Collecting optimum-habana\n",
      "  Downloading optimum_habana-1.6.0-py3-none-any.whl (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.2/149.2 kB\u001b[0m \u001b[31m384.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting transformers<4.29.0\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.6.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2.31.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[habana]) (2023.5.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.9->optimum[habana]) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.29.0->optimum[habana]) (2020.10.28)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]>=4.26.0\n",
      "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.29.0-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers<4.29.0->optimum[habana]) (3.19.5)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m343.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (3.8.4)\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m395.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum[habana]) (1.4.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m388.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m434.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m428.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diffusers>=0.12.0\n",
      "  Downloading diffusers-0.17.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum[habana]) (1.3.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from diffusers>=0.12.0->optimum-habana->optimum[habana]) (6.6.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (3.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum[habana]) (1.9.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (1.26.16)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[habana]) (3.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->optimum-habana->optimum[habana]) (5.9.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.9->optimum[habana]) (2.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum[habana]) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum[habana]) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->diffusers>=0.12.0->optimum-habana->optimum[habana]) (3.15.0)\n",
      "Building wheels for collected packages: optimum\n",
      "  Building wheel for optimum (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for optimum: filename=optimum-1.8.8-py3-none-any.whl size=318455 sha256=2ea2857c786c4385a32099fe4791be36627b7f3eb874093017908ec09333b46c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wjyzd51r/wheels/a0/93/00/e6f8f49a40e3af11f7d883b36482af20073db0520b340e0f24\n",
      "Successfully built optimum\n",
      "Installing collected packages: tokenizers, sentencepiece, xxhash, pyarrow, pillow, humanfriendly, dill, multiprocess, huggingface-hub, coloredlogs, transformers, diffusers, accelerate, datasets, optimum, optimum-habana\n",
      "Successfully installed accelerate-0.20.3 coloredlogs-15.0.1 datasets-2.13.1 diffusers-0.17.1 dill-0.3.6 huggingface-hub-0.15.1 humanfriendly-10.0 multiprocess-0.70.14 optimum-1.8.8 optimum-habana-1.6.0 pillow-9.5.0 pyarrow-12.0.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install optimum[habana]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone the HuggingFace Model Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'optimum-habana'...\n",
      "remote: Enumerating objects: 4208, done.\u001b[K\n",
      "remote: Counting objects: 100% (1319/1319), done.\u001b[K\n",
      "remote: Compressing objects: 100% (517/517), done.\u001b[K\n",
      "remote: Total 4208 (delta 895), reused 1061 (delta 760), pack-reused 2889\u001b[K\n",
      "Receiving objects: 100% (4208/4208), 2.24 MiB | 5.86 MiB/s, done.\n",
      "Resolving deltas: 100% (2677/2677), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/optimum-habana"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go the image-classification example model and install the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/workshop/Gaudi2-Workshop/Model-Optimization/optimum-habana/examples/image-classification\n"
     ]
    }
   ],
   "source": [
    "%cd optimum-habana/examples/image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (2.0.1a0+git37b7ddc)\n",
      "Requirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (0.15.1a0+42759b1)\n",
      "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (2.13.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (4.6.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.12.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.5.0->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 2)) (9.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (0.15.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (2023.5.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (12.0.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (3.8.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (5.4.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.4.0->-r requirements.txt (line 3)) (0.3.6)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m149.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.4.0->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.6.0->-r requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.5.0->-r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=2.4.0->-r requirements.txt (line 3)) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.5.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.4.0->-r requirements.txt (line 3)) (1.16.0)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, responses, evaluate\n",
      "Successfully installed evaluate-0.4.0 joblib-1.2.0 responses-0.18.0 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look in utils.py to see torch profiler code\n",
    "For other models not in optimum-habana, users can refer to [Profiling_with_PyTorch](https://docs.habana.ai/en/latest/Profiling/Profiling_with_PyTorch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   245\t            schedule = torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=1)\n",
      "   246\t            activities = [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.HPU]\n",
      "   247\t\n",
      "   248\t            profiler = torch.profiler.profile(\n",
      "   249\t                schedule=schedule,\n",
      "   250\t                activities=activities,\n",
      "   251\t                on_trace_ready=torch.profiler.tensorboard_trace_handler(output_dir),\n",
      "   252\t                record_shapes=True,\n",
      "   253\t                with_stack=True,\n",
      "   254\t            )\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "cat -n ../../optimum/habana/utils.py | head -n 254 | tail -n 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Model to collect trace file (unoptimized)\n",
    "Swin Transformer is a model that capably serves as a general-purpose backbone for computer vision. `run_image_classification.py` is a script that showcases how to fine-tune Swin Transformer on HPUs.\n",
    "\n",
    "Notice the Habana specific commands:\n",
    "\n",
    "`--use_habana` - allows training to run on Habana Gaudi  \n",
    "`--use_hpu_graphs` - reduces recompilation by replaying the graph  \n",
    "`--gaudi_config_name Habana/swin` - mapping to HuggingFace Swin Model config \n",
    "\n",
    "Notice the torch profiler specific commands:\n",
    "\n",
    "`--profiling_warmup_steps 10` - profiler will wait for warmup steps  \n",
    "`--profiling_steps 3` - records for the next active steps \n",
    "\n",
    "The collected trace files will be saved to `./hpu_profile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/training_args.py:252: FutureWarning: `--use_hpu_graphs` is deprecated and will be removed in a future version of 🤗 Optimum Habana. Use `--use_hpu_graphs_for_inference` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2023 03:10:28 - WARNING - __main__ - Process rank: -1, device: hpu, distributed training: False, mixed-precision training: True\n",
      "06/27/2023 03:10:28 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/swin,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/outputs/runs/Jun27_03-10-27_sc09wynn01-hls2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/tmp/outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_steps=3,\n",
      "profiling_warmup_steps=10,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/outputs/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "skip_memory_metrics=True,\n",
      "throughput_warmup_steps=2,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=True,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)in/gaudi_config.json: 100%|██████████| 450/450 [00:00<00:00, 114kB/s]\n",
      "Downloading builder script: 100%|██████████| 3.61k/3.61k [00:00<00:00, 37.1MB/s]\n",
      "Downloading metadata: 100%|██████████| 1.66k/1.66k [00:00<00:00, 21.4MB/s]\n",
      "Downloading readme: 100%|██████████| 5.00k/5.00k [00:00<00:00, 47.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cifar10/plain_text to /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 170M/170M [00:05<00:00, 31.0MB/s] \n",
      "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]/usr/local/lib/python3.8/dist-packages/datasets/features/image.py:325: UserWarning: Downcasting array dtype uint8 to uint8 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cifar10 downloaded and prepared to /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating train split:   0%|          | 1/50000 [00:00<3:18:50,  4.19 examples/s]\r\n",
      "Generating train split:   1%|          | 446/50000 [00:00<00:29, 1667.00 examples/s]\r\n",
      "Generating train split:   2%|▏         | 895/50000 [00:00<00:18, 2647.98 examples/s]\r\n",
      "Generating train split:   3%|▎         | 1309/50000 [00:00<00:15, 3140.83 examples/s]\r\n",
      "Generating train split:   4%|▎         | 1767/50000 [00:00<00:13, 3601.21 examples/s]\r\n",
      "Generating train split:   4%|▍         | 2196/50000 [00:00<00:12, 3817.27 examples/s]\r\n",
      "Generating train split:   5%|▌         | 2656/50000 [00:00<00:11, 4058.34 examples/s]\r\n",
      "Generating train split:   6%|▌         | 3100/50000 [00:00<00:11, 4173.32 examples/s]\r\n",
      "Generating train split:   7%|▋         | 3559/50000 [00:01<00:10, 4294.56 examples/s]\r\n",
      "Generating train split:   8%|▊         | 4002/50000 [00:01<00:10, 4333.26 examples/s]\r\n",
      "Generating train split:   9%|▉         | 4462/50000 [00:01<00:10, 4411.66 examples/s]\r\n",
      "Generating train split:  10%|▉         | 4922/50000 [00:01<00:10, 4466.12 examples/s]\r\n",
      "Generating train split:  11%|█         | 5584/50000 [00:01<00:09, 4441.67 examples/s]\r\n",
      "Generating train split:  13%|█▎        | 6257/50000 [00:01<00:09, 4453.38 examples/s]\r\n",
      "Generating train split:  13%|█▎        | 6718/50000 [00:01<00:09, 4489.06 examples/s]\r\n",
      "Generating train split:  15%|█▍        | 7388/50000 [00:01<00:09, 4479.05 examples/s]\r\n",
      "Generating train split:  16%|█▌        | 7849/50000 [00:01<00:09, 4510.04 examples/s]\r\n",
      "Generating train split:  17%|█▋        | 8520/50000 [00:02<00:09, 4494.26 examples/s]\r\n",
      "Generating train split:  18%|█▊        | 8980/50000 [00:02<00:09, 4517.07 examples/s]\r\n",
      "Generating train split:  19%|█▉        | 9644/50000 [00:02<00:09, 4482.87 examples/s]\r\n",
      "Generating train split:  20%|██        | 10231/50000 [00:02<00:15, 2537.54 examples/s]\r\n",
      "Generating train split:  21%|██▏       | 10686/50000 [00:02<00:13, 2853.54 examples/s]\r\n",
      "Generating train split:  22%|██▏       | 11114/50000 [00:03<00:12, 3113.86 examples/s]\r\n",
      "Generating train split:  23%|██▎       | 11570/50000 [00:03<00:11, 3412.04 examples/s]\r\n",
      "Generating train split:  24%|██▍       | 12000/50000 [00:03<00:10, 3612.29 examples/s]\r\n",
      "Generating train split:  25%|██▍       | 12458/50000 [00:03<00:09, 3848.23 examples/s]\r\n",
      "Generating train split:  26%|██▌       | 12915/50000 [00:03<00:09, 4033.91 examples/s]\r\n",
      "Generating train split:  27%|██▋       | 13356/50000 [00:03<00:08, 4133.82 examples/s]\r\n",
      "Generating train split:  28%|██▊       | 13814/50000 [00:03<00:08, 4255.37 examples/s]\r\n",
      "Generating train split:  29%|██▉       | 14483/50000 [00:03<00:08, 4328.55 examples/s]\r\n",
      "Generating train split:  30%|██▉       | 14940/50000 [00:03<00:07, 4391.22 examples/s]\r\n",
      "Generating train split:  31%|███       | 15609/50000 [00:04<00:07, 4412.52 examples/s]\r\n",
      "Generating train split:  33%|███▎      | 16261/50000 [00:04<00:07, 4386.70 examples/s]\r\n",
      "Generating train split:  33%|███▎      | 16719/50000 [00:04<00:07, 4430.88 examples/s]\r\n",
      "Generating train split:  35%|███▍      | 17390/50000 [00:04<00:07, 4443.79 examples/s]\r\n",
      "Generating train split:  36%|███▌      | 17849/50000 [00:04<00:07, 4476.70 examples/s]\r\n",
      "Generating train split:  37%|███▋      | 18517/50000 [00:04<00:07, 4464.24 examples/s]\r\n",
      "Generating train split:  38%|███▊      | 18974/50000 [00:04<00:06, 4489.43 examples/s]\r\n",
      "Generating train split:  39%|███▉      | 19645/50000 [00:04<00:06, 4479.58 examples/s]\r\n",
      "Generating train split:  40%|████      | 20228/50000 [00:05<00:09, 3260.48 examples/s]\r\n",
      "Generating train split:  41%|████▏     | 20668/50000 [00:05<00:08, 3478.70 examples/s]\r\n",
      "Generating train split:  42%|████▏     | 21093/50000 [00:05<00:07, 3644.33 examples/s]\r\n",
      "Generating train split:  43%|████▎     | 21546/50000 [00:05<00:07, 3851.73 examples/s]\r\n",
      "Generating train split:  44%|████▍     | 22000/50000 [00:05<00:07, 3988.17 examples/s]\r\n",
      "Generating train split:  45%|████▍     | 22450/50000 [00:05<00:06, 4121.60 examples/s]\r\n",
      "Generating train split:  46%|████▌     | 22907/50000 [00:05<00:06, 4242.28 examples/s]\r\n",
      "Generating train split:  47%|████▋     | 23571/50000 [00:06<00:06, 4306.11 examples/s]\r\n",
      "Generating train split:  48%|████▊     | 24226/50000 [00:06<00:05, 4316.88 examples/s]\r\n",
      "Generating train split:  49%|████▉     | 24680/50000 [00:06<00:05, 4367.71 examples/s]\r\n",
      "Generating train split:  51%|█████     | 25339/50000 [00:06<00:05, 4372.94 examples/s]\r\n",
      "Generating train split:  52%|█████▏    | 25793/50000 [00:06<00:05, 4413.26 examples/s]\r\n",
      "Generating train split:  53%|█████▎    | 26453/50000 [00:06<00:05, 4388.52 examples/s]\r\n",
      "Generating train split:  54%|█████▍    | 26907/50000 [00:06<00:05, 4424.59 examples/s]\r\n",
      "Generating train split:  55%|█████▌    | 27569/50000 [00:06<00:05, 4416.86 examples/s]\r\n",
      "Generating train split:  56%|█████▋    | 28227/50000 [00:07<00:04, 4405.80 examples/s]\r\n",
      "Generating train split:  57%|█████▋    | 28681/50000 [00:07<00:04, 4436.05 examples/s]\r\n",
      "Generating train split:  59%|█████▊    | 29339/50000 [00:07<00:04, 4415.43 examples/s]\r\n",
      "Generating train split:  60%|█████▉    | 29794/50000 [00:07<00:04, 4446.21 examples/s]\r\n",
      "Generating train split:  61%|██████    | 30444/50000 [00:07<00:06, 3113.71 examples/s]\r\n",
      "Generating train split:  62%|██████▏   | 30887/50000 [00:07<00:05, 3359.45 examples/s]\r\n",
      "Generating train split:  63%|██████▎   | 31307/50000 [00:07<00:05, 3535.59 examples/s]\r\n",
      "Generating train split:  64%|██████▎   | 31756/50000 [00:08<00:04, 3755.01 examples/s]\r\n",
      "Generating train split:  64%|██████▍   | 32184/50000 [00:08<00:04, 3884.34 examples/s]\r\n",
      "Generating train split:  65%|██████▌   | 32631/50000 [00:08<00:04, 4034.11 examples/s]\r\n",
      "Generating train split:  66%|██████▌   | 33059/50000 [00:08<00:04, 4098.49 examples/s]\r\n",
      "Generating train split:  67%|██████▋   | 33512/50000 [00:08<00:03, 4216.92 examples/s]\r\n",
      "Generating train split:  68%|██████▊   | 33962/50000 [00:08<00:03, 4294.67 examples/s]\r\n",
      "Generating train split:  69%|██████▉   | 34617/50000 [00:08<00:03, 4318.84 examples/s]\r\n",
      "Generating train split:  71%|███████   | 35269/50000 [00:08<00:03, 4327.50 examples/s]\r\n",
      "Generating train split:  71%|███████▏  | 35720/50000 [00:08<00:03, 4369.86 examples/s]\r\n",
      "Generating train split:  73%|███████▎  | 36375/50000 [00:09<00:03, 4365.05 examples/s]\r\n",
      "Generating train split:  74%|███████▎  | 36821/50000 [00:09<00:03, 4385.91 examples/s]\r\n",
      "Generating train split:  75%|███████▍  | 37475/50000 [00:09<00:02, 4374.67 examples/s]\r\n",
      "Generating train split:  76%|███████▌  | 37923/50000 [00:09<00:02, 4398.12 examples/s]\r\n",
      "Generating train split:  77%|███████▋  | 38567/50000 [00:09<00:02, 4358.16 examples/s]\r\n",
      "Generating train split:  78%|███████▊  | 39221/50000 [00:09<00:02, 4354.79 examples/s]\r\n",
      "Generating train split:  79%|███████▉  | 39673/50000 [00:09<00:02, 4391.26 examples/s]\r\n",
      "Generating train split:  80%|████████  | 40229/50000 [00:10<00:03, 3046.83 examples/s]\r\n",
      "Generating train split:  81%|████████▏ | 40683/50000 [00:10<00:02, 3332.11 examples/s]\r\n",
      "Generating train split:  82%|████████▏ | 41116/50000 [00:10<00:02, 3543.73 examples/s]\r\n",
      "Generating train split:  83%|████████▎ | 41573/50000 [00:10<00:02, 3782.64 examples/s]\r\n",
      "Generating train split:  84%|████████▍ | 42014/50000 [00:10<00:02, 3938.44 examples/s]\r\n",
      "Generating train split:  85%|████████▍ | 42471/50000 [00:10<00:01, 4103.17 examples/s]\r\n",
      "Generating train split:  86%|████████▌ | 42924/50000 [00:10<00:01, 4218.87 examples/s]\r\n",
      "Generating train split:  87%|████████▋ | 43574/50000 [00:10<00:01, 4259.22 examples/s]\r\n",
      "Generating train split:  88%|████████▊ | 44230/50000 [00:11<00:01, 4293.47 examples/s]\r\n",
      "Generating train split:  89%|████████▉ | 44683/50000 [00:11<00:01, 4349.24 examples/s]\r\n",
      "Generating train split:  91%|█████████ | 45347/50000 [00:11<00:01, 4373.50 examples/s]\r\n",
      "Generating train split:  92%|█████████▏| 45800/50000 [00:11<00:00, 4411.95 examples/s]\r\n",
      "Generating train split:  93%|█████████▎| 46465/50000 [00:11<00:00, 4415.43 examples/s]\r\n",
      "Generating train split:  94%|█████████▍| 46922/50000 [00:11<00:00, 4450.48 examples/s]\r\n",
      "Generating train split:  95%|█████████▌| 47580/50000 [00:11<00:00, 4424.76 examples/s]\r\n",
      "Generating train split:  96%|█████████▋| 48240/50000 [00:11<00:00, 4414.53 examples/s]\r\n",
      "Generating train split:  97%|█████████▋| 48691/50000 [00:12<00:00, 4433.93 examples/s]\r\n",
      "Generating train split:  99%|█████████▊| 49352/50000 [00:12<00:00, 4419.93 examples/s]\r\n",
      "Generating train split: 100%|█████████▉| 49801/50000 [00:12<00:00, 4435.08 examples/s]\r\n",
      "                                                                                      \r\n",
      "\r\n",
      "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]\r\n",
      "Generating test split:   0%|          | 1/10000 [00:00<54:46,  3.04 examples/s]\r\n",
      "Generating test split:   5%|▍         | 455/10000 [00:00<00:06, 1377.38 examples/s]\r\n",
      "Generating test split:   9%|▉         | 912/10000 [00:00<00:03, 2340.97 examples/s]\r\n",
      "Generating test split:  13%|█▎        | 1338/10000 [00:00<00:02, 2916.12 examples/s]\r\n",
      "Generating test split:  18%|█▊        | 1794/10000 [00:00<00:02, 3408.00 examples/s]\r\n",
      "Generating test split:  22%|██▏       | 2231/10000 [00:00<00:02, 3693.87 examples/s]\r\n",
      "Generating test split:  27%|██▋       | 2687/10000 [00:00<00:01, 3953.02 examples/s]\r\n",
      "Generating test split:  33%|███▎      | 3345/10000 [00:01<00:01, 4117.87 examples/s]\r\n",
      "Generating test split:  38%|███▊      | 3802/10000 [00:01<00:01, 4237.10 examples/s]\r\n",
      "Generating test split:  45%|████▍     | 4466/10000 [00:01<00:01, 4302.08 examples/s]\r\n",
      "Generating test split:  49%|████▉     | 4926/10000 [00:01<00:01, 4375.13 examples/s]\r\n",
      "Generating test split:  56%|█████▌    | 5599/10000 [00:01<00:00, 4411.23 examples/s]\r\n",
      "Generating test split:  63%|██████▎   | 6263/10000 [00:01<00:00, 4412.52 examples/s]\r\n",
      "Generating test split:  67%|██████▋   | 6719/10000 [00:01<00:00, 4444.97 examples/s]\r\n",
      "Generating test split:  74%|███████▍  | 7391/10000 [00:01<00:00, 4453.67 examples/s]\r\n",
      "Generating test split:  79%|███████▊  | 7851/10000 [00:02<00:00, 4485.23 examples/s]\r\n",
      "Generating test split:  85%|████████▌ | 8509/10000 [00:02<00:00, 4448.36 examples/s]\r\n",
      "Generating test split:  90%|████████▉ | 8967/10000 [00:02<00:00, 4478.74 examples/s]\r\n",
      "Generating test split:  96%|█████████▋| 9631/10000 [00:02<00:00, 4455.57 examples/s]\r\n",
      "                                                                                    \r\n",
      "\r\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\r\n",
      "100%|██████████| 2/2 [00:00<00:00, 672.70it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 28.3MB/s]     \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.67M/1.67M [00:00<00:00, 32.3MB/s]\n",
      "[INFO|configuration_utils.py:668] 2023-06-27 03:10:53,252 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-06-27 03:10:53,256 >> Model config SwinConfig {\n",
      "  \"_name_or_path\": \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"airplane\",\n",
      "    \"1\": \"automobile\",\n",
      "    \"2\": \"bird\",\n",
      "    \"3\": \"cat\",\n",
      "    \"4\": \"deer\",\n",
      "    \"5\": \"dog\",\n",
      "    \"6\": \"frog\",\n",
      "    \"7\": \"horse\",\n",
      "    \"8\": \"ship\",\n",
      "    \"9\": \"truck\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"airplane\": \"0\",\n",
      "    \"automobile\": \"1\",\n",
      "    \"bird\": \"2\",\n",
      "    \"cat\": \"3\",\n",
      "    \"deer\": \"4\",\n",
      "    \"dog\": \"5\",\n",
      "    \"frog\": \"6\",\n",
      "    \"horse\": \"7\",\n",
      "    \"ship\": \"8\",\n",
      "    \"truck\": \"9\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "Downloading pytorch_model.bin: 100%|██████████| 437M/437M [00:03<00:00, 139MB/s]  \n",
      "[INFO|modeling_utils.py:2534] 2023-06-27 03:10:56,631 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:3190] 2023-06-27 03:10:57,597 >> All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "[WARNING|modeling_utils.py:3211] 2023-06-27 03:10:57,597 >> Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)rocessor_config.json: 100%|██████████| 255/255 [00:00<00:00, 280kB/s]\n",
      "[INFO|image_processing_utils.py:308] 2023-06-27 03:10:57,848 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/preprocessor_config.json\n",
      "[WARNING|image_processing_auto.py:327] 2023-06-27 03:10:57,848 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "[INFO|image_processing_utils.py:532] 2023-06-27 03:10:57,850 >> size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_utils.py:353] 2023-06-27 03:10:57,850 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 160\n",
      "CPU RAM = 1056426612 KB \n",
      "============================================================================================ \n",
      "[INFO|trainer.py:770] 2023-06-27 03:10:59,201 >> ***** Running training *****\n",
      "[INFO|trainer.py:771] 2023-06-27 03:10:59,201 >>   Num examples = 42,500\n",
      "[INFO|trainer.py:772] 2023-06-27 03:10:59,201 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:773] 2023-06-27 03:10:59,201 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:774] 2023-06-27 03:10:59,201 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:775] 2023-06-27 03:10:59,201 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:776] 2023-06-27 03:10:59,201 >>   Total optimization steps = 665\n",
      "[INFO|trainer.py:777] 2023-06-27 03:10:59,203 >>   Number of trainable parameters = 86,753,474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/665 [00:35<08:12,  1.33it/s]STAGE:2023-06-27 03:11:34 241:241 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "  2%|▏         | 13/665 [00:48<25:14,  2.32s/it]STAGE:2023-06-27 03:11:48 241:241 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-27 03:11:48 241:241 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "100%|██████████| 665/665 [03:27<00:00,  5.12it/s][INFO|trainer.py:1041] 2023-06-27 03:14:26,800 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 665/665 [03:27<00:00,  3.20it/s]\n",
      "[INFO|trainer.py:1766] 2023-06-27 03:14:26,871 >> Saving model checkpoint to /tmp/outputs/\n",
      "[INFO|configuration_utils.py:457] 2023-06-27 03:14:27,022 >> Configuration saved in /tmp/outputs/config.json\n",
      "[INFO|modeling_utils.py:1847] 2023-06-27 03:14:27,309 >> Model weights saved in /tmp/outputs/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:203] 2023-06-27 03:14:27,309 >> Image processor saved in /tmp/outputs/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:113] 2023-06-27 03:14:27,309 >> Configuration saved in /tmp/outputs/gaudi_config.json\n",
      "[INFO|modelcard.py:451] 2023-06-27 03:14:27,467 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Image Classification', 'type': 'image-classification'}, 'dataset': {'name': 'cifar10', 'type': 'cifar10', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27, 'learning_rate': 7.443609022556391e-06, 'epoch': 0.75, 'memory_allocated (GB)': 90.52, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "{'train_runtime': 207.6646, 'train_samples_per_second': 240.412, 'train_steps_per_second': 3.762, 'train_loss': 0.2721804511278195, 'epoch': 1.0, 'memory_allocated (GB)': 90.84, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "***** train metrics *****\n",
      "  epoch                       =        1.0\n",
      "  max_memory_allocated (GB)   =      92.25\n",
      "  memory_allocated (GB)       =      90.84\n",
      "  total_memory_available (GB) =      93.74\n",
      "  train_loss                  =     0.2722\n",
      "  train_runtime               = 0:03:27.66\n",
      "  train_samples_per_second    =    240.412\n",
      "  train_steps_per_second      =      3.762\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python run_image_classification.py \\\n",
    "--model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "--dataset_name cifar10 \\\n",
    "--output_dir /tmp/outputs/ \\\n",
    "--remove_unused_columns False \\\n",
    "--do_train \\\n",
    "--learning_rate 3e-5 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 64 \\\n",
    "--evaluation_strategy no \\\n",
    "--save_strategy no \\\n",
    "--load_best_model_at_end False \\\n",
    "--save_total_limit 3 \\\n",
    "--seed 1337 \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs \\\n",
    "--gaudi_config_name Habana/swin \\\n",
    "--throughput_warmup_steps 2 \\\n",
    "--overwrite_output_dir \\\n",
    "--ignore_mismatched_sizes \\\n",
    "--profiling_warmup_steps 10 \\\n",
    "--profiling_steps 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 746936\r\n",
      "drwxr-xr-x 2 root root      4096 Jun 27 03:37 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 5 root root      4096 Jun 27 05:49 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-r--r-- 1 root root 247277431 Jun 27 03:27 sc09wynn01-hls2_1725.1687836473515.pt.trace.json\r\n",
      "-rw-r--r-- 1 root root 270297943 Jun 27 03:12 sc09wynn01-hls2_241.1687835527077.pt.trace.json\r\n",
      "-rw-r--r-- 1 root root 247261794 Jun 27 03:37 sc09wynn01-hls2_3618.1687837025280.pt.trace.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls -al ./hpu_profile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two ways to use HPU Performance Analysis tool\n",
    "\n",
    "We can launch Tensorboard to see the performance analysis results:\n",
    "```sh\n",
    "tensorboard --logdir xxx\n",
    "```\n",
    "\n",
    "Or simply use `habana_perf_tool` to see the console output analysis:\n",
    "```sh\n",
    "habana_perf_tool --trace xxx.trace.json\n",
    "```\n",
    "\n",
    "Notice the contents of `habana_perf_tool` console output:\n",
    "\n",
    "`Device/Host ratio` - To show the overall performance, device utilization  \n",
    "`Host Summary` - Host side performance, to show dataloader, graph build, data copy and compile  \n",
    "`Device Summary` - Device side performance, to show MME, TPC and DMA  \n",
    "`Host/Device Recommendations` - Performance Recommendations for model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 200084\r\n",
      "drwxr-xr-x 2 root root      4096 Jun 27 05:49 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 6 root root      4096 Jun 27 06:25 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-r--r-- 1 root root 204870550 Jun 27 05:49 sc09wynn01-hls2_355804.1687765902207.pt.trace.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls -al ./swin_profile/unoptimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-27 07:48:58,085 - pytorch_profiler - DEBUG - Loading ./swin_profile/unoptimized/sc09wynn01-hls2_355804.1687765902207.pt.trace.json\n",
      "Import Data (KB): 100%|█████████████| 200068/200068 [00:01<00:00, 144825.12it/s]\n",
      "2023-06-27 07:49:00,774 - pytorch_profiler - DEBUG - Please wait for initialization to finish ...\n",
      "2023-06-27 07:49:06,352 - pytorch_profiler - DEBUG - PT Track ids: BridgeTrackIds.Result(pt_bridge_launch='46,51,6', pt_bridge_compute='15', pt_mem_copy='6', pt_mem_log='', pt_build_graph='48,49,45,5')\n",
      "2023-06-27 07:49:06,352 - pytorch_profiler - DEBUG - Track ids: TrackIds.Result(forward='4', backward='44', synapse_launch='0,47,50', synapse_wait='1,9', device_mme='40,41,42,43', device_tpc='16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39', device_dma='7,10,11,12,13,14')\n",
      "2023-06-27 07:49:08,488 - pytorch_profiler - DEBUG - Device ratio: 61.66 % (288.393 ms, 467.734 ms)\n",
      "2023-06-27 07:49:08,488 - pytorch_profiler - DEBUG - Device/Host ratio: 61.66% / 38.34%\n",
      "2023-06-27 07:49:09,087 - pytorch_profiler - DEBUG - Host Summary Graph Build: 14.50 % (60.240976 ms, 415.491 ms)\n",
      "2023-06-27 07:49:09,232 - pytorch_profiler - DEBUG - Host Summary DataLoader: 55.98 % (232.607 ms, 415.491 ms)\n",
      "2023-06-27 07:49:09,425 - pytorch_profiler - DEBUG - Host Summary Input Time: 4.62 % (19.187 ms, 415.491 ms)\n",
      "2023-06-27 07:49:09,841 - pytorch_profiler - DEBUG - Host Summary Compile Time: 1.52 % (6.31 ms, 415.491 ms)\n",
      "2023-06-27 07:49:10,164 - pytorch_profiler - DEBUG - Device Summary MME Lower Precision Ratio: 77.08%\n",
      "2023-06-27 07:49:10,164 - pytorch_profiler - DEBUG - Device Host Overlapping degree: 81.88 %\n",
      "2023-06-27 07:49:10,164 - pytorch_profiler - DEBUG - Host Recommendations: \n",
      "2023-06-27 07:49:10,164 - pytorch_profiler - DEBUG - \tThis run has high time cost on input data loading. 55.98% of the step time is in DataLoader. You could use Habana DataLoader. Or you could try to tune num_workers on DataLoader's construction.\n",
      "2023-06-27 07:49:10,164 - pytorch_profiler - DEBUG - \tCompile times per step : [2]. Compile ratio: 1.52% (total time: 6.31 ms)\n",
      "2023-06-27 07:49:10,338 - pytorch_profiler - DEBUG - [Device Summary] MME total time 88.28 ms\n",
      "2023-06-27 07:49:14,199 - pytorch_profiler - DEBUG - [Device Summary] MME/TPC overlap time 57.94 ms\n",
      "2023-06-27 07:49:14,200 - pytorch_profiler - DEBUG - [Device Summary] TPC total time 165.36 ms\n",
      "2023-06-27 07:49:15,302 - pytorch_profiler - DEBUG - [Device Summary] DMA total time 29.43 ms\n",
      "2023-06-27 07:49:15,303 - pytorch_profiler - DEBUG - [Device Summary] Idle total time: 5.32 ms\n"
     ]
    }
   ],
   "source": [
    "!habana_perf_tool --trace ./swin_profile/unoptimized/sc09wynn01-hls2_355804.1687765902207.pt.trace.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply optimization 1 (set dataloader num_workers)\n",
    "Notice the command for optimization:\n",
    "\n",
    "`--dataloader_num_workers 4` - perform multi-process data loading by simply setting the `num_workers` to a positive integer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2023 03:26:42 - WARNING - __main__ - Process rank: -1, device: hpu, distributed training: False, mixed-precision training: True\n",
      "06/27/2023 03:26:42 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/swin,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/outputs/runs/Jun27_03-26-42_sc09wynn01-hls2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/tmp/outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_steps=3,\n",
      "profiling_warmup_steps=10,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/outputs/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "skip_memory_metrics=True,\n",
      "throughput_warmup_steps=2,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=True,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "06/27/2023 03:26:43 - WARNING - datasets.builder - Found cached dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/training_args.py:252: FutureWarning: `--use_hpu_graphs` is deprecated and will be removed in a future version of 🤗 Optimum Habana. Use `--use_hpu_graphs_for_inference` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2023 03:26:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-0e6610774520e174.arrow\n",
      "06/27/2023 03:26:43 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-ab7f4e0bbd39b5eb.arrow\n",
      "06/27/2023 03:26:43 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-ec174b38d51cda7f.arrow and /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-bc2d211ee9dfd2ce.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 629.87it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-06-27 03:26:44,334 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-06-27 03:26:44,338 >> Model config SwinConfig {\n",
      "  \"_name_or_path\": \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"airplane\",\n",
      "    \"1\": \"automobile\",\n",
      "    \"2\": \"bird\",\n",
      "    \"3\": \"cat\",\n",
      "    \"4\": \"deer\",\n",
      "    \"5\": \"dog\",\n",
      "    \"6\": \"frog\",\n",
      "    \"7\": \"horse\",\n",
      "    \"8\": \"ship\",\n",
      "    \"9\": \"truck\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"airplane\": \"0\",\n",
      "    \"automobile\": \"1\",\n",
      "    \"bird\": \"2\",\n",
      "    \"cat\": \"3\",\n",
      "    \"deer\": \"4\",\n",
      "    \"dog\": \"5\",\n",
      "    \"frog\": \"6\",\n",
      "    \"horse\": \"7\",\n",
      "    \"ship\": \"8\",\n",
      "    \"truck\": \"9\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2534] 2023-06-27 03:26:44,341 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:3190] 2023-06-27 03:26:45,251 >> All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "[WARNING|modeling_utils.py:3211] 2023-06-27 03:26:45,251 >> Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|image_processing_utils.py:308] 2023-06-27 03:26:45,361 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/preprocessor_config.json\n",
      "[WARNING|image_processing_auto.py:327] 2023-06-27 03:26:45,361 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "[INFO|image_processing_utils.py:532] 2023-06-27 03:26:45,363 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_utils.py:353] 2023-06-27 03:26:45,363 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 160\n",
      "CPU RAM = 1056426612 KB \n",
      "============================================================================================ \n",
      "[INFO|trainer.py:770] 2023-06-27 03:26:46,729 >> ***** Running training *****\n",
      "[INFO|trainer.py:771] 2023-06-27 03:26:46,729 >>   Num examples = 42,500\n",
      "[INFO|trainer.py:772] 2023-06-27 03:26:46,729 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:773] 2023-06-27 03:26:46,729 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:774] 2023-06-27 03:26:46,729 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:775] 2023-06-27 03:26:46,729 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:776] 2023-06-27 03:26:46,729 >>   Total optimization steps = 665\n",
      "[INFO|trainer.py:777] 2023-06-27 03:26:46,730 >>   Number of trainable parameters = 86,753,474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/665 [00:34<07:19,  1.49it/s]STAGE:2023-06-27 03:27:21 1725:1725 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "  2%|▏         | 13/665 [00:48<24:06,  2.22s/it]STAGE:2023-06-27 03:27:35 1725:1725 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-27 03:27:35 1725:1725 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "100%|██████████| 665/665 [02:43<00:00,  8.75it/s][INFO|trainer.py:1041] 2023-06-27 03:29:30,205 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 665/665 [02:43<00:00,  4.07it/s]\n",
      "[INFO|trainer.py:1766] 2023-06-27 03:29:30,243 >> Saving model checkpoint to /tmp/outputs/\n",
      "[INFO|configuration_utils.py:457] 2023-06-27 03:29:30,355 >> Configuration saved in /tmp/outputs/config.json\n",
      "[INFO|modeling_utils.py:1847] 2023-06-27 03:29:31,002 >> Model weights saved in /tmp/outputs/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:203] 2023-06-27 03:29:31,003 >> Image processor saved in /tmp/outputs/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:113] 2023-06-27 03:29:31,003 >> Configuration saved in /tmp/outputs/gaudi_config.json\n",
      "[INFO|modelcard.py:451] 2023-06-27 03:29:31,180 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Image Classification', 'type': 'image-classification'}, 'dataset': {'name': 'cifar10', 'type': 'cifar10', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.288, 'learning_rate': 7.443609022556391e-06, 'epoch': 0.75, 'memory_allocated (GB)': 90.52, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "{'train_runtime': 163.5047, 'train_samples_per_second': 322.011, 'train_steps_per_second': 5.039, 'train_loss': 0.28533834586466167, 'epoch': 1.0, 'memory_allocated (GB)': 90.84, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "***** train metrics *****\n",
      "  epoch                       =        1.0\n",
      "  max_memory_allocated (GB)   =      92.25\n",
      "  memory_allocated (GB)       =      90.84\n",
      "  total_memory_available (GB) =      93.74\n",
      "  train_loss                  =     0.2853\n",
      "  train_runtime               = 0:02:43.50\n",
      "  train_samples_per_second    =    322.011\n",
      "  train_steps_per_second      =      5.039\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python run_image_classification.py \\\n",
    "--model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "--dataset_name cifar10 \\\n",
    "--output_dir /tmp/outputs/ \\\n",
    "--remove_unused_columns False \\\n",
    "--do_train \\\n",
    "--learning_rate 3e-5 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 64 \\\n",
    "--evaluation_strategy no \\\n",
    "--save_strategy no \\\n",
    "--load_best_model_at_end False \\\n",
    "--save_total_limit 3 \\\n",
    "--seed 1337 \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs \\\n",
    "--gaudi_config_name Habana/swin \\\n",
    "--throughput_warmup_steps 2 \\\n",
    "--overwrite_output_dir \\\n",
    "--ignore_mismatched_sizes \\\n",
    "--dataloader_num_workers 4 \\\n",
    "--profiling_warmup_steps 10 \\\n",
    "--profiling_steps 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 177488\r\n",
      "drwxr-xr-x 2 root root      4096 Jun 27 05:49 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 6 root root      4096 Jun 27 06:25 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-r--r-- 1 root root 181733526 Jun 27 05:49 sc09wynn01-hls2_357081.1687766946032.pt.trace.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls -al ./swin_profile/1st_optim_num_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-27 07:49:39,860 - pytorch_profiler - DEBUG - Loading ./swin_profile/1st_optim_num_worker/sc09wynn01-hls2_357081.1687766946032.pt.trace.json\n",
      "Import Data (KB): 100%|█████████████| 177474/177474 [00:01<00:00, 145866.15it/s]\n",
      "2023-06-27 07:49:42,087 - pytorch_profiler - DEBUG - Please wait for initialization to finish ...\n",
      "2023-06-27 07:49:46,999 - pytorch_profiler - DEBUG - PT Track ids: BridgeTrackIds.Result(pt_bridge_launch='9,54,49', pt_bridge_compute='18', pt_mem_copy='9', pt_mem_log='', pt_build_graph='8,48,51,52')\n",
      "2023-06-27 07:49:47,000 - pytorch_profiler - DEBUG - Track ids: TrackIds.Result(forward='7', backward='47', synapse_launch='0,50,53', synapse_wait='1,12', device_mme='43,45,46,44', device_tpc='36,30,26,31,23,25,35,19,29,38,24,22,33,37,27,20,41,32,28,34,40,42,39,21', device_dma='10,17,15,13,14,16')\n",
      "2023-06-27 07:49:49,017 - pytorch_profiler - DEBUG - Device ratio: 90.84 % (283.428 ms, 312.02 ms)\n",
      "2023-06-27 07:49:49,017 - pytorch_profiler - DEBUG - Device/Host ratio: 90.84% / 9.16%\n",
      "2023-06-27 07:49:49,539 - pytorch_profiler - DEBUG - Host Summary Graph Build: 28.77 % (59.886976 ms, 208.177 ms)\n",
      "2023-06-27 07:49:49,643 - pytorch_profiler - DEBUG - Host Summary DataLoader: 1.56 % (3.249 ms, 208.177 ms)\n",
      "2023-06-27 07:49:49,802 - pytorch_profiler - DEBUG - Host Summary Input Time: 11.58 % (24.109 ms, 208.177 ms)\n",
      "2023-06-27 07:49:50,201 - pytorch_profiler - DEBUG - Host Summary Compile Time: 2.28 % (4.746 ms, 208.177 ms)\n",
      "2023-06-27 07:49:50,531 - pytorch_profiler - DEBUG - Device Summary MME Lower Precision Ratio: 77.08%\n",
      "2023-06-27 07:49:50,531 - pytorch_profiler - DEBUG - Device Host Overlapping degree: 86.27 %\n",
      "2023-06-27 07:49:50,531 - pytorch_profiler - DEBUG - Host Recommendations: \n",
      "2023-06-27 07:49:50,531 - pytorch_profiler - DEBUG - \t11.58% H2D of the step time is in Input Data Time. Step call times: [28, 28, 28]. You could try to set non-blocking in torch.Tensor.to and pin_memory in DataLoader's construction to asynchronously convert CPU tensor with pinned memory to a HPU tensor.\n",
      "2023-06-27 07:49:50,531 - pytorch_profiler - DEBUG - \tCompile times per step : [2]. Compile ratio: 2.28% (total time: 4.75 ms)\n",
      "2023-06-27 07:49:50,704 - pytorch_profiler - DEBUG - [Device Summary] MME total time 88.26 ms\n",
      "2023-06-27 07:49:54,580 - pytorch_profiler - DEBUG - [Device Summary] MME/TPC overlap time 57.95 ms\n",
      "2023-06-27 07:49:54,580 - pytorch_profiler - DEBUG - [Device Summary] TPC total time 165.50 ms\n",
      "2023-06-27 07:49:55,673 - pytorch_profiler - DEBUG - [Device Summary] DMA total time 26.40 ms\n",
      "2023-06-27 07:49:55,673 - pytorch_profiler - DEBUG - [Device Summary] Idle total time: 3.26 ms\n"
     ]
    }
   ],
   "source": [
    "!habana_perf_tool --trace ./swin_profile/1st_optim_num_worker/sc09wynn01-hls2_357081.1687766946032.pt.trace.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply optimization 2 (using asynchronous copy)\n",
    "Notice the command for optimization:\n",
    "\n",
    "`--non_blocking_data_copy True` - specifying the argument `non_blocking=True` during the copy operation, the Python thread can continue to execute other tasks while the copy occurs in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2023 03:35:50 - WARNING - __main__ - Process rank: -1, device: hpu, distributed training: False, mixed-precision training: True\n",
      "06/27/2023 03:35:50 - INFO - __main__ - Training/evaluation parameters GaudiTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "adjust_throughput=False,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=230,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "distribution_strategy=ddp,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gaudi_config_name=Habana/swin,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=hpu_amp,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/outputs/runs/Jun27_03-35-49_sc09wynn01-hls2,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "no_cuda=False,\n",
      "non_blocking_data_copy=True,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=/tmp/outputs/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "pipelining_fwd_bwd=False,\n",
      "prediction_loss_only=False,\n",
      "profiling_steps=3,\n",
      "profiling_warmup_steps=10,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/outputs/,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=3,\n",
      "seed=1337,\n",
      "skip_memory_metrics=True,\n",
      "throughput_warmup_steps=2,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "use_habana=True,\n",
      "use_hpu_graphs=True,\n",
      "use_hpu_graphs_for_inference=False,\n",
      "use_hpu_graphs_for_training=False,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "06/27/2023 03:35:51 - WARNING - datasets.builder - Found cached dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/optimum/habana/transformers/training_args.py:252: FutureWarning: `--use_hpu_graphs` is deprecated and will be removed in a future version of 🤗 Optimum Habana. Use `--use_hpu_graphs_for_inference` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/27/2023 03:35:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-0e6610774520e174.arrow\n",
      "06/27/2023 03:35:51 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-ab7f4e0bbd39b5eb.arrow\n",
      "06/27/2023 03:35:51 - WARNING - datasets.arrow_dataset - Loading cached split indices for dataset at /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-ec174b38d51cda7f.arrow and /root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4/cache-bc2d211ee9dfd2ce.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 591.62it/s]\n",
      "[INFO|configuration_utils.py:668] 2023-06-27 03:35:51,720 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/config.json\n",
      "[INFO|configuration_utils.py:720] 2023-06-27 03:35:51,724 >> Model config SwinConfig {\n",
      "  \"_name_or_path\": \"microsoft/swin-base-patch4-window7-224-in22k\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"finetuning_task\": \"image-classification\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"airplane\",\n",
      "    \"1\": \"automobile\",\n",
      "    \"2\": \"bird\",\n",
      "    \"3\": \"cat\",\n",
      "    \"4\": \"deer\",\n",
      "    \"5\": \"dog\",\n",
      "    \"6\": \"frog\",\n",
      "    \"7\": \"horse\",\n",
      "    \"8\": \"ship\",\n",
      "    \"9\": \"truck\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"airplane\": \"0\",\n",
      "    \"automobile\": \"1\",\n",
      "    \"bird\": \"2\",\n",
      "    \"cat\": \"3\",\n",
      "    \"deer\": \"4\",\n",
      "    \"dog\": \"5\",\n",
      "    \"frog\": \"6\",\n",
      "    \"horse\": \"7\",\n",
      "    \"ship\": \"8\",\n",
      "    \"truck\": \"9\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": [\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"out_indices\": [\n",
      "    4\n",
      "  ],\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2534] 2023-06-27 03:35:51,727 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:3190] 2023-06-27 03:35:52,612 >> All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "[WARNING|modeling_utils.py:3211] 2023-06-27 03:35:52,613 >> Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-base-patch4-window7-224-in22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([10, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|image_processing_utils.py:308] 2023-06-27 03:35:52,712 >> loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--swin-base-patch4-window7-224-in22k/snapshots/790d9b6014f6d157cc34d70afc0604eccc92dadd/preprocessor_config.json\n",
      "[WARNING|image_processing_auto.py:327] 2023-06-27 03:35:52,712 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "[INFO|image_processing_utils.py:532] 2023-06-27 03:35:52,713 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
      "[INFO|image_processing_utils.py:353] 2023-06-27 03:35:52,714 >> Image processor ViTImageProcessor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_processor_type\": \"ViTImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_CLUSTERED_PROGRAM = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_ENFORCE = 0\n",
      " PT_HPU_CLUSTERED_PROGRAM_SPLIT_STR = default\n",
      " PT_HPU_CLUSTERED_PROGRAM_SCHED_STR = default\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 160\n",
      "CPU RAM = 1056426612 KB \n",
      "============================================================================================ \n",
      "[INFO|trainer.py:770] 2023-06-27 03:35:54,115 >> ***** Running training *****\n",
      "[INFO|trainer.py:771] 2023-06-27 03:35:54,116 >>   Num examples = 42,500\n",
      "[INFO|trainer.py:772] 2023-06-27 03:35:54,116 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:773] 2023-06-27 03:35:54,116 >>   Instantaneous batch size per device = 64\n",
      "[INFO|trainer.py:774] 2023-06-27 03:35:54,116 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:775] 2023-06-27 03:35:54,116 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:776] 2023-06-27 03:35:54,116 >>   Total optimization steps = 665\n",
      "[INFO|trainer.py:777] 2023-06-27 03:35:54,117 >>   Number of trainable parameters = 86,753,474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/665 [00:38<07:58,  1.37it/s]STAGE:2023-06-27 03:36:32 3618:3618 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "  2%|▏         | 13/665 [00:50<23:58,  2.21s/it]STAGE:2023-06-27 03:36:45 3618:3618 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-06-27 03:36:45 3618:3618 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
      "100%|██████████| 665/665 [02:43<00:00,  8.45it/s][INFO|trainer.py:1041] 2023-06-27 03:38:37,480 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 665/665 [02:43<00:00,  4.07it/s]\n",
      "[INFO|trainer.py:1766] 2023-06-27 03:38:37,508 >> Saving model checkpoint to /tmp/outputs/\n",
      "[INFO|configuration_utils.py:457] 2023-06-27 03:38:37,591 >> Configuration saved in /tmp/outputs/config.json\n",
      "[INFO|modeling_utils.py:1847] 2023-06-27 03:38:38,242 >> Model weights saved in /tmp/outputs/pytorch_model.bin\n",
      "[INFO|image_processing_utils.py:203] 2023-06-27 03:38:38,242 >> Image processor saved in /tmp/outputs/preprocessor_config.json\n",
      "[INFO|configuration_utils.py:113] 2023-06-27 03:38:38,243 >> Configuration saved in /tmp/outputs/gaudi_config.json\n",
      "[INFO|modelcard.py:451] 2023-06-27 03:38:38,437 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Image Classification', 'type': 'image-classification'}, 'dataset': {'name': 'cifar10', 'type': 'cifar10', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.288, 'learning_rate': 7.443609022556391e-06, 'epoch': 0.75, 'memory_allocated (GB)': 90.52, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "{'train_runtime': 163.3821, 'train_samples_per_second': 330.061, 'train_steps_per_second': 5.165, 'train_loss': 0.28533834586466167, 'epoch': 1.0, 'memory_allocated (GB)': 90.84, 'max_memory_allocated (GB)': 92.25, 'total_memory_available (GB)': 93.74}\n",
      "***** train metrics *****\n",
      "  epoch                       =        1.0\n",
      "  max_memory_allocated (GB)   =      92.25\n",
      "  memory_allocated (GB)       =      90.84\n",
      "  total_memory_available (GB) =      93.74\n",
      "  train_loss                  =     0.2853\n",
      "  train_runtime               = 0:02:43.38\n",
      "  train_samples_per_second    =    330.061\n",
      "  train_steps_per_second      =      5.165\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "python run_image_classification.py \\\n",
    "--model_name_or_path microsoft/swin-base-patch4-window7-224-in22k \\\n",
    "--dataset_name cifar10 \\\n",
    "--output_dir /tmp/outputs/ \\\n",
    "--remove_unused_columns False \\\n",
    "--do_train \\\n",
    "--learning_rate 3e-5 \\\n",
    "--num_train_epochs 1 \\\n",
    "--per_device_train_batch_size 64 \\\n",
    "--evaluation_strategy no \\\n",
    "--save_strategy no \\\n",
    "--load_best_model_at_end False \\\n",
    "--save_total_limit 3 \\\n",
    "--seed 1337 \\\n",
    "--use_habana \\\n",
    "--use_lazy_mode \\\n",
    "--use_hpu_graphs \\\n",
    "--gaudi_config_name Habana/swin \\\n",
    "--throughput_warmup_steps 2 \\\n",
    "--overwrite_output_dir \\\n",
    "--ignore_mismatched_sizes \\\n",
    "--dataloader_num_workers 4 \\\n",
    "--non_blocking_data_copy True \\\n",
    "--profiling_warmup_steps 10 \\\n",
    "--profiling_steps 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 177508\r\n",
      "drwxr-xr-x 2 root root      4096 Jun 27 05:49 \u001b[0m\u001b[01;34m.\u001b[0m/\r\n",
      "drwxr-xr-x 6 root root      4096 Jun 27 06:25 \u001b[01;34m..\u001b[0m/\r\n",
      "-rw-r--r-- 1 root root 181755865 Jun 27 05:49 sc09wynn01-hls2_360115.1687767808701.pt.trace.json\r\n"
     ]
    }
   ],
   "source": [
    "%ls -al ./swin_profile/2nd_optim_non_blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-27 07:50:05,682 - pytorch_profiler - DEBUG - Loading ./swin_profile/2nd_optim_non_blocking/sc09wynn01-hls2_360115.1687767808701.pt.trace.json\n",
      "Import Data (KB): 100%|█████████████| 177495/177495 [00:01<00:00, 137861.97it/s]\n",
      "2023-06-27 07:50:07,981 - pytorch_profiler - DEBUG - Please wait for initialization to finish ...\n",
      "2023-06-27 07:50:12,889 - pytorch_profiler - DEBUG - PT Track ids: BridgeTrackIds.Result(pt_bridge_launch='56,9,51', pt_bridge_compute='15', pt_mem_copy='9,58,13,57', pt_mem_log='', pt_build_graph='8,50,53,54')\n",
      "2023-06-27 07:50:12,890 - pytorch_profiler - DEBUG - Track ids: TrackIds.Result(forward='7', backward='49', synapse_launch='0,52,55', synapse_wait='1,12', device_mme='45,47,48,46', device_tpc='29,31,26,32,41,25,27,21,36,28,30,24,35,43,39,22,44,38,34,42,33,37,40,23', device_dma='10,19,17,20,16,18')\n",
      "2023-06-27 07:50:14,897 - pytorch_profiler - DEBUG - Device ratio: 91.74 % (280.442 ms, 305.698 ms)\n",
      "2023-06-27 07:50:14,897 - pytorch_profiler - DEBUG - Device/Host ratio: 91.74% / 8.26%\n",
      "2023-06-27 07:50:15,407 - pytorch_profiler - DEBUG - Host Summary Graph Build: 33.66 % (67.771976 ms, 201.314 ms)\n",
      "2023-06-27 07:50:15,512 - pytorch_profiler - DEBUG - Host Summary DataLoader: 1.69 % (3.412 ms, 201.314 ms)\n",
      "2023-06-27 07:50:15,671 - pytorch_profiler - DEBUG - Host Summary Input Time: 1.33 % (2.687 ms, 201.314 ms)\n",
      "2023-06-27 07:50:16,067 - pytorch_profiler - DEBUG - Host Summary Compile Time: 2.31 % (4.652 ms, 201.314 ms)\n",
      "2023-06-27 07:50:16,387 - pytorch_profiler - DEBUG - Device Summary MME Lower Precision Ratio: 77.08%\n",
      "2023-06-27 07:50:16,387 - pytorch_profiler - DEBUG - Device Host Overlapping degree: 87.45 %\n",
      "2023-06-27 07:50:16,387 - pytorch_profiler - DEBUG - Host Recommendations: \n",
      "2023-06-27 07:50:16,387 - pytorch_profiler - DEBUG - \tCompile times per step : [2]. Compile ratio: 2.31% (total time: 4.65 ms)\n",
      "2023-06-27 07:50:16,559 - pytorch_profiler - DEBUG - [Device Summary] MME total time 88.22 ms\n",
      "2023-06-27 07:50:20,377 - pytorch_profiler - DEBUG - [Device Summary] MME/TPC overlap time 57.90 ms\n",
      "2023-06-27 07:50:20,378 - pytorch_profiler - DEBUG - [Device Summary] TPC total time 165.44 ms\n",
      "2023-06-27 07:50:21,456 - pytorch_profiler - DEBUG - [Device Summary] DMA total time 33.71 ms\n"
     ]
    }
   ],
   "source": [
    "!habana_perf_tool --trace ./swin_profile/2nd_optim_non_blocking/sc09wynn01-hls2_360115.1687767808701.pt.trace.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
